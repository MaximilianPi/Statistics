\documentclass[a4paper,twoside]{tufte-book} %style file is in the same folder.

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{german}

\usepackage{color}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{listings}

\usepackage{graphicx}

\usepackage{multicol}              
\usepackage{multirow}
\usepackage{booktabs}
%\usepackage{natbib} 

\usepackage[innerrightmargin = 0.7cm, innerleftmargin = 0.3cm]{mdframed}
\usepackage{mdwlist}

\usepackage[]{hyperref}
\definecolor{darkblue}{rgb}{0,0,.5}
\hypersetup{colorlinks=true, breaklinks=true, linkcolor=darkblue, menucolor=darkblue, urlcolor=blue, citecolor=darkblue}

\usepackage[toc,page]{appendix}


\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

\lstset{ % settings for listings needs to be be changed to R sytanx 
language=R,
breaklines = true,
columns=fullflexible,
breakautoindent = false,
%basicstyle=\listingsfont, 
basicstyle=\ttfamily \scriptsize,
keywordstyle=\color{black},                          
identifierstyle=\color{black},
commentstyle=\color{gray},
xleftmargin=3.4pt,
xrightmargin=3.4pt,
numbers=none,
literate={*}{{\char42}}1
         {-}{{\char45}}1
         {\ }{{\copyablespace}}1
}
% http://www.monperrus.net/martin/copy-pastable-listings-in-pdf-from-latex
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}



<<setup, cache=FALSE, include=FALSE>>=
library(knitr)
opts_knit$set(tidy = T, fig=TRUE, fig.height = 4, fig.width=4, fig.align='center')
render_listings()
@

<<echo=FALSE, cache=TRUE, results='hide', message=FALSE, warning=FALSE>>=
set.seed(123)
@



\title{Grundlagen der\\Statistik}
\author{Florian Hartig}


\begin{document}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE} % don't activate this for knitr

\let\cleardoublepage\clearpage % No empty pages between chapters
\maketitle


\thispagestyle{empty}
\null


\href{http://www.uni-regensburg.de/biologie-vorklinische-medizin/theoretische-oekologie/mitarbeiter/hartig/index.html}{Prof. Dr. Florian Hartig}\\
University of Regensburg\\
Germany\\[0.5cm]

\begin{fullwidth}
Vorlesungsunterlagen für Studierende der

\begin{itemize*}
  \item BSc Biostatistik
  \item MSc Research Skills (Webseite \href{http://florianhartig.github.io/ResearchSkills/}{hier})
\end{itemize*}

\vspace{0.5cm}

Fehler oder Verbesserungsvorschläge bitte über \href{https://github.com/florianhartig/Statistics/issues}{issue tracker} des \href{https://github.com/florianhartig/Statistics/tree/master/EssentialStatistics}{GitHub repository} melden. 

\end{fullwidth}


\vfill
\begin{fullwidth}
Created 2014, University of Freiburg. Updated 2015, University of Freiburg. Updated 2016, University of Regensburg, and translated to German with the help of Katrin Pohl. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.
\end{fullwidth}


\newpage
\tableofcontents

\chapter{Einleitung} % Use chapters instead of sections

	\section{Ziel und Zielgruppe diese Textes}
	
	Das Skript "Grundlagen der Statistik" hat das Ziel, eine kurze, aber präzise Einführung in die Grundlegenden Methoden und die Philosophie der modernen Statistik zu liefern, sowie in die statistischen Analysen die typischerweise in einfachen Experimenten und beobachtenden Studien benutzt werden.
	
	\section{Themen der Statistik und Datenwissenschaften}
	
	Die Statistik, oder der moderne, etwas weiter gefasst Begriff "Data Science", befasst sich mit der Visualisierung, Beschreibung und Interpretation von Daten, sowie mit dem Erstellen von datenbasierten Vorhersagen. Dieses Skript beinhaltet eine Einführung in die vier wichtigsten Säulen der statistischen Methodik für einen quantitativen Wissenschaftler:
	
	\paragraph{Deskriptive Statistik:} Die Deskriptive Statistik\marginnote{Deskriptive Statistik = Abbildungen, Visualisierung, Kennzahlen der Statistik} beinhaltet beschreibende Analysen (Kennzahlen), wie beispielsweise Mittel- oder Medianwert, Korrelation, oder Assoziation, aber auch die Beschreibung von Daten durch visuelle Methoden (Abbildungen, interaktive Visualisierung).
	
	\paragraph{Schließende Statistik:} Die schließende Statistik\marginnote{Schließende Statistik = Parameterschätzer, Hypothesentests, p-Werte, Modellvergleiche} (auch: induktive oder inferentielle Statistik) befasst sich damit, wie man aus Beobachtungen allgemeine Schlussfolgerungen ziehen kann. Das Problem hierbei ist, dass Beobachtungen immer ein gewisses Maß an zufälligen "Rauschen" aufweisen, ohne dass dies notwendigerweise auf einen systematischen Effekt zurückzuführen ist.\marginnote{Systematischer Effekt ist z.B. die Tatsache dass Patienten schneller heilen wenn Sie ein bestimmtes Medikament bekommen. Unterschiede können aber auch aus zufälligen Unterschieden (jeder Patient unterscheidet sich, unabhängig vom Medikament) kommen.} Um herauszufinden ob Effekt in den Daten systematische oder zufällige Ursachen haben, macht man in der schließenden Statistik typischerweise eine Reihe von Annahmen, die in einem statistischen Modell zusammengefasst werden. Man sagt das statistisches Modell beschreibt den "`datengenerierenden Prozess"', indem es unsere Annahmen darüber zusammenfasst wie Variation und Muster in den Daten erstehen.\marginnote{Ein statistisches Modell beschreibt, wie Daten erstellt werden = datengenerierender Prozess} Auf Grundlage des Modells kann die schließende Statistik dann Wahrscheinlichkeitsaussagen für bestimmte Fragestellungen machen (z.B. wie wahrscheinlich wäre es die beobachteten Daten zu sehen, wenn man annimmt dass kein systematischer Effekt da ist). 
	
	\paragraph{Prädiktive Statistik und Maschinelles Lernen:} Prädiktive Statistik und maschinelles Lernen\marginnote{Maschinelles Lernen = Modelle deren Zweck vor allem Vorhersagen sind} bezeichnet Methoden die nicht auf Erkenntnis von Zusammenhängen, sondern auf Vorhersagen optimiert sind. Diese Methoden sind besonders wichtig im kommerziell sehr wichtigen Feld der "Big Data". Big Data bezeichnet Methoden um Statistik auf sehr großen Datensätzen zu betreiben, wie  z.B. die Datensätze die von vielen Internetfirmen (Google, Amazon) gesammelt werden.\marginnote{Big Data = Methoden für große Datensätze, wie sie z.B. von vielen Internetfirmen (Google, Amazon) gesammelt werden} Viele Methoden der prädiktiven Statistik benutzen komplexe Algorithmen, die automatisch tausende von möglichen erklärenden Variable in einem Modell kombinieren. Diese Algorithmen machen oft hervorragende Vorhersagen, aber die Modell sind so komplex dass sich schwer verstehen lässt warum. Für wissenschaftliche Grundlagenforschung, in der man normalerweise an einem bestimmten Zusammenhang interessiert ist, werden daher eher schließende statistische Methoden verwendet. Firmen wie Amazon und Google konzentrieren sich jedoch eher auf prädiktive Methoden, da es hier darum geht aus großen Datenmengen möglichst genaue Vorhersagen zu machen, z.B. was Kunden interessiert, ohne dass das "`warum"' im Vordergrund steht. 
	
	\paragraph{Versuchsplanung:} Versuchsplanung oder experimentelles Design\marginnote{Versuchsplanung oder experimentelles Design = wie erzeugt man gute Daten?} umfasst alle Aspekte der Datenerzeugung, insbesondere Fragen wie "`Welche Variablen sollten erfasst werden?"', "`Wie viele Replikate werden benötigt?"', "`Wie sollten die Variablen in einem Experiment optimalerweise verändert werden?"'. Versuchsplanung ist offensichtlich von großer Wichtigkeit für die empirisch arbeitenden Wissenschaften (sowohl die Naturwissenschaften, als auch die empirischen Sozialwissenschaften). 
	
	\section{Die R Umgebung für wissenschaftliches Rechnen}
	
	Die Zeiten als man statistische Analysen mit Papier und Bleistift, oder später mit einem Taschenrechner durchführen konnte sind vorbei. Heute werden statistische Analysen praktische ausschließlich am Computer durchgeführt. Hierfür gibt es eine Reihe von wichtigen Programmen, die alle verschieden Stärken und Schwächen haben \marginnote{Eine Aufstellung von verschiedenen Statistikprogrammen finden sie \href{https://www.inwt-statistics.de/blog-artikel-lesen/Statistik-Software-R_SAS_SPSS_STATA_im_Vergleich.html}{hier}}.
	
	In diesem Text werden alle Beispiele mit dem Statistikprogramm R berechnet. Die Vorteile von R sind:
	
		\begin{itemize}
		\item Es ist kostenlos (open source)
		\item Es ist eine Skriptsprache 
		\item Im Bereich Biologie / Ökologie hat R mit Abstand den größten Funktionsumfang aller verfügbaren Programme
		\item Außerdem ist es in diesem Bereiche das mit Abstand am weitesten verbreitete Programm
	\end{itemize}
	

Also, R ist einfach der Standard, und Sie sollten sich an R gewöhnen wenn Sie es nicht schon kennen. Wir werden in diesem Text immer wieder Beispiele mit R sehen, und mehrere Appendices am Ende zeigen, wie man die hier behandelten Inhalt in R berechnen würde. Trotzdem will dieser Text keine Einführung in die Spezifika der Sprache R geben. Hierzu gibt es schon viele andere Anleitungen. Ein Übersicht ist  \href{http://biometry.github.io/APES/R/R10-gettingStarted.html}{hier} verfügbar, inklusive Hilfestellungen zur Installation von R und zusätzlicher Software wir RStudio.
	
	\begin{figure}[]
		\begin{center}
			\includegraphics[width = 10cm]{rst_interface.png}
			\caption{RStudio ist der wohl beliebteste Editor für R. R ist eine script-basierende Sprache. Das heißt man kommuniziert nicht über Mauseingaben mit dem Computer, sondern anhand von schriftlichen Befehlen in der R Konsole, bzw. über ein Textdokuments, welches dann an die R Konsole geschickt wird. Nach kurzer Eingewöhnungszeit merkt man, wie komfortabel und vorteilhaft diese Vorgehensweise ist - so sind alle Schritte der Analyse in einem Textdokument aufgelistet, und man kann leicht die komplette Analyse nachvollziehen, oder wiederholen wenn Änderungen gemacht werden müssen.}
			\label{fig: Rstudio1}
		\end{center}
	\end{figure}
	

Der Rest dieser Einführung befasst sich mit den vier Grundthemen der Statistik, die in der Einführung genannt wurden: deskriptive Statistik, schließende Statistik, prädiktive Statistik und Versuchsplanung. Bevor wir uns diesen Themen widmen wollen wir uns aber erst noch einmal genauer mit dem befassen worum sich in der Statistik alles dreht: die Daten.
	
	\chapter{Daten, Zufall, und der datengenerierende Prozess}
	
		\section{Die Daten: Darstellung und Skalenniveaus}
	
	Ein typisches Experiment oder eine typische Beobachtungsstudie (mehr zu dem Unterschied später) liefert wiederholte Beobachtungen mehrerer Variablen (beispielhaft: Temperatur, Niederschlag, Produktivität der Vegetation an verschiedenen Standorten). Man kann sich einen solchen Datensatz als eine Tabelle vorstellen, in der jede Variable eine Spalte einnimmt, und jede Zeile einer Beobachtung entspricht. Natürlich gibt es noch viele andere mögliche Datenstrukturen (z.B. hierarchische Beziehungen / Netzwerke), aber die Tabellenform ist mit Abstand das häufigste Ergebnis von einfachen Experimenten.
	
Üblicherweise wird eine wissenschaftliche Studie durchgeführt um eine bestimmte Frage zu untersuchen. Das heißt es gibt eine Variable die uns besonders interessiert, und von der wir wissen wollen wie sie von anderen Variablen beeinflusst wird. \marginnote{Abhängige Variable = unser Fokus, Frage ist wie diese Variable von anderen Faktoren beeinflusst wird.}  Diese Variable wird als ``abhängige Variable'' bezeichnet (auch Antwort- oder Responsevariable), da wir wissen wollen, ob und wie sie von anderen Variablen abhängt. Anderen Variablen, die unsere Antwortvariable beeinflussen, nennt man "`erklärende Variablen"' (auch: Prädiktoren, Kovariaten oder unabhängige Variablen).\marginnote{Erklärende Variablen sind variable die potentiell einen Einfluss auf die abhängige Variable ausüben.} Ein Beispiel: in einer medizinischen Studie ist der Heilungserfolg typischerweise die abhängige Variable, während die Information ob ein Medikament gegeben wurde (Behandlung) oder nicht (Kontrolle) die erklärende Variable ist. 

Normalerweise \marginnote{Die Methoden der multivariaten Statistik befassen sich mit Situationen, in denen wir mehrere Variablen als abhängig betrachten} sind wir daran interessiert, wie eine Variable von externen Bedingungen abhängt. Es gibt aber auch Fälle in denen man gerne die Abhängigkeit von mehreren Variablen mit sich selbst, oder externen Faktoren betrachten will. Ein Beispiel wäre wie sich Arteigenschaften (Gewicht, Physiologie, ...) mit der Umwelt verändern. Das Auswerten solcher Daten wird als multivariate Statistik bezeichnet. Wir werden solche Methoden nur am Rande erwähnen - weitere Informationen über multivariate Statistik ist \href{http://biometry.github.io/APES/Stats/stats50-MultivariateStatistics.html}{hier} verfügbar.

Neben der Unterscheidung in abhängige und unabhängige Variablen unterscheiden sich Variable außerdem in ihrem Typ. Eine numerische Variable (z.B. Gewicht) ist konzeptionell anders als eine Variable, die nur entweder rot, grün, oder blau wird. Der Fachbegriff in der Statistik für den Typ ist das Wort "`Skalenniveau"'.\marginnote{Skalenniveau = Variablentyp. Wichtige Typen: ungeordnet (nominal), geordnet (ordinal), numerisch (metrisch)} Man unterscheidet die folgenden Typen:
	
	\begin{itemize}
	  \item Numerische (metrische) Variablen - metrische Variablen erkennt man daran dass sie sich wie Distanzen verhalten, z.B. können Sie in sinnvoller Weise addiert werden. Die meisten Messgrößen im Labor (Temperatur, Konzentration, Zeit, Längen etc. sind metrisch). Zwei für die Statistik wichtige Untertypen sind
		\begin{itemize}
  		\item Kontinuierliche metrische Variablen, z.B. Temperatur
  		\item Ganzzahlig metrische Variablen. Hierzu zählt der wichtige Spezialfall der Zähldaten, z.B. 0,1,2,3, ...
		\end{itemize}	  
		\item Geordnete (ordinale) Variablen (klein, mittel, groß)
		\item Ungeordnete (nominale) Variablen (z.B. rot, grün, blau). Ein Spezialfall sind die binären (dichotome) Variablen (z.B. verstorben / überlebt) die oft durch 0/1 codiert werden,
	\end{itemize}
	
	Ordinale und nominal Variable werden manchmal auch als "kategorial" bezeichnet, weil sie sie nur eine feste Auswahl von Werten (Kategorien) annehmen. 
	
	Bei statistischen Auswertungen ist es äußerst wichtig sich klarzumachen, welches Skalenniveau die verschieden Variable haben. Wie wir später lernen werden, bestimmt das Skalenniveau welche Methoden der deskriptiven und schließenden Statistik angewandt werden können. Falls Sie ein Statistikprogramm (z.B. R) benutzen, wird dieses Programm typischerweise versuchen automatisch festzustellen, welches Skalenniveau vorliegt (wenn es eine Spalte mit "rot", "grün" gibt kann das offensichtlich nicht numerisch sein), und je nach Skalenniveau automatisch bestimmte Entscheidungen für Abbildungen und statistisch Verfahren treffen.\marginnote{Bei der Arbeit am Computer muss man immer prüfen, ob den Variablen das richtige Skalenniveau zugewiesen wurde.} In vielen Fällen ist es aber nicht möglich automatisch zu erkennen ob sie, z.B., eine Variable nominal oder ordinal behandeln wollen. Nach dem Einlesen der Daten in ein Statistikprogramm müssen Sie deshalb immer kontrollieren, ob das Skalenniveau jeder Variable korrekt kodiert ist. 
	
	Ein weitere Tip: die Erfahrung zeigt, dass viele Studenten dazu neigen, Variablen die eigentlich numerisch sind kategorial zu codieren. Als Beispiel: gemessen wird das Gewicht eines Tieres, aufgeschrieben als leicht, mittel und schwer.\marginnote{Verwende niemals eine kategoriale Codierung für Messgrößen, die eigentlich metrisch sind!} Die Idee die ich oft als Erklärung höre ist dass die Messung ungenau war und die kategoriale Einteilung dem Rechnung tragen würde. Kurz gesagt: Das tut es ganz und gar nicht. Die kategoriale Einteilung erzeugt nur zusätzliche Probleme. Verwende niemals eine kategoriale Codierung für Messgrößen, die eigentlich metrisch sind, egal wie groß der Messfehler ist!

	\section{Der Zufall: Stichprobe, Populationen und der datengenerierende Prozess}
	
	Daten sind der eine Grundpfeiler der Statistik. Der andere ist der Zufall.\marginnote{Daten entstehen aus einer Mischung von systematischen und zufälligen Prozessen.} Die Idee ist dass es systematische Beziehungen in und zwischen Variablen in den Daten gibt, über die wir gerne etwas lernen würden. Diese Beziehungen sind aber von zufälliger Variabilität in den beobachteten Variablen überdeckt. Um die systematischen von den zufälligen Prozessen zu trennen brauchen wir die Methoden der Statistik, insbesondere der schließenden Statistik. Mehr dazu später. Jetzt wollen wir uns zuerst mit der Frage beschäftigen: wie entsteht überhaupt der Zufall in den Daten?
	
	Stellen Sie sich vor,\marginnote{Die Population ist die Menge aller Beobachtungen, die man machen können. Die Stichprobe sind die Beobachtungen, die tatsächlich gemacht wurden.} dass wir gerne wissen würden wie schnell ein Baum in Deutschland im Durchschnitt wächst. Um diese Frage so exakt wie möglich zu beantworten würde man idealerweise alle Bäume in Deutschland wiederholt messen. Dies ist aber offensichtlich unpraktikabel. Also misst man nur eine kleine Menge von normalerweise zufällig ausgewählten Bäumen, und hofft dass die Eigenschaften dieser Bäume ähnlich (=repräsentativ) für die Eigenschaften aller deutschen Bäume sind. Der statistische Terminus für 'alle Bäume in Deutschland' lautet "`Population"' und die Bezeichnung der ausgewählten gemessenen Bäume ist die "`Stichprobe"'.
		
	Ich hoffe es ist einsichtig, dass sich die Stichprobe\marginnote{Stichproben erzeugt Zufälligkeit.} bei repräsentativer Auswahl (mehr dazu im Kapitel zur Versuchsplanung) ähnlich verhält wie die ganze Population. Das heißt wenn wir statistische Berechnungen auf der Stichprobe ausführen können wir erwarten, dass die resultierenden Werte der Population ähnlich. Die Vorstellung ist dass die Werte der Stichproben um den wahren Wert der Population schwanken. Um ein Beispiel zu geben kehren wir zu den Bäumen zurück: aus logistischen Gründen können wir jedes Jahr nur eine Stichprobe von 1000 Bäumen untersuchen. Wenn wir die Wachstumsrate dieser 1000 Bäume berechnen wird sie ähnlich, aber nicht gleich der Wachstumsrate der Population aller Bäume in Deutschland sein, und wenn wir das jedes Jahr wiederholen wird der Wert der Stichprobe in einem Jahr über, und in einem anderen Jahr unter der Rate der ganzen Population sein. 
	
	Der Prozess der\marginnote{Die Stichprobe ist nur einer der vielen Prozesse die Zufälligkeit erzeugen.} Stichprobe ist eine Erklärung wie der Zufall in unsere Daten entsteht. Viele klassische statistische Lehrbücher konzentrieren sich ausschließlich auf diesen Prozess, wodurch sich das Problem der Statistik auf das Problem reduziert die Eigenschaften der Population aus den Eigenschaften der Stichprobe zu schätzen. Leider stößt diese relativ einfache Denkmodell bei vielen komplexeren statistischen Problem an seine Grenzen. Stellen Sie sich z.B. vor Sie erhalten Daten die durch den folgenden Prozess erstellt wurden: eine Person geht zu zufällig ausgewählten Punkten in einem Wald und ermittelt die Strahlung die von der Vegetation absorbiert wird. Beim Messprozess entsteht Variation durch die innerhalb von Minuten wechselnde Bewölkung, der Fehler des Messinstruments, und dadurch dass die messende Person manchmal Ablesefehler macht. Man kann sich die Erzeugung dieser Daten natürlich trotzdem als das Ziehen aus einer abstrakten Population vorstellen, aber meiner Erfahrung nach führt diese Idealisierung oft zu Verwirrung. 
	
	
	Eine moderneres und allgemeineres Gedankenmodell für die Entstehung von Daten ist der "`datengenerierende Prozess"'.\marginnote{Der datengenerierende Prozess beschreibt, wie Zufälligkeit in einer Stichprobe entsteht.} Der datengenerierende Prozess beschreibt, wie die Beobachtungen aus einer Reihe systematischer oder zufälliger Prozesse entstehen. Dies beinhaltet den Prozess der Stichprobenerhebung, aber auch die vielen anderen Vorgänge, die zu systematische und zufällige Mustern in den Daten führen. Mit dieser Vorstellung können wir jetzt sagen: das Ziel der Statistik ist es, aus den Daten auf die Eigenschaften des datengenerierenden Prozesses zu schließen. 
	
	Egal ob man die klassische Vorstellung der Stichprobe, oder die flexiblere Vorstellung des datengenerierenden Prozesses präferiert: die Grundidee ist, dass die Daten / Stichprobe durch eine Mischung aus systematischen und zufälligen Prozessen entstehend. Man muss deshalb gedanklich zwei Objekte strikt trennen:\marginnote{Die deskriptive Statistik beschreibt die Daten / Stichprobe, während die schließende Statistik die Eigenschaften der unterliegenden Population / des datengenerierenden Prozesses beschreibt.} die Eigenschaften der Daten / Stichprobe (z.B. Mittelwert, Minimum, Maximum), und die Eigenschaften der Population oder des datengenerierenden Prozesses. Daten / Stichprobe sind uns vollständig bekannt. Was zu tun bleibt ist diese Information möglichst kompakt darzustellen. Dies ist die Aufgabe der deskriptiven Statistik, mit der wir uns im nächsten Kapitel beschäftigen werden (siehe Kapitel~\ref{ch: deskriptive Statistik}). Die Eigenschaften der Population / des datengenerierenden Prozesses aber sind uns nicht direkt bekannt. Wir müssen sie aus den Daten schätzen. Dies ist die Aufgabe der schließenden Statistik (siehe Kapitel~\ref{ch: inductive statistics}). Die prädiktive Statistik ist eine Mischung von beiden Standpunkten: Ziel hier ist es aus einer Stichprobe die Eigenschaften der nächsten Stichprobe vorherzusagen, ohne aber notwendigerweise explizit die Population oder der datengenerierenden Prozess zu schätzen. 
	
	
	\vspace{1cm}
	\begin{fullwidth}
		\begin{mdframed}
			
			\textbf{Anwendung in R:} 
			
			Die grundlegende Datenstruktur in R ist der "data.frame". Ein data.frame ist eine Datentabelle, wobei jede Spalte einen anderen Datentyp (Skalenniveau) haben kann. Mögliche Typen sind:
			
			\begin{itemize*}
				\item integer - ganzzahlig
				\item numeric - kontinuierliche Zahlen (Gleitkommazahl)
				\item boolean - wahr / falsch
				\item factor - kategoriale Variablen. Der Standard ist ungeordnet (z.B. rot, grün, blau), aber man kann auch einen geordneten factor (z.B. klein, mittel, groß) erzeugen.
			\end{itemize*}
			
			Außerdem taucht in realen Datensätzen oft der Wert "NA" auf. Dies ist die Bezeichnung für eine fehlende Beobachtung. Ähnlich, jedoch nicht identisch, ist "`NaN"' (not a number), was bei einer nicht ausführbaren Rechnung als Ergebnis auftreten kann.
			
			Der einfachste Weg um Daten als data.frame in RStudio einzulesen ist der "Import Dataset" im oberen rechten Teil des RStudio Editors. Natürlich geht es aber auch per Skript. Genaueres kann unter "Handling data in R " im Anhang~\ref{HandlingDataInR}, oder auf \href{http://biometry.github.io/APES/R/R20-DataStructures.html}{der Seite hier} nachgelesen werden.
			
			\textbf{Wichtig:} Nachdem die Daten eingelesen wurden muss man immer überprüfen ob die Werte und auch der Typ richtig eingelesen wurde. Ein häufiges Problem ist das numerische Werte als factor eingelesen werden (z.B. reicht ein versehentlicher Buchstabe in den Rohdaten so dass R die Spalte als Faktor interpretiert). Um die Daten zu überprüfen ist die R Funktion str(), also str(TheNameOfMyData) hilfreich. Der str Befehl steht für "structure", und zeigt dem Benutzer die Struktur inklusive Typ eines Datenobjektes an. 			
		\end{mdframed}
	\end{fullwidth}
	
	
	\chapter{Deskriptive Statistik und Visualisierung}\label{ch: deskriptive Statistik}
	
	Deskriptive Statistik\marginnote{Wie man Daten erstellt wird später noch detaillierter im Kapitel ~\ref{ch: design of experiments} Versuchsplanung behandelt.} behandelt die Zusammenfassung und Veranschaulichung von Daten.
	
	\section{Kennzahlen der Statistik}
	
	Kennzahlen der Statistik\marginnote{Kennzahlen der Statistik fassen Daten numerisch zusammen} bezeichnen Rechenvorschriften (z.B. Mittelwert oder Standardabweichung), die Eigenschaften von Datensätzen numerisch zusammenfassen. Kennzahlen dienen also zur kompakten Veranschaulichung der Eigenschaften eines Datensatzes, oder zu deren Vergleich.
	
	\subsection{Univariate Verteilungen - Kennzahlen einer Variable}
	
	Ein Grundbegriff in diesem Zusammenhang ist die Verteilung. Die Verteilung einer oder mehrerer Variablen beschreibt die Häufigkeit des Vorkommens verschiedener Werte dieser Variablen. Genau genommen muss man zwei Verteilungen unterscheiden: die Verteilung der Population / des datengenerierenden Modells, und die Verteilung der Stichprobe. 	
	
	
	Eine übliche Situation bei der wir von den Kennzahlen der Statistik Gebrauch machen, ist die wiederholte Beobachtung einer stetigen Variable. Als Beispiel kann man sich hier vorstellen, man habe 2000 Bäume gemessen und anschließend erneut vermessen. Man konnte eine Vorkommen an zunehmendem Durchmesser beobachten (siehe Abb.\ref{fig: data distribution}).


\begin{figure}[htbp]
\begin{center}
<<echo=FALSE>>=
seed = (123)
parameter = seq(-2,8,len=500)

mix <- rbinom(2000,1,0.5)
samples = mix * rnorm(2000, mean = 3) + (1-mix)*rnorm(2000, mean = 0.5, sd=0.5)

distr = (dnorm(parameter, mean = 3) + dnorm(parameter, mean = 0.5, sd=0.5)) /2

plot(parameter,distr, type = "l", ylab = "Frequency", xlab = "Diameter growth [cm]", main = "Tree diameter growth data", col = "red", lwd = 2, ylim = c(0,0.5), lty = 2)
hist(samples, breaks = 40, add = T,freq = F, col = "#99999930")
# polygon(parameter, distr, border=NA, col="darksalmon")

MLEEstimate <- parameter[which.max(distr)]
# abline(v=MLEEstimate, col = "red")
#text(3.1,0.4, "Mode", col = "red", cex = 1.5)
@
\caption{Eine Verteilung von beobachteten zunehmenden Durchmessergrößen (graue Balken). Wir nehmen an (in diesem Fall wissen wir es), dass diese Werte von wahren Verteilungen stammen (Population oder datengenerierender Prozess) die hier in einer rot-gestrichelten Linie angedeutet werden. Wenn wir nun mehr Daten aufzeichnen würden, würden sich die grauen Balken der wahren Verteilung immer mehr annähern.}
\label{fig: data distribution}
\end{center}
\end{figure}


Wie kann man nun die Eigenschaften der beobachteten Stichprobe zusammenfassen? Ein paar grundlegende Eigenschaften wären beispielsweise das Minimum und das Maximum, der Mittelwert, oder der Modalwert (die Maximalverteilung, d.h. der Wert der höchsten Beobachtungsdichte). Außerdem  gibt es zwei weitere Kennzahlen, die ebenso oft Gebrauch finden: Moment und Quantil.


Der Begriff "`Moment"' mag vielleicht nicht jedem geläufig sein, vermutlich wurde aber schon einmal das erste und zweite Moment der Verteilung berechnet. Diese sind ebenfalls unter den Bezeichnungen Mittelwert und Standardabweichung bekannt. Allgemein wird das n-te Moment $\mu_n$ einer Verteilung $f(x)$ um einen Wert c definiert als 

\begin{equation}
\mu_n(c) = \int_{-\infty}^{\infty} f(x) (x - c)^n dx
\end{equation}

oder, für eine endliche Anzahl an Beobachtungen 

\begin{equation}
\mu_n(c) = \frac{1}{N}\sum_{i=1}^N (x_i - c)^n
\end{equation}

Das erste Moment mit $c=0$, ist der Mittelwert. Für die folgenden höheren Momente ist es üblich die Zentralmomente zu betrachten, die man durch das Setzen von c auf den Mittelwert erhält, da ihre Werte einfacher als Indikatoren für die Verteilungsform zu interpretieren sind.\footnote{Um die Varianz abzuschätzen, ersetzt man oft den Term 1/N durch den Bias-korrigierten Term 1/(N-1).} Die drei höheren Zentralmomente werden Varianz (n=2, identisch mit dem Quadrat der Standardabweichung, Messgröße der Streuung), Schiefe (n=3, Messgröße für die Asymmetrie in der Verteilung) und Wölbung (n=4) genannt. 

Quantilen stellen die zweite Zentralklasse der Kennzahlen-Statistik dar, um die kontinuierliche Verteilung zu beschreiben. Wenn wir eine Verteilung wie in der oben gezeigten Abbildung vorfinden, kann man sich die Frage stellen: Welcher ist der entscheidende Wert, der den Datensatz in zwei Hälften trennt, sodass eine Hälfte der beobachten Daten kleiner und die andere Hälfte größer als dieser Wert ist?\marginnote{Eine Hälfte der Daten ist kleiner und die andere Hälfte ist größer als die 0.5 Quantile, welches auch als Medianwert bezeichnet wird} Dieser Punkt wird als Medianwert, oder auch als 0.5-Quantil bezeichnet. Allgemeiner kann man sagen, dass die 0.x Quantile der Wert ist, an dem der Anteil 0.x des Datensatzes kleiner ist. 

\subsection{Korrelation - Erläuterung der Abhängigkeit stetiger Variablen}

Ein weiterer wichtiger Bereich der Kennzahlen-Statistik ist die Korrelation. Korrelationsstudien messen die Abhängigkeit von stetigen Variablen. Leider gibt es eine Menge Korrelations-Messgrößen, die es zu unterscheiden gilt. Die zwei wichtigsten sind:

\paragraph{Lineare Koeffizienten:}Lineare Koeffizienten, mit dem "`Pearson'schen Korrelationskoeffizienten"', als dem wohl Häufigsten, ermitteln die lineare Abhängigkeit zwischen zwei Variablen. Der Pearson'sche Korrelationskoeffizient wird wegen seiner schnellen Berechnung und leichten Interpretation am häufigsten verwendet. Jedoch kann es hier zu Fehlern kommen, wenn die Variablen nicht linear abhängig voneinander sind. Fig.~\ref{fig: correlation}.

\paragraph{Rangkorrelationskoeffizient:} Rangkorrelationskoeffizient, wie zum Beispiel der "`Spearman'sche Rangkorrelationskoeffizient"' und der "`Kendall Tau Rangkorrelationskoeffizient"' ermitteln, wie genau die Variablen gemeinsam tendenziell steigen oder fallen, ohne hierbei das Ausmaß oder die Linearität ihrer Steigung zu beachten. Diese werden bevorzugt, wenn man von zueinander nicht-linearen Variablen ausgeht. 

\paragraph{Starke Korrelation != bedeutender Einfluss:} Siehe auch Fig.~\ref{fig: correlation}; ist ein oft falsch verstandener Zustand der Korrelation und Abhängigkeit - ein hoher Korrelationskoeffizient bedeutet nicht, dass eine Variable eine starke Reaktion auf eine andere bewirkt. Um eine hohe Korrelation zu erreichen braucht es nur eine geringe Streuung um die Korrelationslinie (siehe mittlere Reihe - die Auswirkung ist anders, aber die Korrelation bleibt gleich). 


\begin{figure}[htbp]
\begin{center}
<<echo = F>>=
# From https://en.wikipedia.org/wiki/File:Correlation_examples2.svg

#Title: An example of the correlation of x and y for various distributions of (x,y) pairs
#Tags: Mathematics; Statistics; Correlation
#Author: Denis Boigelot
#Packets needed : mvtnorm (rmvnorm), RSVGTipsDevice (devSVGTips)
#How to use: output()
#
#This is an translated version in R of an Matematica 6 code by Imagecreator.

library(mvtnorm)

MyPlot <- function(xy, xlim = c(-4, 4), ylim = c(-4, 4), eps = 1e-15) {
   title = round(cor(xy[,1], xy[,2]), 1)
   if (sd(xy[,2]) < eps) title = "" # corr. coeff. is undefined
   plot(xy, main = title, xlab = "", ylab = "",
        col = "darkblue", pch = 16, cex = 0.2,
        xaxt = "n", yaxt = "n", bty = "n",
        xlim = xlim, ylim = ylim)
}

MvNormal <- function(n = 1000, cor = 0.8) {
   for (i in cor) {
      sd = matrix(c(1, i, i, 1), ncol = 2)
      x = rmvnorm(n, c(0, 0), sd)
      MyPlot(x)
   }
}

rotation <- function(t, X) return(X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2))

RotNormal <- function(n = 1000, t = pi/2) {
   sd = matrix(c(1, 1, 1, 1), ncol = 2)
   x = rmvnorm(n, c(0, 0), sd)
   for (i in t)
      MyPlot(rotation(i, x))
}

Others <- function(n = 1000) {
   x = runif(n, -1, 1)
   y = 4 * (x^2 - 1/2)^2 + runif(n, -1, 1)/3
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1/3, 1+1/3))

   y = runif(n, -1, 1)
   xy = rotation(-pi/8, cbind(x,y))
   lim = sqrt(2+sqrt(2)) / sqrt(2)
   MyPlot(xy, xlim = c(-lim, lim), ylim = c(-lim, lim))

   xy = rotation(-pi/8, xy)
   MyPlot(xy, xlim = c(-sqrt(2), sqrt(2)), ylim = c(-sqrt(2), sqrt(2)))
   
   y = 2*x^2 + runif(n, -1, 1)
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1, 3))

   y = (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   y = cos(x*pi) + rnorm(n, 0, 1/8)
   x = sin(x*pi) + rnorm(n, 0, 1/8)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   xy1 = rmvnorm(n/4, c( 3,  3))
   xy2 = rmvnorm(n/4, c(-3,  3))
   xy3 = rmvnorm(n/4, c(-3, -3))
   xy4 = rmvnorm(n/4, c( 3, -3))
   MyPlot(rbind(xy1, xy2, xy3, xy4), xlim = c(-3-4, 3+4), ylim = c(-3-4, 3+4))
}

 par(mfrow = c(3, 7), oma = c(0,0,0,0), mar=c(2,2,2,0))
 MvNormal(800, c(1.0, 0.8, 0.4, 0.0, -0.4, -0.8, -1.0));
 RotNormal(200, c(0, pi/12, pi/6, pi/4, pi/2-pi/6, pi/2-pi/12, pi/2));
 Others(800)

@
\caption{Beispiele einer möglichen Korrelation mit dem Pearson'schen Korrelationskoeffizienten. Beachte, dass viele Datensätze, die eine klare Abhängigkeit zwischen Variablen aufweisen, einen Pearson'schen Korrelationskoeffizienten von 0 aufweisen, da die Abhängigkeit nicht linear ist.}
\label{fig: correlation}
\end{center}
\end{figure}

\subsection{Kreuztabellen - Erläuterung unstetiger Ergebnisse mehrerer Variablen}

Zu guter Letzt\marginnote{Dieser Datensatz von Berkeley ist ein berühmtes Beispiel für das Simpson-Paradoxon. Weitere Informationen auf Wikipedia über diese wichtige statistische Falle.} gibt es noch die klassischen Kreuztabellen, um binäre oder kategorische Daten zu beschreiben. Hier ist ein Auszug eines üblichen Datensatzes aus R über Sammeldaten von Hochschulbewerber in Berkeley aus dem sechst-größten Departments im Jahre 1973, geordnet nach Zulassung und Geschlecht. Hier wird nur das erste Department gezeigt.

<<>>=
UCBAdmissions[,,1]
@


\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Für die vielseitigen Berechnungsmöglichkeiten der deskriptiven Statistik in R, siehe \href{http://www.uni-kiel.de/psychologie/rexrepos/rerDescriptive.html}{hier}

\end{mdframed}
\end{fullwidth} 


\section{Visualisierung}

\marginnote{Gib ?anscombe in R ein, um den Code zu sehen, der diese Plots ausgibt und um die statistischen Eigenschaften des Datensatzes zu berechnen.}

Kennzahlen sind sehr nützlich, können aber auch zu Fehlinterpretationen führen. Ein bekanntes Beispiel dafür ist das Anscombe Quartett, ein hypothetischer Datensatz aus vier Stichproben, die identisch bzgl. üblicher Kennzahlen wie z.B. Mittelwert, Varianz, Korrelation, Regressionslinie, etc. sind, aber sehr unterschiedliche Verteilungen aufweisen \citep{Anscombe-Graphsinstatistical-1973}. Das Beispiel zeigt, dass die Berechnung von einfachen Kennzahlen kein vollständiger Ersatz für die graphische Darstellung der Daten darstellt.


\begin{figure}[htbp]
\begin{center}
<<echo = F, results="hide">>=

require(stats); require(graphics)
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  print(anova(lmi))
}

## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))

## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)

@
\caption{Anscombe's Quartet, ein hypothetischer Datensatz aus vier Beobachtungen, ein hypothetischer Datensatz aus vier Stichproben, die identisch bzgl. üblicher Kennzahlen wie z.B. Mittelwert, Varianz, Korrelation, Regressionslinie, etc. sind, aber sehr unterschiedliche Verteilungen aufweisen.}
\label{fig: Anscombes Quartet}
\end{center}
\end{figure}

\subsection{Grundsätze der Visualisierung}

Ein Grundsatz\marginnote{Beispiele für missverständliche Abbildungen \href{https://en.wikipedia.org/wiki/Misleading_graph}{siehe}} von Abbildungen und Darstellungen ist, die Daten so einsehbar und wahr wie möglich darzustellen. Der Leser sollte einen bestmöglichen überblick in kürzester Zeit über die Daten erhalten. Zusätzlich sollten die Graphen natürlich auch anschaulich sein. Vorab ein paar generelle Tipps, die vielleicht helfen könnten:

\begin{itemize}
\item Einfach ist besser als kompliziert
\item Vermeide übertriebene Farben. Graphen sollten wenn möglich auch in schwarz-weiß leserlich sein (benutze einen Farbgradienten, der gleichzeitig ein Intensitätsgradient ist; benutze zusätzlich zu Farben auch gestrichelte Linien). Wenn dein Graph auf Farben basiert, achte darauf welche zu verwenden, die auch Menschen mit einer Rot-Grün-Schwäche erkennen können.
\item Ehrlichkeit: vermeide Verzerrungen. Benutze quadratische Formen, außer es gibt besondere Gründe. Achsen sollten bei 0 beginnen, außer es gibt gute Gründe, die dagegen sprechen. Verwende die gleiche Skala bei der Darstellung mehrerer Abbildungen, wenn es keine Gründe gibt, die dagegen sprechen. 
\item Manipuliere deine Abbildungen nicht!
\item Ausgabe in einem Vektor-Format (pdf, eps, svg)
\end{itemize}

\begin{figure}[htbp]
\begin{center}

\setkeys{Gin}{width=\textwidth}
<<echo = FALSE>>=
par(mfrow = c(2,2))

plot(co2, ylab = expression("Atmospheric concentration of CO"[2]),
     las = 1)
title(main = "a) Athmospheric CO2 Hawai")

plot(airquality$Temp, airquality$Ozone, xlab = "Temperature", ylab = "Ozone", main = "b) Temperature vs. Ozone")

barplot(table(mtcars$gear, mtcars$cyl), beside=T, xlab = "cylinders", main = "c) Gears / cylinders of cars")
legend("top", legend = c("3 gears", "4 gears", "5 gears"), pch = 15, col = gray.colors(3, end = 0.7), bg = "#FFFFFF66", bty = "n")

boxplot(weight ~ group, data = PlantGrowth, main = "d) Plant Growth",
        ylab = "Dried weight of plants", col = "lightgray",
        notch = F, names = c("control", "classical music", "heavy metal"), las = 1)
@
\caption{Vier typische Plot-Typen, von oben links nach unten rechts: a) Liniendiagramm, repräsentiert stetige Messungen einer Variablen; b) Streudiagramm, repräsentiert eine Beziehung zwischen zweier stetigen Variablen; c) Balkendiagramm, repräsentiert Messungen in unstetigen Gruppen / Variablen; d) Boxplot, repräsentiert stetige Messungen in unstetigen Gruppen.}
\label{fig: exaple plots}
\end{center}
\end{figure}


\subsection{Graphen-Typen}

Es gibt eine große, fast unendliche Bandbreite an verschiedenen visuellen Darstellungen von Datensätzen. Im Folgenden werden nun vier häufig verwendete Graph-Typen gezeigt. 

\paragraph{Liniendiagramme:} Liniendiagramme werden benutzt, um stetige, geordnete Messwerte darzustellen. Typische Beispiele hierfür wären Zeitreihen, stetige Parameterveränderungen oder mathematische Funktionen. Beispiel siehe Fig.~\ref{fig: exaple plots} a.

\paragraph{Streudiagramme:} Streudiagramme veranschaulichen zwei stetige Variablen die paarweise gemessen wurden. Ein typisches Beispiel hierfür wäre das wiederholte Messen von verschiedenen Variablen mit der Absicht herauszufinden, ob diese korrelieren. Beispiel siehe Fig.~\ref{fig: exaple plots} b.

\paragraph{Balkendiagramme:} Balkendiagramme beinhalten Informationen (Zählungen oder stetige Variablen) über unstetige Gruppen. Beispiel siehe Fig.~\ref{fig: exaple plots} c.

\paragraph{Boxplots:} Boxplots benutzt man häufig bei der Verteilung einer stetigen Variable über mehrere unstetige Gruppen. Klassischerweise bestehen sie aus einer Box, 'Whiskers' (Linien) und gegebenfalls Punktewerte um die 'Whiskers'. Deren Bedeutung ist von der benutzten Software abhängig, mit der man die Plots gestaltet hat. Normalerweise bedeckt die Box die in der Mitte liegenden 50\%, mit dem angedeuteten zentralen Medianwert. Die Linien sollen zur Abschätzung der Daten-Bandbreite dienen, Ausreißer ausgenommen. Natürlich liegt es im Auge des Betrachters, was als Ausreißer angesehen wird und was nicht. Die exakte Definition von Whiskers besagt, dass die weit entfernteste Beobachtung weniger oder gleich der oberen Quantile plus 1,5 der Länge der Interquartilenbreite ist. Beispiel siehe Fig.~\ref{fig: exaple plots} d.

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Es gibt viele gute Einleitungen in Graphiken mit R, sodass diese Details hier nicht weiter aufgeführt werden. Zu Beginn empfehle ich diese  \href{https://github.com/florianhartig/ResearchSkills/tree/master/Labs/Statistics/Practicals/GraphicsInR}{übungen zum graphischen Gestalten mit R}. Diese sind begleitend zu diesem Skript, oder auch zu

\begin{itemize*}
  \item \href{http://www.statmethods.net/graphs/index.html}{QuickR}
  \item \href{http://shinyapps.org/apps/RGraphCompendium/index.php}{RGraphCompendium}
  \item \href{http://www.uni-kiel.de/psychologie/rexrepos/rerDiagrams.html}{rexrepos}
\end{itemize*}

\end{mdframed}
\end{fullwidth} 


\chapter{Schließende Statistik}\label{ch: inductive statistics}

\marginnote{Schließende Statistik ist das Ziehen von Schlüssen aus Beobachtungen durch statistische Methoden} 

Schließende Statistik behandelt das Schlussfolgern, z.B. das Rückschlüsse ziehen aus Beobachtungen. Ein Beispiel einer solchen Folgerung wäre beispielsweise die Beobachtung, dass 15 von 20 Testobjekte, die eine bestimmte Medikation erhielten (Behandlungsgruppe), eine Verbesserung ihres Zustandes zeigten --> Statistische Folge: Wir können davon ausgehen, dass es eine Wahrscheinlichkeit von X gibt, dass die Medikation einen positiven Effekt hat.


\section{Datenerzeugendes Modell}

\marginnote{Schließende Statistik ist nicht immer, aber meist mit dem Prinzip des datengenerierenden Modells verknüpft}

Eine zentrale Idee von vielen Methoden der Schließende Statistik ist das Prinzip des datengenerierenden Modells. Kurz gesagt beinhaltet unser datengenerierendes Modell unsere festgelegte Annahme, wie die Daten entstehen (normalverteiltes Rauschen, lineare Reaktion). Daraus kann man dann abhängig unserer festgelegten Annahme die unbekannten Größen (Unterschiede zwischen Behandlungsgruppe und Kontrollgruppe) berechnen (schlussfolgern).

\marginnote{In der Statistik verwendet man oft das Wort "`behandelt"' um Veränderungen in experimentellen Einheiten zu beschreiben. Hier wären die zwei Musikarten "`Behandlung"' und das "`Nichts-tun"' die Kontrolle} 

Man stelle sich vor man möchte herausfinden, ob das Pflanzenwachstum von Musik beeinflusst werden kann. Hierzu nehmen wir zwei Töpfe, jeder mit einer Pflanze, wobei eine mit klassischer und eine andere mit Heavy Metal -Musik beschallt wird. Eine der beiden wird zwangsläufig höher wachsen, jedoch könnte dies reiner Zufall sein, da es immer Variationen in Wachstumsraten gibt.

Somit bedarf es mehrerer Wiederholungen. Nun nehmen wir im unten aufgeführten Beispiel 30 Töpfe.


\begin{figure}[htbp]
\begin{center}
<<echo = FALSE>>=

boxplot(weight ~ group, data = PlantGrowth, main = "Plant Growth",
        ylab = "Dried weight of plants", col = "lightgray",
        notch = F, names = c("control", "classical music", "heavy metal"))

@
\caption{Wachstumsmessungen unter verschiedenen Behandlungsweisen}\label{fig: plant growth music}
\end{center}
\end{figure}

\marginnote{Beachte die Interpretation eines Boxplots - die dickere Linie in der Mitte der Box ist der Medianwert. Die Box bedeckt die zentrale 0,5 Quantile der Verteilung}
Es scheint Unterschiede zwischen den drei Fällen zu geben, jedoch gab es auch Abweichungen in den Wachstumsraten innerhalb der einzelnen Behandlungsguppen (durchschnittlich haben wir hier sieben Beobachtungen pro Behandlung). Somit ist es immer noch möglich, dass die Unterschiede in den Beobachtungen durch Zufall entstanden sind.

Wenn wir genaue Aussagen über die Wahrscheinlichkeit der Unterschiede zwischen den zwei behandelten und der Kontrollgruppe treffen möchten, müssen wir ein Modell erstellen, welches die stochastische Abweichung in den Daten beschreibt. Dies erlaubt uns wiederum Eigenschaften, wie z.B. die Zufallswahrscheinlichkeit der beobachteten Unterschiede berechnen zu können. Diese Annahmen sind das, was wir als statistisches Modell bezeichnen (oder auch: stochastische Verfahren, datengenerierendes Modell). 

Die üblichere Modellart für solche Zwecke ist das parametrische statistische Modell. Für die Daten, die wir hier haben, würde das parametrische statistische Modell annehmen, dass es eine durchschnittliche Wachstumsrate für jede Behandlungsart (Kontrolle, klassische und Heavy Metal Musik) gibt, jedoch das Wachstum jeder individuellen Pflanze mit einer normalverteilten Abweichung um ihre behandlungsspezifische durchschnittliche Wachstumsrate variiert. Die Parameter dieses Modells sind die unbekannten durchschnittlichen Wachstumsraten und die Abweichung der Normalverteilung. Diese Parameter werden dann mit Methoden, die hier in diesem Kapitel erklärt werden, an die Daten angepasst und basieren auf der Berechnung für z.B. die Wahrscheinlichkeit der Datenresultate, wenn keine Unterschiede zwischen den Gruppen wären.

Eine andere Möglichkeit datengenerierende Modelle zu erstellen sind nicht-parametrische Methoden.\marginnote{Die nicht-parametrische Statistik versucht Vermutungen über datengenerierende Prozesse zu vermeiden. Normalerweise entsteht der datengenerierende Prozess durch das Imitieren der eigentlichen Daten, z.B. durch das erneute Prüfen von Methoden.} Nicht-parametrische Methoden umgehen die Notwendigkeit Modellannahmen zu erstellen z.B. über Resamplingmethoden, in denen wiederholt Teile aus dem beobachteten Datensatz gezogen werden um so "künstlich" neue Daten zu generieren. Für Pflanzenwachstumsbeobachtungen unter drei verschiedenen Bedingungen (Behandlung) könnte man z.B. ein nichtparametrisches Modell für die Annahmen dass es keine Unterschiede in dem Wachstum gibt dadurch erstellen, dass man alle Beobachtungen ungeachtet der Behandlung in einen Topf wirft, sie dann zufällig den drei Gruppen zuweist, und ausrechnet wir groß die Unterschiede zwischen den Gruppen sind. Wenn wir dies nun oft wiederholen (z.B. 1000 mal) kann man einen guten Eindruck davon bekommen, wie wahrscheinlich es ist die beobachteten Unterschiede zu erhalten, wenn die Behandlungen keinen Effekt hätten.

Nicht-parametrische Methoden sind inzwischen ein wichtiger Bestandteil der modernen Statistik.\marginnote{Parametrische Methoden haben normalerweise eine höhere "Teststärke" (das Konzept wird gleich erklärt, aber grob heißt das dass sie eher einen Effekt finden wenn er da ist). Allerdings basieren alle Ergebnisse der parameterischen Statistik auf der Richtigkeit des parametrischen Modells. Wie man die Modellannahmen überprüft kann werden wir später in diesem Kapitel behandeln.} Ihr Vorteil liegt darin dass sie weniger Annahmen über die Daten treffen. Andererseits sind parametrische Methoden meist wesentlich schneller zu berechnen, und, falls ihre Annahmen korrekt sind, mit der selben Menge an Daten sensitiver (höhere Teststärke, siehe Kap.~\ref{ch: p-werte}), wodurch sie ehereinen vorhandenen Effekt erkennen. Deshalb sind die parametrische Methoden immer noch die Grundlage der meisten statistischen Analysen.


\section{Inferentielle Outputs}

Auf dem datengenerierendem Modell basierend (parametrisch oder nicht-parametrisch) können wir nun verschiedene inferentielle Methoden anwenden, um Schlüsse über unsere Daten zu ziehen (in unserem Beispiel: Um entscheiden zu können, ob Musik einen Unterschied macht, oder nicht). In der "normalen" Statistik sind hier vor allem zwei Methoden zu nennen, die fast universell auf jedes Problem angewandt werden: p-Werte und Maximum-Likelihood-Schätzer. Ein weiteres drittes Verfahren wird momentan immer beliebter - die nachträglich, durch Bayes ermittelte Inferenz. Auf diese werde ich noch am Ende dieses Abschnittes eingehen.

\begin{figure}[htb]
\begin{center}
\includegraphics[width = 8cm]{InferenceDE}
\caption{Aus dem datengenerierenden Modell (parametrisch oder nicht-parametrisch) können die drei "Produkte" der schließenden Statistik gewonnen werden: a) der p-Wert, b) der Maximum-Likelihood Schätzer, und c) die Bayes'sche Posterior.}
\label{fig: InferenceMethods}
\end{center}
\end{figure}


\subsection{p-Werte}\label{ch: p-werte}

Der p-Wert ist die am häufigsten benutzte, aber leider auch am schwierigsten zu interpretierende Methode. Die Verwendung des p-Wertes basiert auf dem Ansatz des Hypothesentests (englisch: null hypothesis significance testing, NHST). Die Idee dahinter ist Folgende: wenn wir Beobachtungsdaten und ein statistisches Modell haben, können wir dieses statistische Modell benutzen, um zu testen ob die Daten zu einer festen, vorgegebenen Hypothese passen. Für das Beispiel der Pflanzen und der Musik könnte unsere Hypothese so lauten: "`Musik hat keinen Einfluss auf Pflanzen; alle beobachteten Unterschiede basieren auf zufälligen Abweichungen zwischen den Individuen."' Dieses Szenario wird als Nullhypothese bezeichnet. \marginnote{Eine Nullhypothese $H_0$ ist ein vorgegebenes Szenario, das Vorhersagen über erwartete Wahrscheinlichkeiten von verschiedenen Beobachtungen macht.} In der Praxis wird typischerweise wird als Nullhypothese ein Szenario von "keinem" Einfluss gewählt, so dass man dann zeigen kann das es einen Einfluss gibt. Technisch könnte man aber genauso die Vermutung "`klassische Musik verdoppelt die Wachstumsrate der Pflanzen"' als Nullhypothese setzen. Es liegt im Ermessen des Analytikers, bzw. an der Fragestellung, was als Nullhypothese betrachtet werden soll. Dies ist auch der Grund für die große Auswahlmöglichkeit an verfügbaren Tests. Wir werden noch einige davon im folgenden Kapitel über wichtige Hypothesentests kennen lernen.

Nachdem\marginnote{Die Teststatstik beschreibt eine Eigenschaft des Datensatzes mit dem man die beobachteten Daten und H0 vergleichen will.} wir uns für eine Nullhypothese entschieden haben, wollen wir nun feststellen ob die Daten zu der Nullhypothese passen. Die Frage ist natürlich: wie definiert man denn "passen". Die Strategie der Hypothesentests ist sich zuerst eine Teststatistik zu definieren. Die Teststatistik ist irgendeine deskriptive Statistik die den Datensatz beschreibt (einfachstes Beispiel: der Mittelwert). Im Prinzip steht es jedem frei sein eigenes Maß zu wählen, aber es gibt natürlich Teststatistiken die sinnvoller sind als andere. Die Nullhypothese und die Teststatistik zusammen spezifizieren die Annahmen eines Hypothesentests. 

Mit der Teststatistik gewappnet können wir jetzt die beobachteten Daten mit H0 vergleichen. Hierzu definieren wir den p-Wert 

\begin{equation}
p := p(d >= D_{obs} | H_0)
\end{equation}

als die Wahrscheinlichkeit die beobachteten Daten oder extremer (gemessen an der Teststatsitik) zu bekommen wenn H0 wahr ist (siehe Abb.~\ref{fig: teststatistik}).\marginnote{Der p-Wert ist die Wahrscheinlichkeit der beobachteten Daten oder extremer, gegeben H0.} 


\begin{figure}[htb]
\begin{center}
\includegraphics[width = 6cm]{Teststatistik}
\caption{Eine visualisierung von p-Wert und Teststatistik. Gegen H0 erwarten wir eine gewisse Verteilung der Teststatistik. Wenn $D_obs$ die rote Linie ist, so ist der p-Wert die Wahrscheinlichkeit eine größere Teststatistik zu bekommen als der Wert für $D_obs$.}
\label{fig: teststatistik}
\end{center}
\end{figure}

Die Idee dieser Definition ist dass ein sehr kleiner p-Wert anzeigt, dass die Wahrscheinlichkeit die beobachteten Daten zu bekommen gegen H0 sehr klein ist. 
Jetzt kommt ein Trick der erst mit der weiteren Erklärung richtig sinnvoll wird: Wenn der p-Wert unter eine gewisse Schwelle sinkt (das sog. Signifikanzlevel $\alpha$), so sagt man die "Nullhypothese wird abgelehnt", "es gibt signifikante Evidenz gegen die Nullhypothese", oder einfach: "der Test / Effekt ist siginfikant". Der Wert von $\alpha$ ist eine Übereinkunft, in der Ökologie üblicherweise 0.05, sodass ein p-Wert kleiner als 0.05 ein Abweisen der Nullhypothese veranlässt. \marginnote{Wenn p<0.05, haben wir signifikante Beweise für ein Abweisen der Nullhypothese.} 

Warum diese Definition Sinn macht

\begin{figure}[htb]
\begin{center}
\includegraphics[width = 9cm]{ErrorTypes}
\caption{E}
\label{fig: Error Types}
\end{center}
\end{figure}


Also, wichtig zu merken

\begin{itemize}
  \item H0 wahr: 
\end{itemize}


\begin{equation}
FDR := \frac{p(H0) \cdot \alpha}{ p(H0) \cdot \alpha + p(!H0) \cdot (1-\beta)}
\end{equation}


\begin{figure}[htb]
\begin{center}
\includegraphics[width = 11cm]{FDR}
\caption{False Discovery Rate}
\label{fig: Error Types}
\end{center}
\end{figure}


Ein Problem der Hypothesentests und der p-Werte ist die notorische Fehlinterpretation der Ergebnisse. Der p-Wert ist NICHT die Wahrscheinlichkeit, dass die Nullhypothese wahr, oder die Alternativhypothese falsch ist; auch wenn viele Autoren diesen Fehler bei ihrer Interpretation gemacht haben, wie z.B. \citep[][]{Cohen-earthisround-1994}. Eher kann man den p-Wert als eine Kontrolle der Falsch-Positiven-Rate ansehen (Typ I-Fehler). Wenn man Hypothesentests mit einem $\alpha$ -Level von 0.05 auf Zufallsdaten anwendet, erhält man exakt 5\% Falsch-Positive. Nicht mehr und auch nicht weniger.  

\subsection{Maximum-Likelihood-Schätzwert}

Eine zweite Variante der Ergebnisdarstellung, welche bei den meisten statistischen Methoden verwendet wird, ist das Schätzen von Maximum-Likelihood Parametern. Zusammenfassend kann man sagen, dass die Maximum-Likelihood-Schätzung (maximum-likelihood estimate (MLE)) die beste Schätzung für Parameter in unserem Modell ist (z.B. Unterschiede zwischen den Behandlungsarten und der Kontrolle, wie in unserem Beispiel).

Genauer gesagt wird die Wahrscheinlichkeit in der Statistik als Funktion der Modellparameter $\theta$ beschrieben als

\begin{equation}
L(\theta) := p(dD_{obs} | M(\theta))
\end{equation}

, z.B. als die Funktion die man erhält, wenn man die Wahrscheinlichkeit der erhaltenen beobachteten Daten bei veränderten Modellparameter berechnet.

Der Maximum-Likelihood-Schätzwert\marginnote{Hier ist zu beachten, dass die Maximum-Likelihood-Schätzung ein Parametersatz ist, für den die Daten am Wahrscheinlichsten sind, nicht jedoch der wahrscheinlichste Parametersatz!} ist weiterhin definiert als eine Kombination von Parametern oder Modellannahmen, für welche die Wahrscheinlichkeit maximal ist. Während der p-Wert die Wahrscheinlichkeit der beobachteten 
oder extremeren Daten unter einer festgelegten (Null-) Hypothese beschreibt, gibt uns der Maximum-Likelihood-Schätzwert die "`Hypothese"' mit der höchsten Wahrscheinlichkeit an unsere beobachteten Daten zu erhalten.

Die Maximum-Likelihood-Schätzung ist nur ein einzelner Parameterwert.\marginnote{Eine Punktschätzung ist vergleichbar mit einer einzelnen besten Schätzung.} Diese Art Schätzung wird oft Punktschätzung genannt. Jedoch ist eine Punktschätzung oft unbrauchbar, wenn wir nicht wissen wie genau sie ist.\marginnote{Konfidenzintervalle bieten einen Unsicherheitsschätzwert um die Punktschätzung herum.} Deswegen werden Parameterschätzungen immer mit Konfidenzintervallen angegeben. Salopp kann man sagen, dass das 95\% Konfidenzintervall eines Parameters eine wahrscheinliche Reichweite für diesen Bereich ist. Verwirrend ist hier, dass dies nicht das Intervall ist, in dem der richtige Parameter mit 95\% Wahrscheinlichkeit liegt. Vielmehr beinhaltet das Standard 95\% Konfidenzintervall bei wiederholten Experimenten zu 95\% aller Fälle den richtigen Wert. Dies ist ein kleiner, aber entscheidender Unterschied. Jedoch machen wir uns hierüber nun keine weiteren Gedanken. Das Konfidenzintervall ist der ungefähre Bereich, in dem wir den korrekten Parameter erwarten. 

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}
    
\textbf{Übungsfragen MLE:} 
\textbf{4.1} Wie ist die Likelihood der beobachteten Daten D für ein gegebenes Modell M mit Parameter x definiert?
\textbf{4.2} In einem Experiment wurde der Effekt von Stickstoff auf das Wachstum von Pflanzen getestet - der Likelihood ist maximal für eine Verdoppelung des Wachstums. Die Autoren schreiben: “Die Wahrscheinlichkeit die beobachteten Daten zu erhalten ist maximal wenn man annimmt dass Stickstoff das Wachstum von den beobachteten Pflanzen verdoppelt” - ist diese Aussage Korrekt?
\textbf{4.3} Weiter unten schreiben die Autoren: “Der wahrscheinlichste Wert für den Effekt von Stickstoff ist 2 (verdoppelung)” - ist diese Aussage korrekt?

%\end{mdframed}
\end{fullwidth}

\subsection{Bayes Verfahren}

Um unsere\marginnote{Bayes'sche Verfahren berechnen eine dritte Größe, die posteriore Wahrscheinlichkeit. Auch wenn man diese auf jedes Modell anwenden kann, benutzt man sie üblicherweise bei erweiterten statistischen Berechnungen.} übersicht der schlussfolgernden Methoden zu komplettieren, fehlt noch eine, die erwähnt werden sollte - Bayes'sche Methoden berechnen eine Größe, die sich die posteriore Parameterschätzung (posterior parameter estimate) nennt. Diese ist ähnlich, jedoch nicht identisch zu der vorherigen Parameterschätzung. Näheres ist hier nicht von Nöten, bei Bedarf und Interesse kann jedoch meine Seite weiterhelfen \citep{Gelman-BayesianDataAnalysis-2003} \href{http://florianhartig.github.io/LearningBayes/}{Learning Bayes}.

\subsection{Verschiedene Methoden != verschiedene Modelle}

Wir wissen nun, dass es drei verschiedene Größen gibt, die Statistiker typischerweise berechnen: p-Werte, Maximum-Likelihood und die Posteriori. Man kann bei gegebenen datengenerierenden Prozessen immer jede einzelne davon berechnen.

Ich\marginnote{ANOVA , T-Tests und lineare Regression sind nur unterschiedliche Begutachtungen des selben Modells} formuliere diesen Punkt konkreter, da fälschlicherweise viele Menschen glauben sie benutzen unterschiedliche Modelle, wobei sie lediglich unterschiedliche Wege nutzen, um diese zu begutachten. Ein Beispiel hierfür sind ANOVA, T-Tests und die lineare Regression. Alle basieren auf ein und dem selben datengenerierenden Prozess - einige festgelegte Auswirkungen zwischen Gruppen und obendrein ein unabhängiger und identischer normalverteilter Fehler. ANOVA und T-Tests legen verschiedenste Nullhypothesen fest und die lineare Regression sucht nach dem Maximum-Likelihood-Schätzwert. Hier könnte man bei Bedarf noch die Bayes'sche Posteriori berechnen. 

\section{Wichtige Hypothesentests}

Nachdem jetzt die grundlegenden statistischen Datenausgaben besprochen wurden, gehen wir nun in die Praxis über, um die zwei wohl häufigsten Hypothesentests kennen zu lernen - den T-Test und ANOVA. Wie bereits erwähnt, basieren sie beide auf dem selben datengenerierenden Prozess, legen jedoch geringfügig unterschiedliche Nullhypothesen fest.

\subsection{T-Test}

Ein T-Test testet die Unterschiede der Mittelwerte von zwei normalverteilten Stichproben; oder im Falle einer einzigen Stichprobe zwischen 0 und den Mittelwert der Stichprobe.\marginnote{Um bei unserer vorhergehenden Einteilung zu bleiben, würden wir unsere Response-Variable als stetig und den Prädiktor als kategorisch bezeichnen (Gruppe 1 oder Gruppe 2); oder im Falle einer einzelnen Gruppe gibt es keinen Prädiktor.} Erneut ist hier das zugrundelegende Modell das einer Normalverteilung mit der Nullhypothese, dass es keine Unterschiede zwischen den Mittelwerten der zwei normalverteilten Gruppen gibt, bzw. dass der Stichproben Mittelwert 0 ist, wenn wir nur eine Gruppe betrachten. Desweiteren sind Software-abhängig oft eine Reihe an Anpassungen möglich, z.B. eine Lockerung der Annahme, dass die zwei Gruppen die selbe Varianz aufweisen. Hier folgt nun ein Beispiel in R, mit klassischen Daten von \citet{Student-probableerrormean-1908}. Die Daten zeigen die Wirkung von zwei Schlafmitteln (gesteigerte Anzahl an Schlafstunden verglichen mit der Kontrolle) an 10 Patienten. 



\begin{figure}[htbp]
\begin{center}
<< echo = F>>=
boxplot(extra ~ group, data = sleep, col = "lightgrey", xlab = "treatment", ylab = "Extra hours sleep")
@
\caption{Daten von \citet{Student-probableerrormean-1908}}
\label{fig: Student Sleep Data}
\end{center}
\end{figure}

<<eval=FALSE>>=
## Traditional interface
with(sleep, t.test(sleep$extra[sleep$group == 1], extra[group == 2]))
@

<<>>=
## Formula interface
t.test(extra ~ group, data = sleep)
@

Beachte, dass die Datenausgabe einen p-Wert (H0 = kein Unterschied), aber auch einen Maximum-Likelihood-Schätzwert der Mittelwertsvergleiche, zusammen mit den Konfidenzintervallen bietet. Dies geht weit über den klassischen T-Test hinaus. Vermutlich nahmen die Programmierer an, dass man zusätzlich den besten Schätzwert für die Mittelwertsunterschiede haben möchte.

Vorschlag für das Anzeigen des Ergebnisses: p>0.05: Unterschiede zwischen den Gruppen waren nicht signifikant. p<0.05: Wir erhielten einen Unterschied von X +- Konfidenzintervall zwischen den Gruppen (p-Wert für die Unterschiede eines T-Tests mit X). 

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}
    
\textbf{Übungsfragen Hypothesentests:} 
\textbf{4.4} Definieren Sie den p-Wert.
\textbf{4.5} Wie ist in der vorherigen Definition “extremer” (also >=) definiert?
\textbf{4.6} Stellen Sie sich vor Sie leben in Venedig, und es gibt 3 Taxibootfirmen. Sie wollen wissen ob es Unterschiede in der Beförderungsgeschwindigkeit gibt und machen deshalb 300 Fahrten mit jeder Firma und stoppen die Zeit. Was wäre eine geignete Nullhypothese H0, um auf einen Unterschied zu testen?
\textbf{4.7} Bonusfrage: Nennen Sie eine der vielen möglichen sinnvollen Teststatistiken
\textbf{4.8} In der Physik gibt es das sogenannte Standardmodell, dass die Eigenschaften und Interaktionen der Material beschreibt. In den letzten 20 Jahren wurde das Standardmodell immer und immer wieder getestet, so “erfolgreich” dass die Physiker schon ein bisschen deprimiert sind weil Sie nichts neues entdecken. Was ist die Nullhypothese die bei diesen Tests angewandt wird?
\textbf{4.9} Sie lesen einen Artikel über ein medizinisches Experiment. Getestet wurde ein Medikament gegen eine Kontrolle, und der p-Wert ist 0.03. Die Studie schreibt: “Die Wahrscheinlichkeit dass das Medikament nicht wirkt ist 3\%” - stimmen Sie zu?
\textbf{4.10} Schreiben Sie eine korrekte Interpretation des obigen Ergebnis auf.
\textbf{4.11} Definierent Sie den Typ I Fehler (falschen positive).
\textbf{4.12} Wie viel Typ I Fehler erwarten Sie bei einen Signifikanzlevel von 7\%?
\textbf{4.13} Definieren Sie den Typ II Fehler (falsche negative).
\textbf{4.14} Wie viel Typ II Fehler erwarten Sie bei einen Signifikanzlevel von 7\%?
\textbf{4.15} Nennen Sie 2 Faktoren die Typ II Fehler beeinflussen, und die Richtung des Einflusses (negativ = Typ II nimmt ab wenn Faktor hoch geht).
\textbf{4.16} Definieren Sie die Teststärke / Power.
\textbf{4.17} Sie testen 100 Gene auf eine Assoziation mit Krebs. Bei 5 Genen zeigt der Test Signifikanz an. Wie bewerten Sie dieses Ergebnis?
\textbf{4.18} Sie bekommen von einer allwissenden Macht die Zusatzinformation dass Sie in dem oben genannten Test eine Teststärke von 99\% hatte. Sind Sie nun zuversichtlicher dass Sie einen Effekt gefunden haben?
\textbf{4.19} Definieren Sie die False Discovery Rate (FDR).
\textbf{4.20} Wovon hängt die FDR ab?
\textbf{4.21}  In einer Reihe von Experimenten testen Sie Medikamente auf eine Wirkung. Ihre Power ist 100\%. Sie schätzen dass jedes 20. Medikament das Sie testen eine Wirkung haben solte. Wie ist ihre FDR bei einem Signifikanzlevel von 5\%? Es reicht wenn Sie den Rechenweg aufschreiben, Sie müssen den Wert nicht ausrechnen.

%\end{mdframed}
\end{fullwidth}


\subsection{Varianzanalyse (ANOVA)}

ANOVA (analysis of variance) oder Varianzanalyse kann von verschiedenen Personen unterschiedlich verstanden werden. Die Standard-ANOVA macht grundsätzlich die gleichen Annahmen wie ein T-Test (normalverteilte Resonanz), jedoch für mehr als zwei Gruppen. Genauer gesagt testet es, ob die gemessene Response (z.B. eine abhängige Variable) von einem oder mehreren kategorischen Variablen mit zwei oder mehreren interagierenden Ebenen beeinflusst wird. Eine Wechselwirkung\marginnote{Eine Wechselwirkung: eine Variable verändert die Auswirkung einer anderen Variablen} zwischen zweier Variablen bedeutet, dass sich der Wert einer erläuternden Variable darauf auswirkt, wie sehr eine andere erläuternde Variable die Response beeinträchtigt.

Während der Begriff ANOVA meist mit der oben genannten Erklärung in Verbindung gebracht wird (welche der von T-Tests / linearer Regression entspricht, siehe nächstes Kapitel), kann die Auffassung der ANOVA in der gleichen Weise erweitert werden, wie ein lineares Regressionsmodell zu einem allgemeinen linearen Modell (engl. Generalized Linear Models, glm) erweitert werden kann etc. Dies erlaubt uns also, ANOVAs auf Modelle mit nicht-normalverteilten Fehlern anzuwenden (natürlich muss dies der Software gesagt werden, es geht nicht automatisch davon aus). Deswegen muss man besonders Acht darauf geben, von was andere ausgehen, wenn sie diesen Term benutzen.

Hier ist ein einfaches Beispiel mit einer Standard-ANOVA (normalverteilte Fehler), in der getestet werden soll, ob Gewicht (von Hühnern) von ihrer Ernährung abhängt, wobei 'Ernährung' ein variabler Faktor mit vier Stufen ist:

<<>>=
aovresult <- aov(weight~Diet, ChickWeight)
summary(aovresult)
@

Wir erhalten einen p-Wert von 6.43e-07, welcher mit einem $\alpha$ Level von 0.05 höchst signifikant ist. Somit können wir die Nullhypothese abweisen, dass die Ernährung keinen Einfluss auf die Response "`Gewicht"' habe. Beachte hier, dass wir keine Parameterschätzwerte erhalten und wir keine Aussagen darüber machen können, welche Ernährung sich von welcher unterscheidet. Hierfür gibt es zwei Möglichkeiten:

\begin{itemize}
\item Entweder wendet man den sogenannten Post-Hoc-Test an, welcher auf Unterschiede der Ernährungsweisen testet (Bsp. mit einem T-Test).
\item Oder man wechselt zu einer Regression, die im folgenden Kapitel genauer beschrieben wird.
\end{itemize}

Mit einem Post-Hoc-Test wendet man multiple Tests auf die gleichen Daten an. Dies kann zum Problem werden - der Grundgedanke des p-Werts ist eine Wahrscheinlichkeitkalkulation für das Betrachtet der Daten bezüglich EINER Nullhypothese. Nach einer solchen Durchführung erhält mal allenfalls einen  5\% Fehler bei einem $\alpha$ Level von 0.05. \marginnote{Beim Anwenden multipler Tests auf die selben Daten benötigt man eine Korrektur des p-Werts für multiples Testen.} Wenn wir jedoch multiple Tests durchführen, testen wir auch multiple Nullhypothesen und es ergeben sich mehr Möglichkeiten für die Testgrößen ein Signifikanzlevel nur durch Zufall zu erreichen. Deswegen müssen wir den p-Wert für multiples Testen korrigieren. Um hierzu mehr Informationen zu erhalten, ist Google dein bester Freund. 

\subsection{Weitere wichtige Tests}

T-Tests und ANOVA sind sehr häufig verwendete Tests, wobei es noch viele weitere gibt. Eine Liste mit Tests kann man beispielsweise im Wikipedia-Artikel unter diesem \href{http://en.wikipedia.org/wiki/Category:Statistical_tests}{Link} finden.


\section{Regression}

Wie schon zuvor beschrieben, bedeutet Regression nicht zwingend, dass man ein anderes statistisches Modell wie bei Hypothesentests verwendet (ANOVA und das lineare Regressionsmodell verwenden in R die gleichen Annahmen). Nichtsdestotrotz ist das Ziel einer Regression ein anderes. Während Hypothesentests nur überprüfen, ob die Daten mit einer Nullhypothese vereinbar sind, sucht eine Regression nach einer am besten passenden Hypothese oder Parametern (Maximum-Likelihood-Schätzwert). Somit sucht ein Regressionsmodell nach einer Parameterkombination, die mit der höchsten Wahrscheinlichkeit die beobachteten Daten, mit vorgegebenen Modellannahmen, ausgibt.

\subsection{Lineare Regression}

Das grundlegendste Regressionsmodell ist die lineare Regression. Hier lautet die Annahme, dass wir eine Response haben, die von einem Prädiktor wie folgt abhängt:

\begin{equation} \label{eq: linear regression}
y \sim a \cdot x + b + \epsilon 
\end{equation}

wobei y die Response, x der Prädiktor ist; a ist der Parameter, wie sehr der Prädiktor die Response beeinflusst, b ist der Schnittpunkt und $\epsilon$ ist die Zufallsvariation, welche in einer linearen Regression als normalverteilt angenommen wird.

In R wird solch eine Regression durch folgenden Befehl erreicht

<<>>=
fit = lm(airquality$Temp~airquality$Ozone)
summary(fit)
@


\begin{figure}[htbp]
\begin{center}
<< echo = F>>=
plot(airquality$Temp ~ airquality$Ozone, ylab = "Temperature", xlab = "Ozone")
abline(fit)
@
\caption{Luftqualitäts Datensatz: Temperatur aufgetragen gegen Ozon. Zusammenhang durch die Regressionsgerade angegeben.}
\label{fig: LR}
\end{center}
\end{figure}

Dieser Code kann unabhängig davon verwendet werden, ob der Prädiktor stetig oder kategorisch ist. Im Falle einer stetigen Variable entsteht eine Linie anhand der Daten. Im Falle einer kategorischen Variable mit n Stufen ist die erste Stufe als Referenz gesetzt (Schnittpunkt), und n-1 Faktoren entsprechen den folgenden Stufen, die den Unterschied zur Referenz beschreiben.

Die entsprechenden Parameter erscheinen in der Spalte "`Estimate"'. Dies zeigt uns, wie sehr der Prädiktor, in diesem Fall Ozon, die Response, hier Temperatur, beeinflusst: Für jede Einheit Ozon mehr steigt die Temperatur um 0.201 Einheiten, mit einem Standardfehler (Konfidenzintervall) von 0.019. Abgesehen davon, wie ein Regressions-Output aussieht, lehrt uns dies etwas anderes Wichtiges: Die Tatsache, dass wir die Temperatur als Response und Ozon als Prädiktor benutzt haben, bedeutet nicht, dass Ozon ursächlich die Temperatur beeinflusst. \marginnote{Korrelation ist nicht Kausalität.}Tatsächlich ist es genau anders herum: wenn wir mehr Sonne haben, wird es wärmer und wir haben tendenziell mehr Ozon. Regression schafft nicht, wie die meisten anderen statistischen Analysen, Kausalität. Es schafft Korrelation. Was wir hier ausdrücken ist, dass wenn unsere Ozonmessungen steigen, wir ziemlich sicher davon ausgehen können, dass es auch gleichzeitig wärmer wird. Was wiederum nicht bedeutet, dass Ozon Hitze erschafft. Korrelation ist nicht Kausalität.

Die Regressionsergebnisse geben uns ebenfalls einige p-Werte aus. Das sind die Ergebnisse von mehreren Hypothesentests, die automatisch nach der Regression geschaltet sind. Beispielsweise erhält man einen p-Wert für jeden Parameter. Dieser p-Wert basiert auf einem bestimmten Typ von T-Test, wobei das vollständige Modell gegen ein Modell mit auf 0 gesetzten Parametern getestet wird. Zusätzlich gibt es noch einen weiteren p-Wert, welcher auf einer anderen Testgröße am Ende der Regressionsausgabe beruht. Dieser testet die Nullhypothese mit allen Parametern gleich 0.

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{Präzisieren von verschiedenen Modellannahmen in R:} 

Response y hängt linear von Variable a (stetig oder kategorisch) ab

<<eval = FALSE>>=
fit = lm(y~a)
summary(fit)
@

Response y hängt linear von zwei Variablen a und b (stetig oder kategorisch) ab, aber der Wert beider Variablen beeinflusst nicht den Effekt, welcher die andere Variable auf die Response hat (keine Interaktion)

<<eval = FALSE>>=
fit = lm(y~a+b)
summary(fit)
@

Response y hängt linear von zwei Variablen a und b (stetig oder kategorisch) ab, aber der Wert der einen Variable beeinflusst nicht den Effekt der anderen Variable auf die Response (Interaktion)

<<eval = FALSE>>=
fit = lm(y~a*b)
summary(fit)
@

Response y ist von einer Variablen a (stetig oder kategorisch) wie in $a + a^2$ abhängig

<<eval = FALSE>>=
fit = lm(y~a + I(a^2))
summary(fit)
@

die Schreibweise I() kennzeichnet eine darauf folgende mathematische Formel. 

\end{mdframed}
\end{fullwidth}



\subsection{überprüfen der Annahmen}

Formal gesehen kann man jegliche Datensätze auf lineare Modelle übertragen. Jedoch sind hier wie bei jeder statistischen Folgerung die Ergebnisse (z.B. Parameterschätzungen, p-Werte) von den getroffenen Annahmen abhängig. Dementsprechend ist der p-Wert, den wir erhalten, von der Annahme abhängig, dass die Daten auch tatsächlich mit der Formel ~\ref{eq: linear regression} übereinstimmen. Wenn dies nicht der Fall ist, könnte der p-Wert völlig falsch sein. Somit muss man prüfen, ob diese Annahmen auch tatsächlich erfüllt wurden.

Also was genau waren die Annahmen einer linearen Regression? Ein Problem ist, dass sich Studenten an die Annahme der Normalverteilung erinnern. Deswegen überprüfen sie, ob die Response-Variable normalverteilt ist. Wenn man jedoch die Formel ~\ref{eq: linear regression} genauer betrachtet, erkennt man, dass dies nicht entscheidend ist. Wenn man die Terme in der Formel ~\ref{eq: linear regression} verschiebt,
sehen wir das, was eigentlich normalverteilt sein soll

\begin{equation} \label{eq: linear regression}
y - (a \cdot x + b ) \sim \epsilon 
\end{equation}

nämlich der Unterschied zwischen beobachtetem Wert und den Modellvorhersagen. Diese Unterschiede werden Residuen genannt und sollten, entsprechend unserer Modellannahmen, normalverteilt sein. Um zu überprüfen, ob dies wirklich der Fall ist, sollten einige Tests durchgeführt werden. Der einfachste Test ist das Erstellen eines Plots mit den Residuen gegen die angepassten Werte UND gegen alle Prädiktoren im Modell. Der daraus entstehende Plot erlaubt es dann viele mögliche Probleme zu erkennen (Fig.~\ref{fig: ResidualPatterns})



\begin{figure}[htbp]
\begin{center}
<< echo = F>>=
oldpar <- par(mfrow = c(2,2))

plot(rnorm(1000), ylim = c(-4,4), main = "Perfekt Normal", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)

plot(rnorm(1000, sd = seq(0.01,2, len = 1000)), ylim = c(-4,4), main = "Heteroscedasticity", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)


plot(rnorm(1000, mean = seq(-2,2, len = 1000)), ylim = c(-4,4), main = "Pattern in the residuals", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)

plot(ifelse(runif(1000) <0.5, rnorm(1000, mean = 2, sd = 0.3), rnorm(1000, mean = -2, sd = 0.3)) , ylim = c(-4,4), main = "Distribution not normal", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)

par(oldpar)
@
\caption{Eine Sammlung möglicher Muster, wenn die Residuen gegen den angepassten Wert (voreingestellt in R) oder einen Prädiktor geplottet werden.}
\label{fig: ResidualPatterns}
\end{center}
\end{figure}

übliche Probleme und ihre Lösungen sind  

\begin{itemize}
  \item Heteroskedastizität (Varianzveränderungen) --> Verändere die Response, oder verwende ein Regressionsmodell, das Heteroskedastizität berücksichtigen kann
  \item Muster in den Residuen -> Falsche Funktionsform des Regressionsmodells. Versuche Prädiktoren, quadratische Effekte, Interaktionen oder andere Dinge hinzuzufügen, die die funktionale Form ändern
  \item Verteilung nicht normal -> Angenommen, es liegt nicht an einem der früheren Probleme (kein Muster / keine Heteroskedastizität), kann man noch eine Variablentransformation ausprobieren oder eine Regression mit einer anderen Verteilungsannahme durchführen (siehe nächsten Abschnitt)
\end{itemize}  

Es gibt weitere spezialisierte Plots, um mit der Diagnose dieser Probleme zu helfen. Diese erfolgen bei der Anwendung des Befehls plot()


Sie erhalten grundlegende Residuendiagnosen durch Eingabe von plot(fit), wobei fit das angepasste Modell ist. Weitere Details zur Residuendiagnose siehe \href{http://www.statmethods.net/stats/rdiagnostics.html}{hier}.



\section{Allgemeine lineare Regressionsmodelle (GLM)}

Die allgemeine Vorstellung einer linearen Regression war, dass 1) die Response stetig ist, theoretisch von - unendlich bis + unendlich, und 2) Residuen normal um die Modellvorhersagen verteilt sind. Der Grundgedanke der allgemeinen linearen Regressionsmodell-Rahmenbedingung ist, wie zuvor in dem linearen Regressionsbeispiel zu arbeiten, jedoch beide Annahmen über Responsegrößen von - bis + unendlich und die Normalität zu lockern. Dazu müssen wir zwei Dinge tun

\begin{itemize}
  \item Um die Ausgabewerte in dem Bereich zu erhalten, den wir wollen, wickeln wir das lineare Modell in eine Transformationsfunktion, die die Response ins rechte Intervall zwingt (typische Intervalle sind positiv oder zwischen 0 und 1). Diese Transformation wird als Link-Funktion bezeichnet
  \item Um andere Verteilungen anzupassen, müssen wir dem Modell sagen, dass es etwas anderes als die Gaußsche Fehlerfunktion verwenden soll.
\end{itemize}    
   
Wir werden über diese Punkte nun etwas mehr ins Detail gehen.

\subsection{Die Link-Funktion}

Wir haben gesagt, dass eine lineare Regression folgende Form annimmt

\begin{equation}
y \sim a \cdot x + b 
\end{equation}

Das heißt, wenn x groß wird, könnte y jeden Wert einnehmen, positiv oder negativ. Ein Trick, um sicherzustellen, dass alle Vorhersagen für y positiv sind oder innerhalb eines bestimmten Bereichs liegen, ist eine Link-Funktion der Form

\begin{equation}
y \sim f^{link}(a \cdot x + b )
\end{equation}

Jede Funktion ist möglich, aber wie wir später sehen werden, sind typische Alternativen die Exponentialfunktion, die positive Ergebnisse garantiert, und die inverse Logit, die Bereich zwischen 0 und 1 garantiert.

\subsection{Andere Verteilungen}

Nun, das ist konzeptionell der einfache Teil, aber vielleicht ist noch nicht klar, welche Art von Distributionen neben der normalen vorhanden sind. Zwei typische Entscheidungen, die wir unten verwenden, sind die Binomial- (die Verteilung für das Münzwerfen) und die Poisson-Verteilung (eine diskrete Wahrscheinlichkeitsverteilung). Es stehen viele andere Möglichkeiten zur Verfügung. Vielleicht wird es deutlicher, wenn wir uns in den nächsten Abschnitten zu den konkreten Beispielen bewegen.

\subsection{0/1 Daten - logistische Regression}

Logistische Regression ist die häufigste Analyse für binäre Daten (Präsenz / Abwesenheit, überlebt / tot, infiziert / nicht infiziert). Logistische Regression geht davon aus, dass die Verteilung binomial ist (Münzwurf-Modell). Um den linearen Prädiktor auf einer Skala zwischen 0 und 1 zu erhalten, die für die Binomialverteilung erforderlich ist, verwenden wir die logistische Linkfunktion (oder inverse Logit).

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Hier ein Beispiel mit den Daten der Titanik-überlebenden. Beachte, dass die Logit-Link automatisch ausgewählt wird, wenn in R die Binomialverteilung verwendet wird. Bei Bedarf könnte man diese Wahl überschreiben.

<<>>=
library(effects)
fmt <- glm(survived ~ age + I(age^2) + I(age^3), family=binomial, data = TitanicSurvival)
summary(fmt)
@


\end{mdframed}
\end{fullwidth} 




\subsection{Zähldaten - Poisson-Regression}

Die Poisson-Regression ist die Standard-Wahl für die Arbeit mit Zähldaten, obwohl ein paar andere Optionen zur Verfügung stehen. In der Poisson-Regression wird standardisiert eine Exponentialfunktion gewählt, um alle Werte positiv zu machen. Das Inverse des Exponentials ist der Log, also nennen wir das den Log-Link. Nach wie vor wählt R diese automatisch aus, wenn man angibt die Verteilung zu "`poissonieren"'.

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Ein Beispiel hierfür zeigt einige Daten über die Fütterung von Nesthockern in Bezug auf ihre Attraktivität:


<<>>=
schnaepper <- read.csv("schnaepper.txt", sep="")
fm <- glm(stuecke ~ attrakt, family=poisson, data = schnaepper)
summary(fm)
@

\end{mdframed}
\end{fullwidth} 


\subsection{Residuen-überprüfungen in allgemeinen linearen Regressionsmodellen}

Residuen in allgemeinen linearen Regressionsmodellen sollten nicht normalverteilt sein, deswegen verwendet man keine Standard-Kontrollen für Normalität wie normale Quantile-Quantile-Plots, um auf die Angemessenheit der Residuen zu überprüfen. Für nicht zu komplizierte Modelle gibt es eine Möglichkeit dieses Problem zu umgehen, indem man die sogenannten Pearsons-Residuen verwendet, die die beobachteten Unterschiede zwischen Modell und Daten durch die erwartete Varianz des Modells normiert \footnote{In R kann man die Option Pearson in vielen Funktionen angeben, einschließlich der Residual()-Funktion, die man auf ein angepasstes Objekt anwenden kann}

Ein Standardproblem in Poisson oder binomialen allgemeinen linearen Regressionsmodellen ist, dass die Varianz der Poisson- und Binomialverteilung nicht eingestellt werden kann, sondern durch den Mittelwert festgelegt wird. Dies ist ein Problem, das im normalen linearen Modell nicht auftritt, da hier der zufällige Teil durch eine Normalverteilung modelliert wird, die einen Parameter für die Varianz aufweist. Ein Problem, das sehr häufig in Poisson oder binomialen allgemeinen linearen Regressionsmodellen auftritt, ist eine überdispersion, d.h. dass die Residuen mehr Varianz, als unter dem angepassten Modell erwartet, zeigen. \marginnote{Man kann auf überdispersion prüfen, indem man die angepasste Abweichung betrachtet oder einen überdispersionstest anwendet} Der einfachste Weg, um dies zu korrigieren, ist die Verwendung der Quasi-Poisson- und Quasibinomialmodelle, die in der allgemeinen linearen Regressionsmodell-Funktion verfügbar sind. Diese Modelle bringen einen zusätzlichen Parameter ein, der die Varianz des Poisson- und des binomialen allgemeinen linearen Regressionsmodells abwandelt.

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}
    
\textbf{Übungsfragen Regression:} 
\textbf{4.22} Was sind die Annahmen der linearen Regression?
\textbf{4.23} Wie werden die Parameter in der linearen Regression bestimmt?
\textbf{4.24} Welche H0 steckt hinter den p-Werten der Parameter der linearen Regression?

%\end{mdframed}
\end{fullwidth}


\chapter{Prädiktive Statistik und Maschinelles Lernen}\label{ch: Prädiktive Statistik}

Prädiktive Statistik (auch: maschinelles Lernen) ist ein dritter Bereich der Statistik der in den letzten Jahren zunehmend an Bedeutung gewonnen hat. Das grundlegende Ziel dieser Methoden besteht darin, gute Vorhersagen aus einem komplexen Datensatz zu erzeugen. Dabei verwenden sie typischerweise relativ komplizierte, nicht-parametrische Verfahren, die typischerweise Vorhersagen ermöglichen, aber keine Berechnung klassischer Größen der schließenden Statistik, wie z.B. der p-Wert oder MLE. 

Die vielleicht bekannteste Methode des maschinellen Lernens ist der sogenannte Random Forest Algorithmus. Genau zu erklären wie dieses Algorithmus funktioniert würde hier zu weit führen, aber um eine grobe Idee zu geben: die Idee ist die Daten durch sogenannten Entscheidungsbäume zu beschreiben. Die Entscheidungsbäume sagen: Daten mit Eigenschaft A führen zu Ergebnis X, und Eigenschaft B führt zu Y. Eine Strategie für Vorhersagen wäre jetzt einen sehr detaillierten Entscheidungsbaum zu bauen. Genau das mach der Random Forest nicht. Statt dessen baut er viele einfache (= schlechte) Entscheidungsbäume und kombiniert diese dann. Erstaunlicherweise führt die Kombination von vielen schlechten Modellen oft zu einer besseren Vorhersage als die eines guten Modells. Hier ein Beispiel des Algorithmus, in dem wir die Menge an Ozon in der Luft aus verschiedenen anderen Wetterparametern voraussagen 

Mit der klassischen linearen Regression können wir ungefähr 60\% der Varianz in Ozon erklären

<<>>=
ozone.lm <- lm(Ozone ~ ., data=airquality, na.action=na.omit)
summary(ozone.lm)
@

Random Forest schafft es auf 70\%. Wir haben hier keine Vorhersagen auf neue Daten getestet, aber typischerweise fällt der Unterschied zwischen lm und rf in diesem Fall noch deutlicher aus. 

<<>>=
library(randomForest)
ozone.rf <- randomForest(Ozone ~ ., data=airquality, mtry=3,
                         importance=TRUE, na.action=na.omit)
print(ozone.rf)
@

Der Random Forest liefert keine p-Werte o.ä., aber man kann sich wenigsten die Wichtigkeit der verschiedenen Variablen anzeigen lassen, wobei sich hier Wichtigkeit auf die Wichtigkeit für die Vorhersage bezieht, eine kausale Annahme wird hier nicht gemacht. 

<<>>=
varImpPlot(ozone.rf)
@

Das Verhältnis zwischen den Anhängern der klassischen schließenden Statistik und den maschinellen Lernen ist seit langem von Spannungen geprägt. Klassische Statistiker bemängeln dass das maschinelle Lernen die Idee des "`Lernens von Daten"' im Sinne des Vergleichens von Hypothesen und Daten zugunsten einfacher Prognosen aufgegeben hat. Ein Anhänger des maschinellen Lernens würde darauf antworten, dass es bei vielen angewandten Problemen nicht um das Testen von kausalen Zusammenhängen, sondern nur um das Erkennen von Pattern geht. Das Ziel ist es, einen Algorithmus zu erstellen, der in der Lage ist bei einem komplexen Datensatz korrekt vorherzusagen. \footnote{Typische maschinelle Lernanwendungen beinhalten die Vorhersage der Interessen der Kunden in Web-Shops, die Korrelation komplexer Satellitendaten mit Bodensignalen oder Sprach/Gesichtserkennung.} Der Erfolg ist auf der Seite des maschinellen Lernens: Experten in diesem Feld sind derzeit bei Technologieunternehmen wie Google, Facebook, etc. heiß begehrt, und der Markt scheint immer weiter zu wachsen. Der Unterschied zwischen der schließenden und der prädiktiven Statistik sowie die Spannung zwischen diesen Bereichen ist Inhalt des äußerst empfehlenswerten Artikels "`Statistical Modeling: The Two Cultures"' von \citet{Breiman-StatisticalModelingTwo-2001}, dem Erfinder des Random Forest Algorithms. Hier ein Auszug aus diesem Text:

\begin{quote}
There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.
\end{quote}

Eine abschließende Bemerkung: Aufgrund der großen und wachsenden Bedeutung der prädiktiven Methoden in der modernen Statistik habe ich dieses kurze Kapitel in diesen Einführungstext aufgenommen. Natürlich gibt es wesentlich mehr zu diesem Thema zu sagen als der Platz hier erlaubt. Um mehr über prädiktive Methoden zu lernen würde ich das Lehrbuch von \citet{James-IntroductiontoStatistical-2013} empfehlen, das ich auch am Ende dieses für die weitere Lektüre empfehle. Es ist auch instruktiv zu sehen wie viel Interesse diesen Methoden im Internet entgegengebracht wird, und dass viele Firmen offene Wettbewerbe in prädikativer Statistik ausschreiben um Mitarbeiter zu gewinnen (siehe, z.B. \href{kaggle.com}{https://www.kaggle.com/}). 



\chapter{Versuchsplanung}\label{ch: design of experiments}

Kommen wir zurück zu einem der ersten Punkte in diesem Skript: die Daten. Wenn wir selbst Daten erheben wollen, müssen wir eine Reihe von Fragen beantworten: Welche Variablen sollten gemessen werden? Wie sollten wir die Variablen ändern die wir kontrollieren können. Und wie viele Replikate brauchen wir?


\section{Auswahl der Variablen}

In einem praxisbezogenem Setting sind wir typischerweise daran interessiert, wie eine Response von einer Anzahl von Prädiktor-Variablen beeinflusst wird. Es ist klar, dass wir sowohl die Response als auch die Prädiktoren unseren Interesses durch einige dieser Prädiktorwerte messen müssen, um etwas über die Wirkung der Prädiktoren zu sagen. \marginnote{Korrelation ist keine Kausalität.}Wenn wir nur wissen wollten, ob es eine Korrelation zwischen Prädiktoren und Response gibt, wäre unsere Variablenliste zu diesem Zeitpunkt vollständig. Normalerweise wollen wir aber nicht nur wissen, ob es eine Korrelation gibt, sondern auch, ob wir mit einiger Sicherheit sagen können, dass diese Korrelation kausal ist. Wenn wir diesen Anspruch geltend machen wollen, müssen wir ausschließen, dass es auch gegensätzliche Faktoren gibt, die auch als Störvariablen bezeichnet werden.

\subsection{Was ist eine Störvariable?}

Man stelle sich vor, wir sind an einer Response A interessiert und wir haben angenommen, dass A \~{} B. Man stelle sich vor, dass es eine zweite Prädiktorvariable C gibt, die einen Einfluss auf A hat, an der wir aber für den Zweck der betrachteten Frage nicht interessiert sind. Eine solche Variable, die für die Frage nicht von Interesse ist, wird auch als "`Fremdvariablen"' bezeichnet\marginnote{Eine Fremdvariable ist eine Variable, die die Response beeinflussen kann, aber für den Experimentator nicht von Interesse ist.}. So haben wir auch A \~{} C, aber wir sind nicht an dieser Beziehung interessiert. Wenn wir nun Daten erfassen und nicht C messen, ist es normalerweise kein Problem, solange C mit B unkorreliert ist - es könnte ein bisschen mehr Variabilität in der Response erzeugen, aber im Großen und Ganzen sollte der Effekt von C durchschnittlich sein und wir sollten die Auswirkung von B noch feststellen können.

\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{Confounding}
\caption{Diagramm, das eine Störvariable darstellt. Eine wichtige Voraussetzung für einen Störfaktor ist, dass die Variable sowohl mit der Response korreliert als auch mit den Prädiktorvariablen, die unsere ursprüngliche Hypothese bilden. Wenn die zweite Verbindung nicht vorhanden ist, ist die Variable nicht störend und kann ignoriert werden.}
\label{fig: Confounding}
\end{center}
\end{figure}

Das Problem des Störfaktors tritt auf, wenn die Fremdvariable C aus irgendeinem Grund mit der zu betrachtenden Prädiktorvariable B korreliert. \marginnote{Eine Störvariable ist eine externe Variable, die sowohl mit der Response als auch mit einer zu betrachtenden Prädiktorvariable korreliert.}In dem Fall, wenn wir nur B messen, sehen wir die Auswirkungen von B und C. In diesem Fall schreiben wir die Auswirkung von C auf A fälschlicherweise der Auswirkung von B auf A zu. \marginnote{Eine Scheinkorrelation ist eine Korrelation, die durch eine Störvariable verursacht wird.} Eine Korrelation, die durch eine nicht gemessene Störvariable verursacht wird, wird als Scheinkorrelation bezeichnet.

\subsection{Was macht man mit Störvariablen}

Wenn wir denken, es gibt einen Faktor, der gestört werden könnte, haben wir grundsätzlich drei Optionen

\begin{enumerate}
\item Am besten: steuert man den Wert dieser Faktoren. Entweder durch Fixieren des Wertes (bevorzugt, wenn wir nicht an diesem Faktor interessiert sind), oder durch ändern des Wertes in einer kontrollierten Weise (siehe unten).
\item Zweitens: zufällig anordnen und messen
\item Drittens: Nur zufällig anordnen oder nur messen
\end{enumerate}

Die Randomisierung bedeutet, dass wir versuchen, sicherzustellen, dass der Störfaktor nicht systematisch mit der Variablen von Interesse korreliert (kann jedoch potentiell noch immer Probleme mit Wechselwirkungen und nichtlinearen Beziehungen hervorrufen).


Das Messen\marginnote{Variablen, die wir einbeziehen, aber für uns nicht interessant sind, werden oft als Störungsvariablen bezeichnet.} erlaubt es uns, die Auswirkungen in einer statistischen Analyse zu berücksichtigen, aber es kostet Kraft (siehe unten) und wir können nicht alles messen.
\section{Definition und Bias von Variablen}

Ein häufiger Fehler bei diesem Schritt des Versuchsentwurfs ist es, die Variablendefinition und die Messungen für selbstverständlich zu erachten und sich eher mit Entscheidungen über Wiederholungen usw. auseinander zu setzen. Der fehlende Schritt ist jedoch über die folgenden zwei Fragen nachzudenken. \marginnote{Die Betrachtung dieser beiden Fragen wird oft als Konstruktvalidität bezeichnet.}

\begin{enumerate}
  \item Messen meine Variablen, was ich messen möchte
  \item Was ist der erwartete statistische (stochastische) Fehler in meinen Messungen und was ist der mögliche systematische Fehler in meinen Messungen
\end{enumerate}

\begin{figure}[]
\begin{center}
\includegraphics[width = 10cm]{RandomizedBlockDesign}
\caption{Illustration eines randomisierten Blockdesigns, der wohl am häufigsten verwendeten Anordnung bei (Beobachtungs)-Experimente zur Randomisierung der Auswirkung unbekannter und nicht gemessener verwandter Variablen. Die Idee dieses Entwurfs ist, dass die unbekannten Variablen wahrscheinlich im Raum korrelieren. Indem wir alle experimentell verändernden Variablen zu einem Block zusammenfassen, vermeiden wir, dass sie von den unbekannten räumlichen Variablen gestört werden können.}
\label{fig: RandomizedBlockDesign}
\end{center}
\end{figure}


Der erste Punkt mag ein wenig seltsam erscheinen, weil man denken würde man wisse was man misst. Doch in vielen Fällen der ökologischen Statistik und darüber hinaus messen wir nicht direkt die Variable an der wir interessiert sind, sondern einen Stellvertreter. Zum Beispiel, wollen wir die Temperatur an einer bestimmten Stelle wissen und verwenden dabei die Temperatur von einer Wetterstation 5 km entfernt. Oder wir wollen die funktionale Vielfalt untersuchen, aber wie können wir dies in Form von Variablen ausdrücken, die wir im Feld messen?

Die zweite Frage bezieht sich darauf, wie sehr sich zwei Messungen voneinander unterscheiden würden, wenn wir sie wiederholt (stochastisch) durchführen und wie viele Messungen systematisch ausfallen könnten (z.B. weil eine Methode oder ein Instrument systematisch falsch ist oder weil der Mensch bestimmte Einflüsse (Bias) zeigt).


\section{Auswahl der Werte für die unabhängigen (Prädiktor-) Variablen}

Wenn wir entschieden haben, welche Variablen zu messen sind/variieren, müssen wir uns für die Größen entscheiden, an denen wir sie messen wollen.

In\marginnote{Die experimentelle Einheit ist die Einheit, die einer bestimmten Variablenkombination (z.B. Behandlung oder Kontrolle) zugewiesen werden kann. Beispiel: eine einzelne Pflanze oder ein Topf.} einer experimentellen Studie verändern wir in der Regel Variablen systematisch für eine bestimmte Einheit, z.B. eine Pflanze, ein Topf oder ein Grundstück. Diese Einheit heißt die experimentelle Einheit. Auch Beobachtungsstudien haben experimentelle Einheiten (die Einheiten, für die Messungen durchgeführt werden), aber es ist meist nicht möglich, die Variablen vollständig zu kontrollieren. Allerdings hat man in der Regel die Möglichkeit bestimmte Selektionen vorzunehmen. Auch in Beobachtungsstudien ist es entscheidend eine ausreichende Variation der Prädiktorvariablen über die experimentellen Einheiten zu gewährleisten, um eine aussagekräftige statistische Analyse zu ermöglichen.

Hier sind ein paar Punkte, die zu beachten sind

\subsection{Variieren aller Variablen unabhängig voneinander}

Ein übliches Problem in der Praxis ist, dass wir zwei Variablen haben, ihre Werte sich jedoch in einer korrelierten Weise ändern. Man stelle sich vor, wir testen auf das Vorhandensein einer Art, aber wir haben nur warme trockene und kalte nasse Stellen. Wir behaupten die beiden Variablen seien kollinear. In diesem Fall wissen wir nicht, ob der beobachtete Effekt auf Temperatur oder Wasserverfügbarkeit zurückzuführen ist. Das Fazit: Wenn man zwei Effekte trennen möchte, darf die Korrelation zwischen ihnen nicht übereinstimmend sein - idealerweise wäre es Null oder so niedrig wie möglich. 

\subsection{Interaktionen}

Um Wechselwirkungen zwischen Variablen erkennen zu können, genügt es nicht, alle zu verändern, sondern es bedarf bestimmter Kombinationen. Das Schlagwort hier ist (fraktionierte) faktorielle Versuchsplanung. Google wird hier weiterhelfen.

\subsection{Nichtlineare Effekte}

Die Verbindung von zwei Punkten ist eine Linie. Wenn man herausfinden möchte, ob die Response auf eine Variable nichtlinear ist, benötigt man daher mehr als zwei Werte für jede Variable. 


\section{Wie viele Replikate?}

Wir haben bereits erwähnt, dass das Signifikanzniveau $\alpha$ die Wahrscheinlichkeit ist, falsch Positives zu finden. Dies wird als Typ-I-Fehler bezeichnet. Es gibt einen weiteren Fehler, den man machen kann: es wird keine Signifikanz für einen wahren Effekt erkannt. Dies wird als Typ-II-Fehler bezeichnet und die Wahrscheinlichkeit, einen Effekt zu finden, wird als Teststärke (engl. power) bezeichnet\marginnote{Teststärke ist die Wahrscheinlichkeit, Signifikanz für einen Effekt zu finden, wenn es welche gibt}. Für statistische Standardverfahren kann die Stärke berechnet werden. Man muss für jede einzelne Methode nachsehen, aber im Allgemeinen geht man davon aus, dass 

\begin{enumerate}
\item die Teststärke mit zunehmender Einflussgröße steigt 
\item die Teststärke mit zunehmender Variabilität in der Response sinkt
\end{enumerate}

Das bedeutet, dass, im Gegensatz zum festgelegten Typ-I-Fehler, die Berechnung der Teststärke Kenntnisse über den erwarteten Effekt und die Variabilität erfordert. Das klingt nicht gut, jedoch kann man in den meisten Fällen aus Erfahrung abschätzen, wie viel Variation es geben wird und in den meisten Fällen weiß man auch, wie groß der Effekt mindestens sein muss, um relevant zu sein. Danach kann man berechnen, wie viele Proben man benötigt.

\newpage
\begin{mdframed}
    
\textbf{Checkliste für die Versuchsplanung}

\begin{description}

\item[( )] Eindeutige, logisch einheitliche Frage? Schreib es auf. Lesen des Kapitels über gültige/gute wissenschaftliche Fragen in den Vorlesungsunterlagen.

\item[( )] Stelle sicher, dass alle Fragen der Richtigkeit, wie im Haupt-Skript besprochen, gelesen und berücksichtigt wurden. Gehen Sie durch die Checkliste Richtigkeit am Ende des Abschnitts im Haupt-Skript.

\item[( )] Entwerfe einen Versuchsaufbau.

  \begin{description}

  \item[( )] Verändere die Variablen, die zu messen sind, um die Fragen beantworten zu können. Entscheide, ob man an linearen Hauptauswirkungen oder auch an nichtlinearen Effekten oder Interaktionen interessiert ist.
  
  \item[( )] Schreibe potenzielle Störvariablen auf. Entscheide, ob sie besser kontrolliert, randomisiert oder gemessen werden sollten? Sind diese sicher störend (korreliert mit Response UND einem oder mehreren der Prädiktoren)?
  
  \item[( )] Definiere die zu testende statistische Hypothese, einschließlich der Störgröße. Schreibe es auf, wie in $height  \sim age + soil * precipitation + precipitation^2$. 
  
  \item[( )] Wähle aus, wie die Variablen im Experiment variiert werden. Ziehe die Verwendung von Software dafür in Betracht, z.B. für fraktielle faktorielle Versuchsplanungen (in Beobachtungsstudien hat man manchmal begrenzte Kontrolle, aber man kann vielleicht schätzen, welche Variablenkombinationen beobachtet werden).
  
  \item[( )] Blocking - versuche, verschiedene Behandlungen / verschiedenste Kombinationen zusammen zu gruppieren. Das Ziel ist, dass unbekannte / nicht gemessene Variablen nicht mit experimentellen Variablen korrelieren (siehe Pseudoreplikation).
  
  \item[( )] Bestimme die Anzahl der Replikate. Mache eine Vermutung für Effektgröße und Variabilität der Daten, und berechne oder errate die Anzahl der notwendigen Wiederholungen, um genügend Teststärke zu erhalten. Was genügend bedeutet, hängt vom Sachgebiet ab, aber ich würde behaupten, dass du eine hohe Chance haben möchtest, einen Effekt zu sehen, wenn einer da ist, also wäre eine Teststärke von $>80\%$ gut.
  
  \end{description}
  
\item[( )] Versuchsaufbau prüfen.
  
  \begin{description}
  
  \item[( )] Spiele die Abläufe des Datenerhalts durch: simuliere es im Geiste oder in R, mache einige Daten, schreibe es auf. Scheint alles in Ordnung?
  
  \item[( )] Spiele die Abläufe der Datenanalyse durch. Welche Methode? Kannst du deine Frage beantworten? Mach eine Teststärken-Analyse!

  \end{description}


\item[( )] überarbeite alles, wenn nötig

\end{description}

\end{mdframed}


\chapter{Wissenswertes und weiterführende Lektüren}

\section{Reproduzierbarkeit und gute wissenschaftliche Vorgehensweise}

Reproduzierbarkeit bedeutet, dass jeder Schritt der Analyse wiederholbar ist. Die Erfahrung zeigt, dass es nicht so trivial ist, wie man denkt, die Reproduzierbarkeit zu gewährleisten. Hier einige Hinweise, wie man Datenanalyse reproduzierbar machen kann

\begin{itemize}

\item{Sobald man Rohdaten produziert hat, ändert man sie NIE. Man speichert sie an einem Speicherort, erstellt eine Sicherung und berührt sie niemals erneut}

\item{In der Regel muss man einige Aufwertungen, Umbenennung etc. vor der Datenanalyse erledigen. Wenn möglich, macht man dies durch ein Skript (z. B. R, Python, perl). Man speichert das Skript zusammen mit der Analyse ab.}

\item{Verwende ein Versionskontrollsystem für den Code und notiere für jede Ausgabe die Revisionsnummer, mit der die Ausgabe erstellt wurde.}

\item{Wenn die Analyse durchgeführt wird, speichere die random-seed und die Einstellungen des Computers, um die Reproduzierbarkeit sicherzustellen. In R ist der einfachste Weg dies zu tun, die random-seed auf random.seed(123) zu setzen und die Ergebnisse von sessionInfo(), welche die Versionsnummern aller verwendeten Pakete enthält, zu speichern.}

\item{Denke darüber nach, den Code in einer Reporting-Umgebung wie z.B. Rmd oder sweave auszuführen.}


%\footnote{Siehe auch die R-Task-Ansicht über \href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{reproduzierbare Forschung}

\end{itemize}

\section{Wie man mehr über die Statistik lernen kann}\label{sec: further readings}


\begin{itemize}

\item Um dieses einführende Skript zu vervollständigen, würde ich empfehlen die \href{https://github.com/florianhartig/ResearchSkills/tree/master/Labs/Statistics}{Praxis des Skripts} durchzugehen.

\item Wenn man ein weiteres praktisches Lehrbuch für Anfänger möchte, empfehle ich \citet{Dormann-ParametrischeStatistik-2013} für Deutschsprachige (eBook Kostenlos für Studenten aus Freiburg, kontaktieren Sie mich) und \citet{Gotelli-PrimerEcologicalStatistics-2004} für Englischsprechende. 

\item Für die technisch etwas Ehrgeizigeren (es ist immer noch sehr elementar), empfehle ich \citet{James-IntroductiontoStatistical-2013}. Man kann die PDF kostenlos herunterladen und es gibt ein MOOC für das Buch mit Vorträgen und übungen.

\item Weitere Hilfe und Hinweise kann man in der Statistik-Hilfe unserer Abteilung \href{http://biometry.github.io/APES/}{hier} finden; insbesondere die Empfehlungen zu R-Skripten und statistischen Lehrbüchern.

\end{itemize}


\bibliographystyle{chicago}
\bibliography{/Users/Florian/Home/Bibliography/Databases/flo}

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\begin{appendices}

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}

\chapter{Lösungen der Übungsfragen}

\textbf{4.1} p(D|M,x) - in Worten: die Wahrscheinlichkeits(dichte) für D, gegeben M und x.
\textbf{4.2} Ja.
\textbf{4.3} Nein. Der MLE gibt den Wert für den die Wahrscheinlichkeit der Daten maximual ist. Um die Aussage umzudrehen müssen wir Zusatzannahmen machen (alle Parameter gleich wahrscheinlich).
\textbf{4.4} p(d >= D | H0), in Worten: Wahrscheinlickhkeit die beobachteten oder extremere Daten zu bekommen wenn H0 wahr ist.
\textbf{4.5} Extremer bezieht sich auf die Teststatistik - diese ist ein Abstandsmaß dass der “Erfinder” des Testes wählt. Weil es hierfür unterschiedliche Möglichkeiten gibt gibt durchaus verschiedene Tests mit gleichen H0, aber anderen Teststatistiken.
\textbf{4.6} H0: Die durchschnittliche Zeit aller 3 Firmen ist gleich.
\textbf{4.7} Mögliche Antworten: Varian der 3 Mittelwerte, Unterschied größter / kleinster Mittelwert, Durchschnitt der Differenzen der Mittelwerte, … [alles womit man die Firmen vergleichen könnte .. natürlich könnten Sie auch Mittewert durch Median ersetzen].
\textbf{4.8} H0 = das Standardmodell.
Anmerkung: ich wollte Ihnen mit diesem Beispiel zeigen dass es 2 Möglichkeiten gibt Nullhypothesen aufzustellen: Wenn wir einen Effekt bestätigen wollen, stellen wir die umgekehrte Nullhypothese auf, also dass es keinen Effekt gibt (das ist der Normalfall in der Analyse von biologischen Daten). Der Hintergrund hier ist dass wir einen Effekt vermuten, aber nicht sicher sind oder: Wenn der Effekt / das Modell aber schon gut bekannt ist, d.h. es eine feste Theorie gibt, wäre das normale Vorgehen diesen Effekt / Theorie als H0 aufzustellen, und zu schauen ob es eine Abweichung von dem gibt was wir gerade als wahr ansehen.
\textbf{4.9} Nein, der p-Wert gibt die Wahrscheinlichkeit der Daten gegeben H0 an, nicht die Wahrscheinlichkeit von H0.
\textbf{4.10}“Die Wahrscheinlichkeit dass die beobachteten Effekte oder stärker unter H0 (d.h. ohne Wirkung des Medikaments) auftreten ist 3\%. Der Unterschied zwischen Kontrolle und Behandlung ist deshalb signifikant bei einen Signifikanzlevel von 5\%.”
\textbf{4.11} Wahrscheinlichkeit dass der p-Wert signifikant wird wenn H0 wahr ist.
\textbf{4.12} 7\%.
\textbf{4.13} Wahrscheinlichkeit dass der p-Wert nicht signifikant wird wenn H0 nicht wahr ist.
\textbf{4.14} Nur mit dieser Information kann man das nicht sagen, weil der Typ II von mehreren Faktoren abhängt.
\textbf{4.15} Mögliche Antworten: Sigifikanzlevel - negativ; Varianz - positiv; Effektstärke: negativ; power / Stichprobengröße: negativ.
\textbf{4.16} power = 1 - Typ I Fehler / in Worten: also Wahrscheinlichkeit dass der p-Wert signifikant wird wenn H0 nicht wahr ist.
\textbf{4.17} 5 von 100 signifikante Tests sind zu erwarten wenn keines der Gene eine Wirkung hat (wir machen hier multiples Testen). Das Ergebnis zeigt keine besondere Evidenz für einen Effekt an. Bemerkung: Trotzdem würde man natürlich die 5 signifikanten mit einen weiteren Experiment noch mal nachtesten, man weiß ja nie. Aber statistisch ist so ein Ergebnis zu erwarten, auch wenn keines der Gene eine Wirkung hat.
\textbf{4.18} Nein, denn wenn Ihre Teststärke fast 100\% ist und Sie wirklich Assoziationen in den Datensatz hätten dann müssten Sie ja trotzdem MEHR als 5\% positive sehen.
\textbf{4.19} Die Rate an Experimenten die Signifikanz anzeigt obwohl kein Effekt da ist wenn man viele Experimente macht.
\textbf{4.20} FDR hängt ab von Typ I und II Fehlerrate und der Wahrscheinlichkeit dass H0 wahr ist (bzw. dass die in den Experimente getesteten Effekt da sind).
\textbf{4.21} 0.95 * 0.05 / (0.95 * 0.05 + 0.05 * 1) - Rate an Typ I / Rate an signifikante Ergebnissen.
\textbf{4.22} Abh. Variable beschreibbar durch Polynom der unabhängigen Variable(n) + normalverteilte Streuung.
\textbf{4.23} Man sucht den MLE für die Annnahmen der linearen Regresion.
\textbf{4.24} H0 = Parameter ist 0.

%\end{mdframed}
\end{fullwidth}

\chapter{R und Rstudio}

R selbst ist ein Befehlszeilenprogramm, d.h. man kommuniziert mit R über schriftliche Befehle, die man in die R-Konsole eingibt. Also, entweder man schreibt direkt in die Konsole, oder man schreibt den R Code in einem Texteditor vor und fügt ihn dann in die R-Konsole ein.

Für die tägliche Arbeit ist dies jedoch recht unpraktisch. Man hätte gerne ein Programm, dass einen Texteditor mit der Konsole und weiteren Optionen (z.B. die Möglichkeit graphische Ausgaben aus R anzuzeigen) kombiniert. R bietet eine einfache Version eines solchen Programms an, die RGui (ist ist zu beachaten, dass die Mac und Windows Versionen der RGui leicht unterschiedlich aussehen).  Um die RGui unter Windows zu starten, startet man einfach das Programm "R". Wenn man in der RGui 2 + 2 im Hauptfenster eintippt und Enter drückt, sollte Folgendes zu sehen sein:

\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{rgui1.png}
\caption{Eine einfache Rechnung in RGui tippen}
\label{fig: Rgui1}
\end{center}
\end{figure}

Das Fenster, das man hier sieht, ist die R-Konsole. Durch die Konsole interagiert man mit dem R Kern, der die statistischen Berechnungen ausführt.

Nun schreiben wir etwas anderes: R hat einige Standarddatensätze, die automatisch geladen werden. Wir werden den airmiles-Datensatz verwenden, der die Anzahl der vergebenen Luftmeilen anzeigt. Wir werden nun kurz erläutern, wie man ein Diagramm mit diesem Datensatz erstellt. Tippe:

<<eval=F>>=
plot(airmiles, col = 4)
@

Das Ergebnis sollte wie in Fig.~\ref{fig: Rgui2} aussehen:

\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{rgui2.png}
\caption{Ein Graph erstellt mit RGui}
\label{fig: Rgui2}
\end{center}
\end{figure}

Wie man erkennen kann, öffnet dies anscheinend ein neues Fenster (eine Grafik-Ausgabe) und plottet die Luft-Meilen gegen die Zeit. Wir werden diskutieren, warum und wie das funktioniert. Bevor man sich nun an RGui gewöhnt, sehen wir uns zuerst RStudio an, ein alternatives Programm, um mit R zu interagieren.

\section{Der Rstudio Editor}
 
RStudio bietet grundsätzlich die gleichen Funktionen wie RGui, aber man kann vieles einfacher machen oder bequemer handhaben. So sieht es aus:

\begin{figure}[]
\begin{center}
\includegraphics[width = 9cm]{rst_interface.png}
\caption{Der RStudio Editor, der wohl beliebteste Editor für R}
\label{fig: Rstudio}
\end{center}
\end{figure}


\paragraph{Konsole:} Die Konsole, die wir bereits gesehen haben, befindet sich im linken unteren Bereich. Man kann nachweisen, dass sie sich gleich verhält, indem man die gleichen Befehle wie zuvor eingibt, d.h. 2 + 2, oder das Diagramm erneut grafisch darstellt.

\paragraph{Editor:} über der Konsole (oben links) werden R Skriptdateien angezeigt und können im Editor geändert werden. Die Idee einer Skriptdatei ist, dass man alle Befehle sammelt, die man an die Konsole in einer Datei sendet, sodass man sie später erneut ausführen kann.  

Ein typisches Skript kann so aussehen:

<<eval=F>>=
# the hash means this is treated as a comment
# this file is written by FH, 25.10.13

rm(list=ls(all=TRUE))  # this command means all variables in the memory are erase

# load some data

# do some plots

@

Um einen Teil des Skripts an die Konsole zu senden, kann man den run-Button am oberen rechten Rand des Editorfensters verwenden. Für alles, was wir von nun an tun, würde ich dringend empfehlen, es in das Skript zu schreiben und es dann von dort aus an die Konsole zu senden.

\chapter{Verarbeitung von Daten in R}
\label{HandlingDataInR}

\section{Variablen}

Wer hat jemals mit einer Programmiersprache gearbeitet? In einer Programmiersprache werden die Daten in Variablen/Objekten gespeichert. So weisen wir der Variablen "?VariableX"' das Wort "`test"' zu,

<<>>=
VariableX = "test"
@


Ich kann jetzt auf die Variable zugreifen, indem ich seinen Namen in der Konsole eingebe und es wird den Wert ausgeben, den es gespeichert hat.

<<>>=
VariableX
@

Wir können alle Variablen, die wir in der globalen Umgebung oder in R angegeben haben, in der oberen rechten Ecke oder in RStudio sehen. Wie man sieht haben wir hier die Variable "?VariableX"' zusammen mit ihrem Wert.


\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{rst_globenv.png}
\caption{Die globale Umgebung wird in der oberen rechten Ecke des R Studio-Editors angezeigt.}
\label{fig: Rstudio}
\end{center}
\end{figure}

\section{Datentypen und Strukturen}

Eine Variable kann verschiedene Dinge speichern: eine Zahl, ein Wort, eine Liste oder eine ganze Datenmenge.

Der einfachste Fall sind Variablen, die nur einen **einzigen Wert** enthalten. Hier ist die Frage, welche Art von Werten die Variable enthält. Die verschiedenen Datentypen, die ein einzelner Wert haben kann, heißen die **atomic types** - sie entsprechen den grundlegenden Datentypen in R. Wichtige atomic types sind:

  \begin{itemize}
		\item boolean (TRUE / FALSE)
		\item ganzzahlig (1, 2, 3, 5)
		\item numerisch (1.1, 2.5, 3.456)
		\item Faktor ("`rot"', "`grün"', "`blau"')
		\item Subjekt ("`ein Wort"', "`ein weiteres Wort"')
	\end{itemize}

Wenn wir eine Sammlung von mehreren ätomic types" haben, sprechen wir von einer Datenstruktur oder einem Objekt (es gibt einen Unterschied, der hier aber keine Rolle spielt). Wichtige Beispiele hierfür sind:

  \begin{itemize}
		\item **Vektor** (Eine Reihe der gleichen 'atomic types', z.B. [1,2,3,4,5] )
		\item **Liste** (Im Grunde wie ein Vektor, kann aber verschiedene Typen enthalten, Bsp. [1, "`rot"', FALSE] )
		\item **data.frame** (Eine Liste von Vektoren; dies ist das Standardformat für Daten in R. Man stellt sich dies wie eine Tabelle vor - jede Spalte ist ein Vektor und kann einen anderen Typ haben)
	\end{itemize}

Eine vollständige Liste der Datentypen findet man \href{http://www.statmethods.net/input/datatypes.html}{hier}. 
 
\subsection{Prüfung von Datentypen und Strukturen}

Besonders nach dem Einlesen der Daten ist es wichtig zu prüfen, welchen Typ die Daten haben. Die Funktionen in R reagieren je nach atomic type unterschiedlich.

Wenn man wissen will, welchen Typ oder Struktur eine Variable hat, verwendet man den str-Befehl:

<<eval=F>>=
str(object)
@

Um eine Zusammenfassung der Struktur (z.B. Mittelwert pro Spalte; was genau zusammengefasst wird, ist abhängig von Datenstruktur und Typ) zu erhalten, verwenden man:

<<eval=F>>=
summary(object)
@


Um eine automatischen Plot zu erstellen (R wird selbst entscheiden, welcher am meisten für diese Struktur/Typ geeignet ist), tippt man:

<<eval=F>>=
plot(object)
@

Probiere dies mit dem Objekt Luftqualität.

\subsection{Zugriff auf Spalten, Zeilen und Elemente in einem Datenframe oder einer Matrix}

Wie bereits erwähnt, ist die häufigste Struktur in R der Datenframe. Grundsätzlich werden Spalten als eine Liste von Vektoren gespeichert, sodass jede Spalte ein anderer Datentyp sein kann.

Man kann Spalten auf verschiedenste Weisen auswählen:

- Nach Name:
<<eval=F>>=
airquality$Ozone
@

- Nach Spaltenindex: 

<<eval=F>>=
airquality[,1]
@

Zu beachten sei hier, dass die erste Spalte durch [,1] ausgedrückt wird. Die erste Reihe erhält man durch [1,].

\section{Daten auswählen}

Die letzte Art des Datenzugriffs ist ein Beispiel für das "`Slicen"'. Slicing ist eine sehr leistungsfähige Technik, die in den meisten wissenschaftlichen Programmiersprachen verfügbar ist. Das bedeutet, dass man auf seine Daten zugreifen kann, indem man Spalten, Zeilen oder bestimmte Elemente eingibt. Man schaue sich hierfür die folgenden Befehle an (Erläuterung immer unterhalb):

<<eval=F>>=
airquality[,1:2]
@

verschafft die ersten Spalten 1 und 2.


<<eval=F>>=
airquality[4:6,1]]
@

selektiert die Reihen 4 bis 6 in Spalte 1, und


<<eval=F>>=
airquality[c(1,2,3,4,7,8),1]
@


selektiert die Reihen 1,2,3,4,7,8 in Spalte 1. So können wir eine beliebige Kombination von gewünschten Elementen sehr bequem aus dem Datenframe auswählen. 

Man muss übrigens nicht immer alle Zahlen von Hand eintippen. Die Kombination 

<<eval=F>>=
1:10
@

verschafft uns zum Beispiel die Werte von 1 bis 10, und

<<eval=F>>=
c(1:5,7)
@

erzeugt die Werte 1 bis 5 und 7. Die c() - Funktion kombiniert Werte in einem Vektor (für das Slicen ist es notwendig die Werte in einem Vektor zu haben)

Wir können aber auch Selektionen mit logischen Operatoren erzeugen

<<eval=F>>=
airquality$Temp > 80
@

Erstellt einen Vektor mit True auf allen Temperaturwerten, die >80 sind. Ich kann diesen speichern und für eine Selektion, oder auch sofort verwenden

<<eval=F>>=
airquality[airquality$Temp > 80 , ]
@

Wählt alle Zeilen mit einer Temperatur >80 aus.


\section{Laden von Daten in R}

Bisher hatten wir die Daten schon im R Programm. Nun werden wir demonstrieren, wie einige Daten geladen werden (wir verwenden die airquality.txt-Datei, die zur Verfügung gestellt wird)

Es gibt grundsätzlich zwei Möglichkeiten Daten zu laden

- mit RStudio (Punkt und Klick) - gehe zu Environment (oben rechts), importiere den Datensatz und folge den Anweisungen

- aus dem Skript mit dem Befehl read.table ()

Das funktioniert so

<<eval=F>>=
data = read.table("airquality.txt", header = T)
@

In diesem Fall funktioniert es gut, weil ich die Daten auf die einfachste Weise vorbereitet habe. Wenn man andere Datenformate (Kommas, Semikolons, überschriften / keine überschriften) hat, befragt man die Hilfe-Option mit dem Befehl read.table. Siehe auch http://www.statmethods.net/input/importingdata.html für weitere Optionen, z.B. Excel oder Datenbankimport.

\subsection{überprüfen der Daten}

überprüfe nach dem Laden der Daten immer, ob das Datenformat korrekt ist

<<>>=
str(data)
@

Hier sieht man den atomic type jeder Säule. Stelle sicher, dass es dem entspricht, was man möchte (manchmal wird numerisch als Faktor eingelesen , oder umgekehrt). Wenn eine Spalte den falschen Typ haben würde, müssen wir das manuell ändern durch:

<<eval=F>>=
as.factor(x)
@

oder

<<eval=F>>=
as.numeric(x)
@

Hinweis: Diese Datei ist bereits in R enthalten. Wenn man es nicht schafft den Datensatz zu laden, kann man trotzdem fortfahren.

\chapter{Plot-Befehle in R}

Davor einige wichtige Hinweise

\begin{itemize}
\item \href{http://rgraphgallery.blogspot.de/search/label/3%20vartiable%20plots}{hier}
\item Sehr hilfreich, der \href{http://shiny.stat.ubc.ca/r-graph-catalog/#}{R Graphenkatalog}
\item \href{http://rgm3.lab.nig.ac.jp/RGM/R_image_list?page=2282&init=true}{R Graphik-Handbuch}
\item \href{http://www.statmethods.net/graphs/line.html}{QuickR}
\end{itemize}


\marginnote{Ein Überblick über die Farbennamen in R \href{here}{http://research.stowers-institute.org/efg/R/Color/Chart/ }}

<<fig.height = 4, fig.width=4, fig.align='center'>>=
plot(airquality$Ozone, airquality$Temp)
@


<<fig.height = 4, fig.width=4, fig.align='center'>>=
hist(airquality$Ozone, breaks = 30, col = "darkred", xlab = "Ozone ")
@


<<fig.height = 4, fig.width=4, fig.align='center'>>=
plot(airquality)
@


pairs(airquality)


Einfaches Balkendiagramm

<<fig.height = 4, fig.width=4, fig.align='center'>>=
counts <- table(mtcars$gear)
barplot(counts, main="Car Distribution", 
   xlab="Number of Gears")
@

Gruppiertes Balkendiagramm

<<fig.height = 4, fig.width=4, fig.align='center'>>=
counts <- table(mtcars$vs, mtcars$gear)
barplot(counts, main="Car Distribution by Gears and VS",
  xlab="Number of Gears", col=c("darkblue","red"),
  legend = rownames(counts), beside=TRUE)
@


<<fig.height = 4, fig.width=4, fig.align='center'>>=
boxplot(mpg~cyl,data=mtcars, main="Car Milage Data", 
   xlab="Number of Cylinders", ylab="Miles Per Gallon")
@


Gekerbter Boxplot des Zahnwachstums gegen 2 gekreuzte Faktoren aufgetragen;
Boxen für eine leichtere Interpretation eingefärbt

<<fig.height = 4, fig.width=4, fig.align='center'>>=
boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE, 
  col=(c("gold","darkgreen")),
  main="Tooth Growth", xlab="Suppliment and Dose")
@

%See http://www.statmethods.net/graphs/index.html 


Korrelations-Heatmap. Die Heatmap visualisiert die Korrelation zwischen Variablen. Eine praktische Eigenschaft dieser Funktion ist, dass Variablen neu angeordnet werden können, sodass eng korrelierte Variablen nahe beieinander liegen. Dies ist oft nützlich, wenn man nicht korrelierte Variablen für eine Analyse auswählen möchte.

<<fig.height = 6, fig.width=6, fig.align='center'>>=
round(Ca <- cor(attitude), 2)
symnum(Ca) # simple graphic
heatmap(Ca, symm = TRUE, margins = c(6,6)) # with reorder()
@

\chapter{Regression in R}


\section{Stetige Response - lineare Regression}

Die lineare Regression ist die einfachste Form der Regression. Wir können diese bei einer stetigen Response verwenden. Die Annahme hierbei ist, dass die Response von den Prädiktoren wie folgt abhängt:

<<eval = F>>=
y ~ par1 * pred1 +  par2 * pred2 +  par3 * pred2^2 + ... + residual Error
@

Wobei die Parameter par1 ... par3 geschätzt werden und der Residuenfehler normalverteilt ist.

Werfen wir einen Blick auf einige Beispiele, mit den Daten von Montag. Wir sehen, dass es eine Korrelation zwischen Ozon und Temperatur gibt.

<<>>=
plot(airquality$Temp~airquality$Ozone)
@

Mit dem Befehl lm() können wir R versuchen lassen, eine bestmöglich passende Gerade zwischen den beiden Variablen zu finden.

<<>>=
fit = lm(airquality$Temp~airquality$Ozone)
@


Lassen Sie uns das Ergebnis zuerst visuell betrachten

<<>>=
plot(airquality$Ozone, airquality$Temp)
abline(fit, col = "blue")
@

Hier ist die Ausgabe im Detail

<<>>=
summary(fit)
@

In der Ausgabe sehen wir die Parameter für die Auswirkung von Ozon (genannt Regressionssteigung) und den Schnittpunkt.

R erkennt, dass die Linie gerade ist, weil wir dem Programm sagen, dass "`airquality$Temp~airquality$Ozone"'. Wir werden später sehen, wie wir das ändern können, wenn wir andere Funktionen anpassen wollen. 

\subsection{Residuenanalyse}

Mit plot(fit) erhalten wir die Residuen (die Abweichung von der Geraden). Wie bereits gesagt, geht die lineare Regression davon aus, dass diese normal verteilt sind, also sollten wir überprüfen, ob dies wirklich der Fall ist.

<<>>=
par(mfrow=c(2,2))
plot(fit)
@

Hier sind die Residuen nicht wirklich homogen um den vorhergesagten Wert gestreut, was darauf hindeutet, dass das Modell nicht sehr gut passt. Dies hätte man schon erahnen können, weil die Korrelation nicht sehr linear aussieht. Wir können einen quadratischen Term hinzufügen durch

<<>>=
fit2 = lm(airquality$Temp~airquality$Ozone + I(airquality$Ozone^2))
summary(fit2)
@

Die Residuen sehen jetzt besser aus

<<>>=
par(mfrow=c(2,2))
plot(fit2)
@

Plotten der Ergebnisse

<<>>=
plot(airquality$Ozone, airquality$Temp)
points(fit2$model[,2], predict(fit2), col = "blue")
@

\subsection{Kategorische Prädiktoren}

Wenn wir kategoriale Variablen wie in diesem Datensatz haben

<<>>=
boxplot(PlantGrowth$weight~PlantGrowth$group, main = "growth of plants")
@

Funktioniert auch noch dies:

<<>>=
fit <- lm(weight~group, data = PlantGrowth)
summary(fit)
@

Eine Sache, die viele Menschen jetzt verwirrt, ist, dass wir zwei Parameter-Schätzungen für die eine Prädiktor-Variable haben. Der Grund dafür ist, dass es 3 Gruppen in der Datenmenge gibt. Die erste Gruppe wird in diesem Fall automatisch als Referenz gesetzt, und für die anderen Gruppen werden Prädiktoren geschätzt. Die p-Werte für diese Prädiktoren sind also gegen die Referenz (wenn ich Prädiktor trt1 herausnehme, erhält er den Wert von ctrl). Daher hängen diese p-Werte für kategoriale Variablen von der Reihenfolge der Variablen ab (man kann die Reihenfolge von trt1 als Referenz ändern.)


\subsection{ANOVA}

Eine Frage, die oft in diesem Zusammenhang auftaucht, ist: Gibt es überhaupt Unterschiede zwischen den Gruppen? Die Regression zeigt uns nur, ob es einen signifikanten Unterschied zwischen dem ersten Faktor und den zwei anderen gibt. Wenn wir auf allgemeine Differenzen testen wollen, können wir eine ANOVA des angepassten Objekts machen

<<>>=
aovresult <- aov(fit)
summary(aovresult)
@

Beachte, dass wir jetzt Signifikanz für einen allgemeinen Unterschied erhalten, obwohl wir keine Signifikanz in der Regression zuvor hatten.

Wir können nun sogenannte Post-Hoc-Tests verwenden, um herauszufinden, welche Unterschiede signifikant sind.

Weitere Beispiele mit weiteren Faktoren findt man \href{http://www.statmethods.net/stats/anova.html}{hier}.


Anmerkung: Eine Varianzanalyse ist für ausgeglichene Versuchsanordnungen ausgelegt und die Ergebnisse können schwer ohne Abgleich interpretiert werden: Achte darauf, dass fehlende Werte in der Response wahrscheinlich den Ausgleich kosten. Wenn es zwei oder mehr Fehler in den Stichprobenschichten gibt, sind die verwendeten Methoden ohne Ausgleich statistisch ineffizient und es kann vorteilhaft sein lme in Paket nlme zu verwenden.

\subsection{T-Test}

Eine einfache Option für das Testen von nur zwei Gruppen gegeneinander ist der T-Test. Er nimmt eine normale Verteilung innerhalb der Gruppen an. Wir können dies verwenden, um Post-Hoc-Tests für das obige Beispiel durchzuführen:

<<>>=
attach(PlantGrowth)
t.test(weight[group=='ctrl'], weight[group=="trt1"])
@

Testen gegen Gruppe 2

<<>>=
t.test(weight[group=='ctrl'], weight[group=="trt2"])
detach(PlantGrowth)
@

Aber wenn wir wirklich beide Tests ausgeführt hätten, müssten wir für multiple Tests korrigieren

<<>>=
p.adjust(c(0.2504, 0.0479), method = "holm")
  @

Wie funktioniert das? Schreibe ?p.adjust in die Konsole. Lese auch \href{das hier}{http://webdev.cas.msu.edu/cas992/weeks/week10.html}.

\section{Allgemeine lineare Modell-Rahmenbedingung}

Das GML ist eine Verallgemeinerung von linearen Modellen (lm) zu anderen Response-Typen. Wir könnten ein Modell erzeugen, das zu lm() identisch ist durch glm(formula, family = gaussian(link = "`identity"')), aber mit dem Vorteil, dass GLM mehr Optionen hat; darunter die folgenden Vorgaben

<<eval = F>>=
binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
@


Aber Schritt für Schritt ... schauen wir uns ein Beispiel für binomische Daten an


\section{0/1 Response - die logistische Regression}

<<>>=
library(effects) 
data(TitanicSurvival)
head(TitanicSurvival)
str(TitanicSurvival)
attach(TitanicSurvival)
@


Veranschaulichen wir uns dies. Informationen zur Visualisierung von Assoziationen findet man unter \href{http://www.statmethods.net/advgraphs/mosaic.html}{diesem Link}.

Wir benutzen das Mosaikplot. Möglicherweise muss man zuvor das dazu ben?tigte Paket installieren: install.packages ("vcd")

<<>>=
library(vcd)
mosaic(~ sex + passengerClass + survived, shade=TRUE, legend=TRUE) 
surv <- as.numeric(survived)-1 # glm requires 0 / 1 not true false
@


Wie analysieren wir diese Daten? Die Response ist eindeutig nicht normal, jedoch 1/0. Das GLM ist im Grunde das gleiche wie lm (), nur dass wir die Familie angeben.

Wir werden zuerst testen, ob das überleben mit dem Alter korreliert

<<>>=
fmt <- glm(surv ~ age, family=binomial)
summary(fmt)
@

Ergebnis geplottet

<<>>=
plot(surv ~ age, main="only age term")
newage <- seq(min(age, na.rm=T), max(age, na.rm=T), len=100)
preds <- predict(fmt, newdata=data.frame("age"=newage), se.fit=T)
lines(newage, plogis(preds$fit), col="purple", lwd=3)
lines(newage, plogis(preds$fit-2*preds$se.fit), col="purple", lwd=3, lty=2)
lines(newage, plogis(preds$fit+2*preds$se.fit), col="purple", lwd=3, lty=2)
@


Nun setzen wir alle relevanten Variablen ein in:

<<>>=
surv <- as.numeric(survived)-1 # glm requires 0 / 1 not true false
fmt <- glm(surv ~ age  + sex + passengerClass, family=binomial)
summary(fmt)
@

\subsection{ANOVA für GLM}

Wenn eine ANOVA erwünscht ist


<<>>=
library(car)
Anova(fmt)

detach(TitanicSurvival)
@

\section{Zähldaten - Poisson's Regression}

Für Zähldaten verwenden wir den GLM mit der Poisson'schen-Fehlerverteilung. Hier sind einige Beobachtungen der Verteilung von Futterstücke an junge Vögel und ihre wahrgenommene Attraktivität.

<<>>=
cfc <- data.frame(
  stuecke = c(3,6,8,4,2,7,6,8,10,3,5,7,6,7,5,6,7,11,8,11,13,11,7,7,6),
  attrakt = c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5) 
)
attach(cfc)
plot(stuecke ~ attrakt)
@


So wird der Poisson angegeben

<<>>=
fm <- glm(stuecke ~ attrakt, family=poisson)
summary(fm)
@

Vorhersagen

<<>>=
newattrakt <- c(1,1.5,2,2.5,3,3.5,4,4.5,5)
preds <- predict(fm, newdata=data.frame("attrakt"=newattrakt))
plot(stuecke ~ attrakt)
lines(newattrakt, exp(preds), lwd=2, col="green")
@

Das gleiche mit 95\% Konfidenzinterval:

<<>>=
preds <- predict(fm, newdata=data.frame("attrakt"=newattrakt), se.fit=T)
str(preds)
plot(stuecke ~ attrakt)
lines(newattrakt, exp(preds$fit), lwd=2, col="green")
lines(newattrakt, exp(preds$fit+2*preds$se.fit), lwd=2, col="green", lty=2)
lines(newattrakt, exp(preds$fit-2*preds$se.fit), lwd=2, col="green", lty=2)

detach(cfc)
@


\section{Multinomiale Daten - multinomiale Regression}

Wenn man mehrere Optionen für die Response habt (rot, grün, blau), passt dies zu einer multinomialen Regression. Dies ist nicht im Standard-GLM-Paket. Das Standardpaket dazu wäre mlogit. Ich gebe dazu unten ein Beispiel an. Das Problem mit mlogit ist, dass es Daten in einer bestimmten Weise erfordert, d.h. dass für jede Beobachtung jede Auswahl eine einzelne Zeile ist. Zusätzlich gibt es eine Spalte, die sagt, welche Wahl getroffen wurde (ja / nein). Um mlogit verwenden zu können, müssen Sie die Daten in dieses Format umwandeln.

Wenn man die Daten nicht in diesem Format hat und nicht neu formatieren möchte, ist eine weniger leistungsstarke, aber einfacher zu bedienende Alternative die multinomiale Funktion aus dem nnet-Paket. Im Folgenden finden Sie ein Beispiel für die Verwendung dieser Funktion oder \href{http://www.ats.ucla.edu/stat/stata/dae/mlogit.htm}{hier}.

\subsection{mlogit Beispiel}

<<message=FALSE>>=
library(mlogit)
 
data("Fishing", package = "mlogit")
head(Fishing)
@

Die Daten, die wir verwenden, ist ein Datenframe mit folgendem Inhalt :

\begin{enumerate}
\setlength\itemsep{-0.5em}
\item price.beach -price for beach mode

\item price.pier -price for pier mode

\item price.boat -price for private boat mode

\item price.charter -price for charter boat mode

\item catch.beach -catch rate for beach mode

\item catch.pier -catch rate for pier mode

\item catch.boat -catch rate for private boat mode

\item catch.charter -catch rate for charter boat mode

\item income - monthly income

\item mode -recreation mode choice, one of : beach, pier, boat and charter

\end{enumerate}

Wir transformieren das in ein Objekt, mit dem mlogit arbeiten kann


<<>>=
Fish <- mlogit.data(Fishing, varying = c(2:9), shape = "wide", choice = "mode")
@


Variation zeigt dem Modell an, dass es sich um Variablen handelt, die für die alternativen Outputs der Response spezifisch sind, z.B. der Preis des Bootes; während Variablen, die nicht variieren, unabhängig vom Output sind, z.B. Einkommen.


<<>>=
## a pure "conditional" model
summary(mlogit(mode ~ price + catch, data = Fish))
@

Für jeden Outcome passt die Erhöhung der "choice" zu "price" und "catch". Daher erhalten wir die Schnittpunkte für jeden Outcome und ein Parameter pro Variable, was die Auswirkung von "price" / "choice" auf die Wahrscheinlichkeit ist, eines der 4 Ergebnisse zu wählen.

Wenn wir die Variable "income" korrelieren, ist dies anders. Das Einkommen ist nicht spezifisch für "choice" (nicht als variabel ausgewählt, als wir die Daten eingelesen haben). In diesem Fall fragen wir uns, wie sich "choice" verändert, wenn wir Menschen mit unterschiedlichen Einkommen haben. Daher erhalten wir einen Intervallwert pro Parameter und eine Schätzung, ob diese Wahl durch eine Erhöhung des Einkommens, bezogen auf die Grundlinie (Pier), beeinflusst wird.

<<>>=
## a pure "multinomial model"
summary(mlogit(mode ~ 0 | income, data = Fish))
@

Wenn wir die Parameterwerte interpretieren wollen, müssen wir die Response mit der relativ komplizierten Linkfunktion der multinomialen logistischen Regression zurück transformieren, siehe \href{http://en.wikipedia.org/wiki/Multinomial_logistic_regression}{hier}. Mehr Beispiele \href{http://www.inside-r.org/packages/cran/mlogit/docs/suml}{hier} und ein mlogit Tutorial findet man \href{http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf}{hier}.

\subsection{mlogit Beispiel}

<<message=FALSE>>=
require(nnet)
@

Wir verwenden die ursprünglichen fishing- Daten (ohne Umformung), wie für das mlogit Beispiel siehe oben.
Und auch die gleiche Modellstruktur

<<>>=
## a pure "conditional" model
summary(multinom(mode ~ price.beach + price.pier + price.boat + price.charter + 
                   catch.beach + catch.pier + catch.boat + catch.charter, data = Fishing))
@
\end{appendices}


 
\end{document}
