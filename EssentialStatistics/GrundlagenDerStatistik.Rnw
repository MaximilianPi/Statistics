\documentclass[a4paper,twoside]{tufte-book} %style file is in the same folder.

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{german}

\usepackage{color}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{listings}

\usepackage{graphicx}

\usepackage{multicol}              
\usepackage{multirow}
\usepackage{booktabs}
%\usepackage{natbib} 

\usepackage[innerrightmargin = 0.7cm, innerleftmargin = 0.3cm]{mdframed}
\usepackage{mdwlist}

\usepackage[]{hyperref}
\definecolor{darkblue}{rgb}{0,0,.5}
\hypersetup{colorlinks=true, breaklinks=true, linkcolor=darkblue, menucolor=darkblue, urlcolor=blue, citecolor=darkblue}

\usepackage[toc,page]{appendix}


\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

\lstset{ % settings for listings needs to be be changed to R sytanx 
language=R,
breaklines = true,
columns=fullflexible,
breakautoindent = false,
%basicstyle=\listingsfont, 
basicstyle=\ttfamily \scriptsize,
keywordstyle=\color{black},                          
identifierstyle=\color{black},
commentstyle=\color{gray},
xleftmargin=3.4pt,
xrightmargin=3.4pt,
numbers=none,
literate={*}{{\char42}}1
         {-}{{\char45}}1
         {\ }{{\copyablespace}}1
}
% http://www.monperrus.net/martin/copy-pastable-listings-in-pdf-from-latex
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}



<<setup, cache=FALSE, include=FALSE>>=
library(knitr)
opts_knit$set(tidy = T, fig=TRUE, fig.height = 4, fig.width=4, fig.align='center')
render_listings()
@

<<echo=FALSE, cache=TRUE, results='hide', message=FALSE, warning=FALSE>>=
set.seed(123)
@



\title{Grundlagen der\\Statistik}
\author{Florian Hartig}


\begin{document}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE} % don't activate this for knitr

\let\cleardoublepage\clearpage % No empty pages between chapters
\maketitle


\thispagestyle{empty}
\null


\href{http://www.uni-regensburg.de/biologie-vorklinische-medizin/theoretische-oekologie/mitarbeiter/hartig/index.html}{Prof. Dr. Florian Hartig}\\
University of Regensburg\\
Germany\\[0.5cm]

\begin{fullwidth}
Vorlesungsunterlagen für Studierende der

\begin{itemize*}
  \item BSc Biostatistik
  \item MSc Research Skills (Webseite \href{http://florianhartig.github.io/ResearchSkills/}{hier})
\end{itemize*}

\vspace{0.5cm}

Fehler oder Verbesserungsvorschläge bitte über \href{https://github.com/florianhartig/Statistics/issues}{issue tracker} des \href{https://github.com/florianhartig/Statistics/tree/master/EssentialStatistics}{GitHub repository} melden. 

\end{fullwidth}


\vfill
\begin{fullwidth}
Created 2014, University of Freiburg. Updated 2015, University of Freiburg. Updated 2016, University of Regensburg, and translated to German. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.
\end{fullwidth}


\newpage
\tableofcontents

\chapter{Einleitung} % Use chapters instead of sections

	\section{Ziel und Zielgruppe diese Textes}
	
	Ziel dieses Textes ist es, eine kurze, aber präzise Einführung in die Methoden und Philosophie der modernen Statistik zu liefern, sowie in die praktischen Arbeitsschritte, die typischerweise für die statistische Analyse von einfachen Experimenten und Beobachtungsstudien benötigt werden.
	
	\section{Themen der Statistik und Datenwissenschaften}
	
	Statistik, oder etwas weiter gefasste modernere Begriff "Data Science" = Datenwissenschaften, befasst sich mit der Visualisierung, Beschreibung und Interpretation von Daten, sowie mit dem Erstellen von datenbasierten Vorhersagen. Dieses Skript beinhaltet eine Einführung in die vier wichtigsten Säulen der statistischen Methodik für einen quantitativen Wissenschaftler:
	
	\paragraph{Deskriptive Statistik:} Die deskriptive Statistik\marginnote{Deskriptive Statistik = Abbildungen, Visualisierung, Kennzahlen der Statistik} beinhaltet beschreibende Analysen (Kennzahlen), beispielsweise Mittel- oder Medianwert, Korrelation, oder Assoziation, und die Visualisierung von Daten (Abbildungen, interaktive Visualisierung).
	
	\paragraph{Schließende Statistik:} Die schließende Statistik\marginnote{Schließende Statistik = Parameterschätzer, Hypothesentests, p-Werte, Modellvergleiche} (auch: induktive oder inferentielle Statistik) befasst sich damit, wie man aus Beobachtungen allgemeine Schlussfolgerungen ziehen kann. Das Problem hierbei ist systematische Abhängigkeiten von zufälligem "Rauschen"  zu trennen.\marginnote{Systematischer Effekt ist z.B. die Tatsache dass Patienten schneller heilen wenn Sie ein bestimmtes Medikament bekommen. Unterschiede können aber auch aus zufälligen Unterschieden (jeder Patient unterscheidet sich, unabhängig vom Medikament) kommen.} Um herauszufinden ob Abhängigkeiten in den Daten systematische oder zufällige Ursachen haben, macht man in der schließenden Statistik typischerweise eine Reihe von Annahmen, die in einem statistischen Modell zusammengefasst werden. Man sagt das statistisches Modell beschreibt den "`datengenerierenden Prozess"', indem es unsere Annahmen darüber zusammenfasst wie Variation und Muster in den Daten erstehen.\marginnote{Ein statistisches Modell beschreibt, wie Daten erstellt werden = datengenerierender Prozess} Auf Grundlage des Modells kann die schließende Statistik dann Wahrscheinlichkeitsaussagen für bestimmte Fragestellungen machen (z.B. wie wahrscheinlich wäre es die beobachteten Daten zu sehen, wenn man annimmt dass kein systematischer Effekt da ist). 
	
	\paragraph{Prädiktive Statistik und Maschinelles Lernen:} Prädiktive Statistik und maschinelles Lernen\marginnote{Maschinelles Lernen = Modelle deren Zweck vor allem Vorhersagen sind} bezeichnet Methoden die nicht auf Erkenntnis von Zusammenhängen, sondern auf Vorhersagen optimiert sind. Diese Methoden sind besonders wichtig im kommerziell immer wichtiger werdenden Feld der "künstlichen Intelligenz" und "Big Data". Big Data bezeichnet statistische Methoden für sehr große und komplexe Datensätze, wie  z.B. die Datensätze die von vielen Internetfirmen (Google, Amazon) gesammelt werden.\marginnote{Big Data = Methoden für große Datensätze, wie sie z.B. von vielen Internetfirmen (Google, Amazon) gesammelt werden} Viele Methoden der prädiktiven Statistik benutzen komplexe Algorithmen, die automatisch tausende von möglichen erklärenden Variable in einem Modell kombinieren. Diese Algorithmen machen oft hervorragende Vorhersagen, aber die Modell sind so komplex dass sich schwer verstehen lässt warum. Für wissenschaftliche Grundlagenforschung, in der man normalerweise an einem bestimmten Zusammenhang interessiert ist, werden daher eher schließende statistische Methoden verwendet. Firmen wie Amazon und Google konzentrieren sich jedoch eher auf prädiktive Methoden, da es hier darum geht aus großen Datenmengen möglichst genaue Vorhersagen zu machen, z.B. was Kunden interessiert, ohne dass das "`warum"' im Vordergrund steht. 
	
	\paragraph{Versuchsplanung:} Versuchsplanung oder experimentelles Design\marginnote{Versuchsplanung oder experimentelles Design = wie erzeugt man gute Daten?} umfasst alle Aspekte der Datenerzeugung, insbesondere Fragen wie "`Welche Variablen sollten erfasst werden?"', "`Wie viele Replikate werden benötigt?"', "`Wie sollten die Variablen in einem Experiment optimalerweise verändert werden?"'. Versuchsplanung ist offensichtlich von großer Wichtigkeit für die empirisch arbeitenden Wissenschaften (sowohl die Naturwissenschaften, als auch die empirischen Sozialwissenschaften). 
	
	\section{Die R Umgebung für wissenschaftliches Rechnen}
	
	Die Zeiten als man statistische Analysen mit Papier und Bleistift, oder später mit einem Taschenrechner durchführen konnte sind vorbei. Heute werden statistische Analysen praktische ausschließlich am Computer durchgeführt. Hierfür gibt es eine Reihe von wichtigen Programmen, die alle verschieden Stärken und Schwächen haben \marginnote{Eine Aufstellung von verschiedenen Statistikprogrammen finden sie \href{https://www.inwt-statistics.de/blog-artikel-lesen/Statistik-Software-R_SAS_SPSS_STATA_im_Vergleich.html}{hier}}.
	
	In diesem Text werden alle Beispiele mit dem Statistikprogramm R berechnet. Die Vorteile von R sind:
	
		\begin{itemize}
		\item Es ist kostenlos (open source)
		\item Es ist eine Skriptsprache 
		\item Im Bereich Biologie / Ökologie hat R mit Abstand den größten Funktionsumfang aller verfügbaren Programme
		\item Außerdem ist es in diesem Bereiche das mit Abstand am weitesten verbreitete Programm
	\end{itemize}
	

Also, R ist einfach der Standard, und Sie sollten sich an R gewöhnen wenn Sie es nicht schon kennen. Wir werden in diesem Text immer wieder Beispiele mit R sehen, und mehrere Appendices am Ende zeigen, wie man die hier behandelten Inhalt in R berechnen würde. Trotzdem will dieser Text keine Einführung in die Spezifika der Sprache R geben. Hierzu gibt es schon viele andere Anleitungen. Ein Übersicht ist  \href{http://biometry.github.io/APES/R/R10-gettingStarted.html}{hier} verfügbar, inklusive Hilfestellungen zur Installation von R und zusätzlicher Software wir RStudio.
	
	\begin{figure}[]
		\begin{center}
			\includegraphics[width = 10cm]{rst_interface.png}
			\caption{RStudio ist der wohl beliebteste Editor für R. R ist eine script-basierende Sprache. Das heißt man kommuniziert nicht über Mauseingaben mit dem Computer, sondern anhand von schriftlichen Befehlen in der R Konsole, bzw. über ein Textdokuments, welches dann an die R Konsole geschickt wird. Nach kurzer Eingewöhnungszeit merkt man, wie komfortabel und vorteilhaft diese Vorgehensweise ist - so sind alle Schritte der Analyse in einem Textdokument aufgelistet, und man kann leicht die komplette Analyse nachvollziehen, oder wiederholen wenn Änderungen gemacht werden müssen.}
			\label{fig: Rstudio1}
		\end{center}
	\end{figure}
	

Der Rest dieser Einführung befasst sich mit den vier Grundthemen der Statistik, die in der Einführung genannt wurden: deskriptive Statistik, schließende Statistik, prädiktive Statistik und Versuchsplanung. Bevor wir uns diesen Themen widmen wollen wir uns aber erst noch einmal genauer mit dem befassen worum sich in der Statistik alles dreht: die Daten.
	
	\chapter{Daten, Zufall, und der datengenerierende Prozess}
	
		\section{Die Daten: Darstellung und Skalenniveaus}
	
	Ein typisches Experiment oder eine typische Beobachtungsstudie (mehr zu dem Unterschied später) liefert wiederholte Beobachtungen mehrerer Variablen (beispielhaft: Temperatur, Niederschlag, Produktivität der Vegetation an verschiedenen Standorten). Man kann sich einen solchen Datensatz als eine Tabelle vorstellen, in der jede Variable eine Spalte einnimmt, und jede Zeile einer Beobachtung entspricht. Natürlich gibt es noch viele andere mögliche Datenstrukturen (z.B. hierarchische Beziehungen / Netzwerke), aber die Tabellenform ist mit Abstand das häufigste Ergebnis von einfachen Experimenten.
	
Üblicherweise wird eine wissenschaftliche Studie durchgeführt um eine bestimmte Frage zu untersuchen. Das heißt es gibt eine Variable die uns besonders interessiert, und von der wir wissen wollen wie sie von anderen Variablen beeinflusst wird. \marginnote{Abhängige Variable = unser Fokus, Frage ist wie diese Variable von anderen Faktoren beeinflusst wird.}  Diese Variable wird als ``abhängige Variable'' bezeichnet (auch Antwort- oder Responsevariable), da wir wissen wollen, ob und wie sie von anderen Variablen abhängt. Anderen Variablen, die unsere Antwortvariable beeinflussen, nennt man "`erklärende Variablen"' (auch: Prädiktoren, Kovariaten oder unabhängige Variablen).\marginnote{Erklärende Variablen sind variable die potentiell einen Einfluss auf die abhängige Variable ausüben.} Ein Beispiel: in einer medizinischen Studie ist der Heilungserfolg typischerweise die abhängige Variable, während die Information ob ein Medikament gegeben wurde (Behandlung) oder nicht (Kontrolle) die erklärende Variable ist. 

Normalerweise \marginnote{Die Methoden der multivariaten Statistik befassen sich mit Situationen, in denen wir mehrere Variablen als abhängig betrachten} sind wir daran interessiert, wie eine Variable von externen Bedingungen abhängt. Es gibt aber auch Fälle in denen man gerne die Abhängigkeit von mehreren Variablen mit sich selbst, oder externen Faktoren betrachten will. Ein Beispiel wäre wie sich Arteigenschaften (Gewicht, Physiologie, ...) mit der Umwelt verändern. Das Auswerten solcher Daten wird als multivariate Statistik bezeichnet. Wir werden solche Methoden nur am Rande erwähnen - weitere Informationen über multivariate Statistik ist \href{http://biometry.github.io/APES/Stats/stats50-MultivariateStatistics.html}{hier} verfügbar.

Neben der Unterscheidung in abhängige und unabhängige Variablen unterscheiden sich Variable außerdem in ihrem Typ. Eine numerische Variable (z.B. Gewicht) ist konzeptionell anders als eine Variable, die nur entweder rot, grün, oder blau wird. Der Fachbegriff in der Statistik für den Typ ist das Wort "`Skalenniveau"'.\marginnote{Skalenniveau = Variablentyp. Wichtige Typen: ungeordnet (nominal), geordnet (ordinal), numerisch (metrisch)} Man unterscheidet die folgenden Typen:
	
	\begin{itemize}
	  \item Numerische (metrische) Variablen - metrische Variablen erkennt man daran dass sie sich wie Distanzen verhalten, z.B. können Sie in sinnvoller Weise addiert werden. Die meisten Messgrößen im Labor (Temperatur, Konzentration, Zeit, Längen etc. sind metrisch). Zwei für die Statistik wichtige Untertypen sind
		\begin{itemize}
  		\item Kontinuierliche metrische Variablen, z.B. Temperatur
  		\item Ganzzahlig metrische Variablen. Hierzu zählt der wichtige Spezialfall der Zähldaten, z.B. 0,1,2,3, ...
		\end{itemize}	  
		\item Geordnete (ordinale) Variablen (klein, mittel, groß)
		\item Ungeordnete (nominale) Variablen (z.B. rot, grün, blau). Ein Spezialfall sind die binären (dichotome) Variablen (z.B. verstorben / überlebt) die oft durch 0/1 codiert werden,
	\end{itemize}
	
	Ordinale und nominal Variable werden manchmal auch als "kategorial" bezeichnet, weil sie sie nur eine feste Auswahl von Werten (Kategorien) annehmen. 
	
	Bei statistischen Auswertungen ist es äußerst wichtig sich klarzumachen, welches Skalenniveau die verschieden Variable haben. Wie wir später lernen werden, bestimmt das Skalenniveau welche Methoden der deskriptiven und schließenden Statistik angewandt werden können. Falls Sie ein Statistikprogramm (z.B. R) benutzen, wird dieses Programm typischerweise versuchen automatisch festzustellen, welches Skalenniveau vorliegt (wenn es eine Spalte mit "rot", "grün" gibt kann das offensichtlich nicht numerisch sein), und je nach Skalenniveau automatisch bestimmte Entscheidungen für Abbildungen und statistisch Verfahren treffen.\marginnote{Bei der Arbeit am Computer muss man immer prüfen, ob den Variablen das richtige Skalenniveau zugewiesen wurde.} In vielen Fällen ist es aber nicht möglich automatisch zu erkennen ob sie, z.B., eine Variable nominal oder ordinal behandeln wollen. Nach dem Einlesen der Daten in ein Statistikprogramm müssen Sie deshalb immer kontrollieren, ob das Skalenniveau jeder Variable korrekt kodiert ist. 
	
	Ein weitere Tip: die Erfahrung zeigt, dass viele Studenten dazu neigen, Variablen die eigentlich numerisch sind kategorial zu codieren. Als Beispiel: gemessen wird das Gewicht eines Tieres, aufgeschrieben als leicht, mittel und schwer.\marginnote{Verwende niemals eine kategoriale Codierung für Messgrößen, die eigentlich metrisch sind!} Die Idee die ich oft als Erklärung höre ist dass die Messung ungenau war und die kategoriale Einteilung dem Rechnung tragen würde. Kurz gesagt: Das tut es ganz und gar nicht. Die kategoriale Einteilung erzeugt nur zusätzliche Probleme. Verwende niemals eine kategoriale Codierung für Messgrößen, die eigentlich metrisch sind, egal wie groß der Messfehler ist!

	\section{Der Zufall: Stichprobe, Populationen und der datengenerierende Prozess}
	
	Daten sind der eine Grundpfeiler der Statistik. Der andere ist der Zufall.\marginnote{Daten entstehen aus einer Mischung von systematischen und zufälligen Prozessen.} Die Idee ist dass es systematische Beziehungen in und zwischen Variablen in den Daten gibt, über die wir gerne etwas lernen würden. Diese Beziehungen sind aber von zufälliger Variabilität in den beobachteten Variablen überdeckt. Um die systematischen von den zufälligen Prozessen zu trennen brauchen wir die Methoden der Statistik, insbesondere der schließenden Statistik. Mehr dazu später. Jetzt wollen wir uns zuerst mit der Frage beschäftigen: wie entsteht überhaupt der Zufall in den Daten?
	
	Stellen Sie sich vor,\marginnote{Die Population ist die Menge aller Beobachtungen, die man machen können. Die Stichprobe sind die Beobachtungen, die tatsächlich gemacht wurden.} dass wir gerne wissen würden wie schnell ein Baum in Deutschland im Durchschnitt wächst. Um diese Frage so exakt wie möglich zu beantworten würde man idealerweise alle Bäume in Deutschland wiederholt messen. Dies ist aber offensichtlich unpraktikabel. Also misst man nur eine kleine Menge von normalerweise zufällig ausgewählten Bäumen, und hofft dass die Eigenschaften dieser Bäume ähnlich (=repräsentativ) für die Eigenschaften aller deutschen Bäume sind. Der statistische Terminus für 'alle Bäume in Deutschland' lautet "`Population"' und die Bezeichnung der ausgewählten gemessenen Bäume ist die "`Stichprobe"'.
		
	Ich hoffe es ist einsichtig, dass sich die Stichprobe\marginnote{Stichproben erzeugt Zufälligkeit.} bei repräsentativer Auswahl (mehr dazu im Kapitel zur Versuchsplanung) ähnlich verhält wie die ganze Population. Das heißt wenn wir statistische Berechnungen auf der Stichprobe ausführen können wir erwarten, dass die resultierenden Werte der Population ähnlich. Die Vorstellung ist dass die Werte der Stichproben um den wahren Wert der Population schwanken. Um ein Beispiel zu geben kehren wir zu den Bäumen zurück: aus logistischen Gründen können wir jedes Jahr nur eine Stichprobe von 1000 Bäumen untersuchen. Wenn wir die Wachstumsrate dieser 1000 Bäume berechnen wird sie ähnlich, aber nicht gleich der Wachstumsrate der Population aller Bäume in Deutschland sein, und wenn wir das jedes Jahr wiederholen wird der Wert der Stichprobe in einem Jahr über, und in einem anderen Jahr unter der Rate der ganzen Population sein. 
	
	Der Prozess der\marginnote{Die Stichprobe ist nur einer der vielen Prozesse die Zufälligkeit erzeugen.} Stichprobe ist eine Erklärung wie der Zufall in unsere Daten entsteht. Viele klassische statistische Lehrbücher konzentrieren sich ausschließlich auf diesen Prozess, wodurch sich das Problem der Statistik auf das Problem reduziert die Eigenschaften der Population aus den Eigenschaften der Stichprobe zu schätzen. Leider stößt diese relativ einfache Denkmodell bei vielen komplexeren statistischen Problem an seine Grenzen. Stellen Sie sich z.B. vor Sie erhalten Daten die durch den folgenden Prozess erstellt wurden: eine Person geht zu zufällig ausgewählten Punkten in einem Wald und ermittelt die Strahlung die von der Vegetation absorbiert wird. Beim Messprozess entsteht Variation durch die innerhalb von Minuten wechselnde Bewölkung, der Fehler des Messinstruments, und dadurch dass die messende Person manchmal Ablesefehler macht. Man kann sich die Erzeugung dieser Daten natürlich trotzdem als das Ziehen aus einer abstrakten Population vorstellen, aber meiner Erfahrung nach führt diese Idealisierung oft zu Verwirrung. 
	
	
	Eine moderneres und allgemeineres Gedankenmodell für die Entstehung von Daten ist der "`datengenerierende Prozess"'.\marginnote{Der datengenerierende Prozess beschreibt, wie Zufälligkeit in einer Stichprobe entsteht.} Der datengenerierende Prozess beschreibt, wie die Beobachtungen aus einer Reihe systematischer oder zufälliger Prozesse entstehen. Dies beinhaltet den Prozess der Stichprobenerhebung, aber auch die vielen anderen Vorgänge, die zu systematische und zufällige Mustern in den Daten führen. Mit dieser Vorstellung können wir jetzt sagen: das Ziel der Statistik ist es, aus den Daten auf die Eigenschaften des datengenerierenden Prozesses zu schließen. 
	
	Egal ob man die klassische Vorstellung der Stichprobe, oder die flexiblere Vorstellung des datengenerierenden Prozesses präferiert: die Grundidee ist, dass die Daten / Stichprobe durch eine Mischung aus systematischen und zufälligen Prozessen entstehend. Man muss deshalb gedanklich zwei Objekte strikt trennen:\marginnote{Die deskriptive Statistik beschreibt die Daten / Stichprobe, während die schließende Statistik die Eigenschaften der unterliegenden Population / des datengenerierenden Prozesses beschreibt.} die Eigenschaften der Daten / Stichprobe (z.B. Mittelwert, Minimum, Maximum), und die Eigenschaften der Population oder des datengenerierenden Prozesses. Daten / Stichprobe sind uns vollständig bekannt. Was zu tun bleibt ist diese Information möglichst kompakt darzustellen. Dies ist die Aufgabe der deskriptiven Statistik, mit der wir uns im nächsten Kapitel beschäftigen werden (siehe Kapitel~\ref{ch: deskriptive Statistik}). Die Eigenschaften der Population / des datengenerierenden Prozesses aber sind uns nicht direkt bekannt. Wir müssen sie aus den Daten schätzen. Dies ist die Aufgabe der schließenden Statistik (siehe Kapitel~\ref{ch: inductive statistics}). Die prädiktive Statistik ist eine Mischung von beiden Standpunkten: Ziel hier ist es aus einer Stichprobe die Eigenschaften der nächsten Stichprobe vorherzusagen, ohne aber notwendigerweise explizit die Population oder der datengenerierenden Prozess zu schätzen. 
	
	
	\vspace{1cm}
	\begin{fullwidth}
		\begin{mdframed}
			
			\textbf{Anwendung in R:} 
			
			Die grundlegende Datenstruktur in R ist der "data.frame". Ein data.frame ist eine Datentabelle, wobei jede Spalte einen anderen Datentyp (Skalenniveau) haben kann. Mögliche Typen sind:
			
			\begin{itemize*}
				\item integer - ganzzahlig
				\item numeric - kontinuierliche Zahlen (Gleitkommazahl)
				\item boolean - wahr / falsch
				\item factor - kategoriale Variablen. Der Standard ist ungeordnet (z.B. rot, grün, blau), aber man kann auch einen geordneten factor (z.B. klein, mittel, groß) erzeugen.
			\end{itemize*}
			
			Außerdem taucht in realen Datensätzen oft der Wert "NA" auf. Dies ist die Bezeichnung für eine fehlende Beobachtung. Ähnlich, jedoch nicht identisch, ist "`NaN"' (not a number), was bei einer nicht ausführbaren Rechnung als Ergebnis auftreten kann.
			
			Der einfachste Weg um Daten als data.frame in RStudio einzulesen ist der "Import Dataset" im oberen rechten Teil des RStudio Editors. Natürlich geht es aber auch per Skript. Genaueres kann unter "Handling data in R " im Anhang~\ref{HandlingDataInR}, oder auf \href{http://biometry.github.io/APES/R/R20-DataStructures.html}{der Seite hier} nachgelesen werden.
			
			\textbf{Wichtig:} Nachdem die Daten eingelesen wurden muss man immer überprüfen ob die Werte und auch der Typ richtig eingelesen wurde. Ein häufiges Problem ist das numerische Werte als factor eingelesen werden (z.B. reicht ein versehentlicher Buchstabe in den Rohdaten so dass R die Spalte als Faktor interpretiert). Um die Daten zu überprüfen ist die R Funktion str(), also str(TheNameOfMyData) hilfreich. Der str Befehl steht für "structure", und zeigt dem Benutzer die Struktur inklusive Typ eines Datenobjektes an. 			
		\end{mdframed}
	\end{fullwidth}
	
	
	\chapter{Deskriptive Statistik und Visualisierung}\label{ch: deskriptive Statistik}
	
	Deskriptive Statistik\marginnote{Wie man Daten erstellt wird später noch detaillierter im Kapitel ~\ref{ch: design of experiments} Versuchsplanung behandelt.} behandelt die Zusammenfassung und Veranschaulichung von Daten.
	
	\section{Kennzahlen der Statistik}
	
	Kennzahlen der Statistik\marginnote{Kennzahlen der Statistik fassen Daten numerisch zusammen} bezeichnen Rechenvorschriften (z.B. Mittelwert oder Standardabweichung), die Eigenschaften von Datensätzen numerisch zusammenfassen. Kennzahlen dienen also zur kompakten Veranschaulichung der Eigenschaften eines Datensatzes, oder zu deren Vergleich.
	
	\subsection{Univariate Verteilungen - Kennzahlen einer Variable}
	
	Ein Grundbegriff in diesem Zusammenhang ist die Verteilung. Die Verteilung einer oder mehrerer Variablen beschreibt die Häufigkeit des Vorkommens verschiedener Werte dieser Variablen. Genau genommen muss man zwei Verteilungen unterscheiden: die Verteilung der Population / des datengenerierenden Modells, und die Verteilung der Stichprobe. 	
	
	
	Eine übliche Situation bei der wir von den Kennzahlen der Statistik Gebrauch machen, ist die wiederholte Beobachtung einer stetigen Variable. Als Beispiel kann man sich hier vorstellen, man habe 2000 Bäume gemessen und anschließend erneut vermessen. Man konnte eine Vorkommen an zunehmendem Durchmesser beobachten (siehe Abb.\ref{fig: data distribution}).


\begin{figure}[htbp]
\begin{center}
<<echo=FALSE>>=
seed = (123)
parameter = seq(-2,8,len=500)

mix <- rbinom(2000,1,0.5)
samples = mix * rnorm(2000, mean = 3) + (1-mix)*rnorm(2000, mean = 0.5, sd=0.5)

distr = (dnorm(parameter, mean = 3) + dnorm(parameter, mean = 0.5, sd=0.5)) /2

plot(parameter,distr, type = "l", ylab = "Frequency", xlab = "Diameter growth [cm]", main = "Tree diameter growth data", col = "red", lwd = 2, ylim = c(0,0.5), lty = 2)
hist(samples, breaks = 40, add = T,freq = F, col = "#99999930")
# polygon(parameter, distr, border=NA, col="darksalmon")

MLEEstimate <- parameter[which.max(distr)]
# abline(v=MLEEstimate, col = "red")
#text(3.1,0.4, "Mode", col = "red", cex = 1.5)
@
\caption{Eine Verteilung von beobachteten zunehmenden Durchmessergrößen (graue Balken). Wir nehmen an (in diesem Fall wissen wir es), dass diese Werte von wahren Verteilungen stammen (Population oder datengenerierender Prozess) die hier in einer rot-gestrichelten Linie angedeutet werden. Wenn wir nun mehr Daten aufzeichnen würden, würden sich die grauen Balken der wahren Verteilung immer mehr annähern.}
\label{fig: data distribution}
\end{center}
\end{figure}


Wie kann man nun die Eigenschaften der beobachteten Stichprobe zusammenfassen? Ein paar grundlegende Eigenschaften wären beispielsweise das Minimum und das Maximum, der Mittelwert, oder der Modalwert (die Maximalverteilung, d.h. der Wert der höchsten Beobachtungsdichte). Außerdem  gibt es zwei weitere Kennzahlen, die ebenso oft Gebrauch finden: Moment und Quantil.


Der Begriff "`Moment"' mag vielleicht nicht jedem geläufig sein, vermutlich wurde aber schon einmal das erste und zweite Moment der Verteilung berechnet. Diese sind ebenfalls unter den Bezeichnungen Mittelwert und Standardabweichung bekannt. Allgemein wird das n-te Moment $\mu_n$ einer Verteilung $f(x)$ um einen Wert c definiert als 

\begin{equation}
\mu_n(c) = \int_{-\infty}^{\infty} f(x) (x - c)^n dx
\end{equation}

oder, für eine endliche Anzahl an Beobachtungen 

\begin{equation}
\mu_n(c) = \frac{1}{N}\sum_{i=1}^N (x_i - c)^n
\end{equation}

Das erste Moment mit $c=0$, ist der Mittelwert. Für die folgenden höheren Momente ist es üblich die Zentralmomente zu betrachten, die man durch das Setzen von c auf den Mittelwert erhält, da ihre Werte einfacher als Indikatoren für die Verteilungsform zu interpretieren sind.\footnote{Um die Varianz abzuschätzen, ersetzt man oft den Term 1/N durch den Bias-korrigierten Term 1/(N-1).} Die drei höheren Zentralmomente werden Varianz (n=2, identisch mit dem Quadrat der Standardabweichung, Messgröße der Streuung), Schiefe (n=3, Messgröße für die Asymmetrie in der Verteilung) und Wölbung (n=4) genannt. 

Quantilen stellen die zweite Zentralklasse der Kennzahlen-Statistik dar, um die kontinuierliche Verteilung zu beschreiben. Wenn wir eine Verteilung wie in der oben gezeigten Abbildung vorfinden, kann man sich die Frage stellen: Welcher ist der entscheidende Wert, der den Datensatz in zwei Hälften trennt, sodass eine Hälfte der beobachten Daten kleiner und die andere Hälfte größer als dieser Wert ist?\marginnote{Eine Hälfte der Daten ist kleiner und die andere Hälfte ist größer als die 0.5 Quantile, welches auch als Medianwert bezeichnet wird} Dieser Punkt wird als Medianwert, oder auch als 0.5-Quantil bezeichnet. Allgemeiner kann man sagen, dass die 0.x Quantile der Wert ist, an dem der Anteil 0.x des Datensatzes kleiner ist. 

\subsection{Korrelation - Erläuterung der Abhängigkeit stetiger Variablen}

Ein weiterer wichtiger Bereich der Kennzahlen-Statistik ist die Korrelation. Korrelationsstudien messen die Abhängigkeit von stetigen Variablen. Leider gibt es eine Menge Korrelations-Messgrößen, die es zu unterscheiden gilt. Die zwei wichtigsten sind:

\paragraph{Lineare Koeffizienten:}Lineare Koeffizienten, mit dem "`Pearson'schen Korrelationskoeffizienten"', als dem wohl Häufigsten, ermitteln die lineare Abhängigkeit zwischen zwei Variablen. Der Pearson'sche Korrelationskoeffizient wird wegen seiner schnellen Berechnung und leichten Interpretation am häufigsten verwendet. Jedoch kann es hier zu Fehlern kommen, wenn die Variablen nicht linear abhängig voneinander sind. Fig.~\ref{fig: correlation}.

\paragraph{Rangkorrelationskoeffizient:} Rangkorrelationskoeffizient, wie zum Beispiel der "`Spearman'sche Rangkorrelationskoeffizient"' und der "`Kendall Tau Rangkorrelationskoeffizient"' ermitteln, wie genau die Variablen gemeinsam tendenziell steigen oder fallen, ohne hierbei das Ausmaß oder die Linearität ihrer Steigung zu beachten. Diese werden bevorzugt, wenn man von zueinander nicht-linearen Variablen ausgeht. 

\paragraph{Starke Korrelation != bedeutender Einfluss:} Siehe auch Fig.~\ref{fig: correlation}; ist ein oft falsch verstandener Zustand der Korrelation und Abhängigkeit - ein hoher Korrelationskoeffizient bedeutet nicht, dass eine Variable eine starke Reaktion auf eine andere bewirkt. Um eine hohe Korrelation zu erreichen braucht es nur eine geringe Streuung um die Korrelationslinie (siehe mittlere Reihe - die Auswirkung ist anders, aber die Korrelation bleibt gleich). 


\begin{figure}[htbp]
\begin{center}
<<echo = F>>=
# From https://en.wikipedia.org/wiki/File:Correlation_examples2.svg

#Title: An example of the correlation of x and y for various distributions of (x,y) pairs
#Tags: Mathematics; Statistics; Correlation
#Author: Denis Boigelot
#Packets needed : mvtnorm (rmvnorm), RSVGTipsDevice (devSVGTips)
#How to use: output()
#
#This is an translated version in R of an Matematica 6 code by Imagecreator.

library(mvtnorm)

MyPlot <- function(xy, xlim = c(-4, 4), ylim = c(-4, 4), eps = 1e-15) {
   title = round(cor(xy[,1], xy[,2]), 1)
   if (sd(xy[,2]) < eps) title = "" # corr. coeff. is undefined
   plot(xy, main = title, xlab = "", ylab = "",
        col = "darkblue", pch = 16, cex = 0.2,
        xaxt = "n", yaxt = "n", bty = "n",
        xlim = xlim, ylim = ylim)
}

MvNormal <- function(n = 1000, cor = 0.8) {
   for (i in cor) {
      sd = matrix(c(1, i, i, 1), ncol = 2)
      x = rmvnorm(n, c(0, 0), sd)
      MyPlot(x)
   }
}

rotation <- function(t, X) return(X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2))

RotNormal <- function(n = 1000, t = pi/2) {
   sd = matrix(c(1, 1, 1, 1), ncol = 2)
   x = rmvnorm(n, c(0, 0), sd)
   for (i in t)
      MyPlot(rotation(i, x))
}

Others <- function(n = 1000) {
   x = runif(n, -1, 1)
   y = 4 * (x^2 - 1/2)^2 + runif(n, -1, 1)/3
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1/3, 1+1/3))

   y = runif(n, -1, 1)
   xy = rotation(-pi/8, cbind(x,y))
   lim = sqrt(2+sqrt(2)) / sqrt(2)
   MyPlot(xy, xlim = c(-lim, lim), ylim = c(-lim, lim))

   xy = rotation(-pi/8, xy)
   MyPlot(xy, xlim = c(-sqrt(2), sqrt(2)), ylim = c(-sqrt(2), sqrt(2)))
   
   y = 2*x^2 + runif(n, -1, 1)
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1, 3))

   y = (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   y = cos(x*pi) + rnorm(n, 0, 1/8)
   x = sin(x*pi) + rnorm(n, 0, 1/8)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   xy1 = rmvnorm(n/4, c( 3,  3))
   xy2 = rmvnorm(n/4, c(-3,  3))
   xy3 = rmvnorm(n/4, c(-3, -3))
   xy4 = rmvnorm(n/4, c( 3, -3))
   MyPlot(rbind(xy1, xy2, xy3, xy4), xlim = c(-3-4, 3+4), ylim = c(-3-4, 3+4))
}

 par(mfrow = c(3, 7), oma = c(0,0,0,0), mar=c(2,2,2,0))
 MvNormal(800, c(1.0, 0.8, 0.4, 0.0, -0.4, -0.8, -1.0));
 RotNormal(200, c(0, pi/12, pi/6, pi/4, pi/2-pi/6, pi/2-pi/12, pi/2));
 Others(800)

@
\caption{Beispiele einer möglichen Korrelation mit dem Pearson'schen Korrelationskoeffizienten. Beachte, dass viele Datensätze, die eine klare Abhängigkeit zwischen Variablen aufweisen, einen Pearson'schen Korrelationskoeffizienten von 0 aufweisen, da die Abhängigkeit nicht linear ist.}
\label{fig: correlation}
\end{center}
\end{figure}

\subsection{Kreuztabellen - Erläuterung unstetiger Ergebnisse mehrerer Variablen}

Zu guter Letzt\marginnote{Dieser Datensatz von Berkeley ist ein berühmtes Beispiel für das Simpson-Paradoxon. Weitere Informationen auf Wikipedia über diese wichtige statistische Falle.} gibt es noch die klassischen Kreuztabellen, um binäre oder kategorische Daten zu beschreiben. Hier ist ein Auszug eines üblichen Datensatzes aus R über Sammeldaten von Hochschulbewerber in Berkeley aus dem sechst-größten Departments im Jahre 1973, geordnet nach Zulassung und Geschlecht. Hier wird nur das erste Department gezeigt.

<<>>=
UCBAdmissions[,,1]
@


\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Für die vielseitigen Berechnungsmöglichkeiten der deskriptiven Statistik in R, siehe \href{http://www.uni-kiel.de/psychologie/rexrepos/rerDescriptive.html}{hier}

\end{mdframed}
\end{fullwidth} 


\section{Visualisierung}

\marginnote{Gib ?anscombe in R ein, um den Code zu sehen, der diese Plots ausgibt und um die statistischen Eigenschaften des Datensatzes zu berechnen.}

Kennzahlen sind sehr nützlich, können aber auch zu Fehlinterpretationen führen. Ein bekanntes Beispiel dafür ist das Anscombe Quartett, ein hypothetischer Datensatz aus vier Stichproben, die identisch bzgl. üblicher Kennzahlen wie z.B. Mittelwert, Varianz, Korrelation, Regressionslinie, etc. sind, aber sehr unterschiedliche Verteilungen aufweisen \citep{Anscombe-Graphsinstatistical-1973}. Das Beispiel zeigt, dass die Berechnung von einfachen Kennzahlen kein vollständiger Ersatz für die graphische Darstellung der Daten darstellt.


\begin{figure}[htbp]
\begin{center}
<<echo = F, results="hide">>=

require(stats); require(graphics)
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  print(anova(lmi))
}

## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))

## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)

@
\caption{Anscombe's Quartet, ein hypothetischer Datensatz aus vier Beobachtungen, ein hypothetischer Datensatz aus vier Stichproben, die identisch bzgl. üblicher Kennzahlen wie z.B. Mittelwert, Varianz, Korrelation, Regressionslinie, etc. sind, aber sehr unterschiedliche Verteilungen aufweisen.}
\label{fig: Anscombes Quartet}
\end{center}
\end{figure}

\subsection{Grundsätze der Visualisierung}

Ein Grundsatz\marginnote{Beispiele für missverständliche Abbildungen \href{https://en.wikipedia.org/wiki/Misleading_graph}{siehe}} von Abbildungen und Darstellungen ist, die Daten so einsehbar und wahr wie möglich darzustellen. Der Leser sollte einen bestmöglichen überblick in kürzester Zeit über die Daten erhalten. Zusätzlich sollten die Graphen natürlich auch anschaulich sein. Vorab ein paar generelle Tipps, die vielleicht helfen könnten:

\begin{itemize}
\item Einfach ist besser als kompliziert
\item Vermeide übertriebene Farben. Graphen sollten wenn möglich auch in schwarz-weiß leserlich sein (benutze einen Farbgradienten, der gleichzeitig ein Intensitätsgradient ist; benutze zusätzlich zu Farben auch gestrichelte Linien). Wenn dein Graph auf Farben basiert, achte darauf welche zu verwenden, die auch Menschen mit einer Rot-Grün-Schwäche erkennen können.
\item Ehrlichkeit: vermeide Verzerrungen. Benutze quadratische Formen, außer es gibt besondere Gründe. Achsen sollten bei 0 beginnen, außer es gibt gute Gründe, die dagegen sprechen. Verwende die gleiche Skala bei der Darstellung mehrerer Abbildungen, wenn es keine Gründe gibt, die dagegen sprechen. 
\item Manipuliere deine Abbildungen nicht!
\item Ausgabe in einem Vektor-Format (pdf, eps, svg)
\end{itemize}

\begin{figure}[htbp]
\begin{center}

\setkeys{Gin}{width=\textwidth}
<<echo = FALSE>>=
par(mfrow = c(2,2))

plot(co2, ylab = expression("Atmospheric concentration of CO"[2]),
     las = 1)
title(main = "a) Athmospheric CO2 Hawai")

plot(airquality$Temp, airquality$Ozone, xlab = "Temperature", ylab = "Ozone", main = "b) Temperature vs. Ozone")

barplot(table(mtcars$gear, mtcars$cyl), beside=T, xlab = "cylinders", main = "c) Gears / cylinders of cars")
legend("top", legend = c("3 gears", "4 gears", "5 gears"), pch = 15, col = gray.colors(3, end = 0.7), bg = "#FFFFFF66", bty = "n")

boxplot(weight ~ group, data = PlantGrowth, main = "d) Plant Growth",
        ylab = "Dried weight of plants", col = "lightgray",
        notch = F, names = c("control", "classical music", "heavy metal"), las = 1)
@
\caption{Vier typische Plot-Typen, von oben links nach unten rechts: a) Liniendiagramm, repräsentiert stetige Messungen einer Variablen; b) Streudiagramm, repräsentiert eine Beziehung zwischen zweier stetigen Variablen; c) Balkendiagramm, repräsentiert Messungen in unstetigen Gruppen / Variablen; d) Boxplot, repräsentiert stetige Messungen in unstetigen Gruppen.}
\label{fig: exaple plots}
\end{center}
\end{figure}


\subsection{Graphen-Typen}

Es gibt eine große, fast unendliche Bandbreite an verschiedenen visuellen Darstellungen von Datensätzen. Im Folgenden werden nun vier häufig verwendete Graph-Typen gezeigt. 

\paragraph{Liniendiagramme:} Liniendiagramme werden benutzt, um stetige, geordnete Messwerte darzustellen. Typische Beispiele hierfür wären Zeitreihen, stetige Parameterveränderungen oder mathematische Funktionen. Beispiel siehe Fig.~\ref{fig: exaple plots} a.

\paragraph{Streudiagramme:} Streudiagramme veranschaulichen zwei stetige Variablen die paarweise gemessen wurden. Ein typisches Beispiel hierfür wäre das wiederholte Messen von verschiedenen Variablen mit der Absicht herauszufinden, ob diese korrelieren. Beispiel siehe Fig.~\ref{fig: exaple plots} b.

\paragraph{Balkendiagramme:} Balkendiagramme beinhalten Informationen (Zählungen oder stetige Variablen) über unstetige Gruppen. Beispiel siehe Fig.~\ref{fig: exaple plots} c.

\paragraph{Boxplots:} Boxplots benutzt man häufig bei der Verteilung einer stetigen Variable über mehrere unstetige Gruppen. Klassischerweise bestehen sie aus einer Box, 'Whiskers' (Linien) und gegebenfalls Punktewerte um die 'Whiskers'. Deren Bedeutung ist von der benutzten Software abhängig, mit der man die Plots gestaltet hat. Normalerweise bedeckt die Box die in der Mitte liegenden 50\%, mit dem angedeuteten zentralen Medianwert. Die Linien sollen zur Abschätzung der Daten-Bandbreite dienen, Ausreißer ausgenommen. Natürlich liegt es im Auge des Betrachters, was als Ausreißer angesehen wird und was nicht. Die exakte Definition von Whiskers besagt, dass die weit entfernteste Beobachtung weniger oder gleich der oberen Quantile plus 1,5 der Länge der Interquartilenbreite ist. Beispiel siehe Fig.~\ref{fig: exaple plots} d.

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Es gibt viele gute Einleitungen in Graphiken mit R, sodass diese Details hier nicht weiter aufgeführt werden. Zu Beginn empfehle ich diese  \href{https://github.com/florianhartig/ResearchSkills/tree/master/Labs/Statistics/Practicals/GraphicsInR}{übungen zum graphischen Gestalten mit R}. Diese sind begleitend zu diesem Skript, oder auch zu

\begin{itemize*}
  \item \href{http://www.statmethods.net/graphs/index.html}{QuickR}
  \item \href{http://shinyapps.org/apps/RGraphCompendium/index.php}{RGraphCompendium}
  \item \href{http://www.uni-kiel.de/psychologie/rexrepos/rerDiagrams.html}{rexrepos}
\end{itemize*}

\end{mdframed}
\end{fullwidth} 


\chapter{Schließende Statistik}\label{ch: inductive statistics}

\marginnote{Schließende Statistik ist das Ziehen von Schlüssen aus Beobachtungen durch statistische Methoden} 

Schließende Statistik behandelt das Schlussfolgern, z.B. das Rückschlüsse ziehen aus Beobachtungen. Ein Beispiel einer solchen Folgerung wäre beispielsweise die Beobachtung, dass 15 von 20 Testobjekte, die eine bestimmte Medikation erhielten (Behandlungsgruppe), eine Verbesserung ihres Zustandes zeigten --> Statistische Folge: Wir können davon ausgehen, dass es eine Wahrscheinlichkeit von X gibt, dass die Medikation einen positiven Effekt hat.


\section{Datenerzeugendes Modell}

\marginnote{Schließende Statistik ist nicht immer, aber meist mit dem Prinzip des datengenerierenden Modells verknüpft}

Eine zentrale Idee von vielen Methoden der Schließende Statistik ist das Prinzip des datengenerierenden Modells. Kurz gesagt beinhaltet unser datengenerierendes Modell unsere festgelegte Annahme, wie die Daten entstehen (normalverteiltes Rauschen, lineare Reaktion). Daraus kann man dann abhängig unserer festgelegten Annahme die unbekannten Größen (Unterschiede zwischen Behandlungsgruppe und Kontrollgruppe) berechnen (schlussfolgern).

\marginnote{In der Statistik verwendet man oft das Wort "`behandelt"' um Veränderungen in experimentellen Einheiten zu beschreiben. Hier wären die zwei Musikarten "`Behandlung"' und das "`Nichts-tun"' die Kontrolle} 

Man stelle sich vor man möchte herausfinden, ob das Pflanzenwachstum von Musik beeinflusst werden kann. Hierzu nehmen wir zwei Töpfe, jeder mit einer Pflanze, wobei eine mit klassischer und eine andere mit Heavy Metal -Musik beschallt wird. Eine der beiden wird zwangsläufig höher wachsen, jedoch könnte dies reiner Zufall sein, da es immer Variationen in Wachstumsraten gibt.

Somit bedarf es mehrerer Wiederholungen. Nun nehmen wir im unten aufgeführten Beispiel 30 Töpfe.


\begin{figure}[htbp]
\begin{center}
<<echo = FALSE>>=

boxplot(weight ~ group, data = PlantGrowth, main = "Plant Growth",
        ylab = "Dried weight of plants", col = "lightgray",
        notch = F, names = c("control", "classical music", "heavy metal"))

@
\caption{Wachstumsmessungen unter verschiedenen Behandlungsweisen}\label{fig: plant growth music}
\end{center}
\end{figure}

\marginnote{Beachte die Interpretation eines Boxplots - die dickere Linie in der Mitte der Box ist der Medianwert. Die Box bedeckt die zentrale 0,5 Quantile der Verteilung}
Es scheint Unterschiede zwischen den drei Fällen zu geben, jedoch gab es auch Abweichungen in den Wachstumsraten innerhalb der einzelnen Behandlungsguppen (durchschnittlich haben wir hier sieben Beobachtungen pro Behandlung). Somit ist es immer noch möglich, dass die Unterschiede in den Beobachtungen durch Zufall entstanden sind.

Wenn wir genaue Aussagen über die Wahrscheinlichkeit der Unterschiede zwischen den zwei behandelten und der Kontrollgruppe treffen möchten, müssen wir ein Modell erstellen, welches die stochastische Abweichung in den Daten beschreibt. Dies erlaubt uns wiederum Eigenschaften, wie z.B. die Zufallswahrscheinlichkeit der beobachteten Unterschiede berechnen zu können. Diese Annahmen sind das, was wir als statistisches Modell bezeichnen (oder auch: stochastische Verfahren, datengenerierendes Modell). 

Die üblichere Modellart für solche Zwecke ist das parametrische statistische Modell. Für die Daten, die wir hier haben, würde das parametrische statistische Modell annehmen, dass es eine durchschnittliche Wachstumsrate für jede Behandlungsart (Kontrolle, klassische und Heavy Metal Musik) gibt, jedoch das Wachstum jeder individuellen Pflanze mit einer normalverteilten Abweichung um ihre behandlungsspezifische durchschnittliche Wachstumsrate variiert. Die Parameter dieses Modells sind die unbekannten durchschnittlichen Wachstumsraten und die Abweichung der Normalverteilung. Diese Parameter werden dann mit Methoden, die hier in diesem Kapitel erklärt werden, an die Daten angepasst und basieren auf der Berechnung für z.B. die Wahrscheinlichkeit der Datenresultate, wenn keine Unterschiede zwischen den Gruppen wären.

Eine andere Möglichkeit datengenerierende Modelle zu erstellen sind nicht-parametrische Methoden.\marginnote{Die nicht-parametrische Statistik versucht Vermutungen über datengenerierende Prozesse zu vermeiden. Normalerweise entsteht der datengenerierende Prozess durch das Imitieren der eigentlichen Daten, z.B. durch das erneute Prüfen von Methoden.} Nicht-parametrische Methoden umgehen die Notwendigkeit Modellannahmen zu erstellen z.B. über Resamplingmethoden, in denen wiederholt Teile aus dem beobachteten Datensatz gezogen werden um so "künstlich" neue Daten zu generieren. Für Pflanzenwachstumsbeobachtungen unter drei verschiedenen Bedingungen (Behandlung) könnte man z.B. ein nichtparametrisches Modell für die Annahmen dass es keine Unterschiede in dem Wachstum gibt dadurch erstellen, dass man alle Beobachtungen ungeachtet der Behandlung in einen Topf wirft, sie dann zufällig den drei Gruppen zuweist, und ausrechnet wir groß die Unterschiede zwischen den Gruppen sind. Wenn wir dies nun oft wiederholen (z.B. 1000 mal) kann man einen guten Eindruck davon bekommen, wie wahrscheinlich es ist die beobachteten Unterschiede zu erhalten, wenn die Behandlungen keinen Effekt hätten.

Nicht-parametrische Methoden sind inzwischen ein wichtiger Bestandteil der modernen Statistik.\marginnote{Parametrische Methoden haben normalerweise eine höhere "Teststärke" (das Konzept wird gleich erklärt, aber grob heißt das dass sie eher einen Effekt finden wenn er da ist). Allerdings basieren alle Ergebnisse der parameterischen Statistik auf der Richtigkeit des parametrischen Modells. Wie man die Modellannahmen überprüft kann werden wir später in diesem Kapitel behandeln.} Ihr Vorteil liegt darin dass sie weniger Annahmen über die Daten treffen. Andererseits sind parametrische Methoden meist wesentlich schneller zu berechnen, und, falls ihre Annahmen korrekt sind, mit der selben Menge an Daten sensitiver (höhere Teststärke, siehe Kap.~\ref{ch: p-werte}), wodurch sie ehereinen vorhandenen Effekt erkennen. Deshalb sind die parametrische Methoden immer noch die Grundlage der meisten statistischen Analysen.


\section{Inferentielle Outputs}

Auf dem datengenerierendem Modell basierend (parametrisch oder nicht-parametrisch) können wir nun verschiedene schließenden Methoden anwenden, um Schlüsse über unsere Daten zu ziehen (in unserem Beispiel: Um entscheiden zu können, ob Musik einen Unterschied macht, oder nicht). In der "normalen" Statistik sind hier vor allem zwei Methoden zu nennen, die fast universell auf jedes Problem angewandt werden: p-Werte und Maximum-Likelihood-Schätzer. Ein weiteres drittes Verfahren wird momentan immer beliebter - die nachträglich, durch Bayes ermittelte Inferenz. Auf diese werde ich noch am Ende dieses Abschnittes eingehen.

\begin{figure}[htb]
\begin{center}
\includegraphics[width = 8cm]{InferenceDE}
\caption{Aus dem datengenerierenden Modell (parametrisch oder nicht-parametrisch) können die drei "Produkte" der schließenden Statistik gewonnen werden: a) der p-Wert, b) der Maximum-Likelihood Schätzer, und c) die Bayes'sche Posterior.}
\label{fig: InferenceMethods}
\end{center}
\end{figure}


\subsection{p-Werte}\label{ch: p-werte}

Der p-Wert ist die am häufigsten benutzte, aber leider auch am schwierigsten zu interpretierende Methode. Die Verwendung des p-Wertes basiert auf dem Ansatz des Hypothesentests (englisch: null hypothesis significance testing, NHST). Die Idee dahinter ist Folgende: wenn wir Beobachtungsdaten und ein statistisches Modell haben, können wir dieses statistische Modell benutzen, um zu testen ob die Daten zu einer festen, vorgegebenen Hypothese passen. Für das Beispiel der Pflanzen und der Musik könnte unsere Hypothese so lauten: "`Musik hat keinen Einfluss auf Pflanzen; alle beobachteten Unterschiede basieren auf zufälligen Abweichungen zwischen den Individuen."' Dieses Szenario wird als Nullhypothese bezeichnet. \marginnote{Eine Nullhypothese $H_0$ ist ein vorgegebenes Szenario, das Vorhersagen über erwartete Wahrscheinlichkeiten von verschiedenen Beobachtungen macht.} In der Praxis wird typischerweise wird als Nullhypothese ein Szenario von "keinem" Einfluss gewählt, so dass man dann zeigen kann das es einen Einfluss gibt. Technisch könnte man aber genauso die Vermutung "`klassische Musik verdoppelt die Wachstumsrate der Pflanzen"' als Nullhypothese setzen. Es liegt im Ermessen des Analytikers, bzw. an der Fragestellung, was als Nullhypothese betrachtet werden soll. Dies ist auch der Grund für die große Auswahlmöglichkeit an verfügbaren Tests. Wir werden noch einige davon im folgenden Kapitel über wichtige Hypothesentests kennen lernen.

Nachdem\marginnote{Die Teststatstik ist eine deskriptive Statistik die die beobachteten Daten und H0 vergleicht.} wir uns für eine Nullhypothese entschieden haben, wollen wir nun feststellen ob die Daten zu der Nullhypothese passen. Die Frage ist natürlich: wie definiert man denn "passen"? Die Strategie der Hypothesentests ist sich zuerst eine Teststatistik zu definieren. Die Teststatistik ist irgendeine deskriptive Statistik die den Datensatz beschreibt (einfachstes Beispiel: der Mittelwert). Im Prinzip steht es jedem frei sein eigenes Maß zu wählen, aber es gibt natürlich Teststatistiken die sinnvoller sind als andere. Die Nullhypothese und die Teststatistik zusammen spezifizieren die Annahmen eines Hypothesentests. 

Mit der Teststatistik gewappnet können wir jetzt die beobachteten Daten mit H0 vergleichen. Hierzu definieren wir den p-Wert 

\begin{equation}
p := p(d >= D_{obs} | H_0)
\end{equation}

als die Wahrscheinlichkeit die beobachteten Daten oder extremer (gemessen an der Teststatsitik) zu bekommen wenn H0 wahr ist (siehe Abb.~\ref{fig: teststatistik}).\marginnote{Der p-Wert ist die Wahrscheinlichkeit der beobachteten Daten oder extremer, gegeben H0.} 


\begin{figure}[htb]
\begin{center}
\includegraphics[width = 6cm]{Teststatistik}
\caption{Eine visualisierung von p-Wert und Teststatistik. Gegen H0 erwarten wir eine gewisse Verteilung der Teststatistik. Wenn $D_obs$ die rote Linie ist, so ist der p-Wert die Wahrscheinlichkeit eine größere Teststatistik zu bekommen als der Wert für $D_obs$.}
\label{fig: teststatistik}
\end{center}
\end{figure}

Ein kleiner p-Wert zeigt also an, dass die Wahrscheinlichkeit die beobachteten Daten zu bekommen für H0 sehr klein ist. Jetzt kommt ein Trick der erst mit der weiteren Erklärung richtig sinnvoll wird: Wenn der p-Wert unter eine gewisse Schwelle sinkt (das sog. Signifikanzlevel $\alpha$), so sagt man die "Nullhypothese wird abgelehnt", "es gibt signifikante Evidenz gegen die Nullhypothese", oder einfach: "der Test / Effekt ist siginfikant". Der Wert von $\alpha$ ist eine Übereinkunft, in der Ökologie üblicherweise 0.05, sodass ein p-Wert kleiner als 0.05 ein Abweisen der Nullhypothese veranlässt. \marginnote{Wenn p<0.05, haben wir signifikante Beweise für ein Abweisen der Nullhypothese.} 

Warum diese Definition Sinn macht sieht man wenn man sich überlegt was passiert wenn man diese Regel anwendet. Zuerst: wie oft wird denn ein Ergebnis signifikant wenn H0 wahr ist? Wenn man scharf auf die Definition schaut, sieht man: genau $\alpha$

\begin{itemize}
  \item H0 wahr: p(p-Wert < \alpha) = \alpha
\end{itemize}

Man weiß also dass, wenn H0 wahr ist, man genau $\alpha$ signifikante Ergebnisse bekommt. Man nennt diese falschen Positiven Tests den Typ I Fehler. Der Typ I Fehler ist also immer $\alpha$. 

Die andere Sache die passieren kann ist dass ein Ergebnis nicht signifikant wird, obwohl H0 falsch ist. Dieses nennt man falsche negative, oder Typ II Fehler. Den umgekehrten Wert, 1-TypII, also die Wahrscheinlichkeit Signifikanz zu bekommen wenn H0 nicht wahr ist, nennt man die Teststärke. 


Der Typ II Fehler ist im Allgemeinen nicht bekannt, kann aber berechnet werden wenn man den datengenerierenden Prozess und $\alpha$ kennt. Typischerweise nimmt der Fehler ab mit größerer Effektstärke, mehr Daten, größerem $\alpha$, und weniger Stochastizität (Rauschen). 

Ein Überblick über die Fehler ist in Fig.~\ref{fig: Error Types}

\begin{figure}[htb]
\begin{center}
\includegraphics[width = 9cm]{ErrorTypes}
\caption{E}
\label{fig: Error Types}
\end{center}
\end{figure}

Ein Problem der Hypothesentests und der p-Werte ist die notorische Fehlinterpretation der Ergebnisse. Der p-Wert ist NICHT die Wahrscheinlichkeit, dass die Nullhypothese wahr, oder die Alternativhypothese falsch ist \citep[siehe][]{Cohen-earthisround-1994}. Noch garantiert der kontrollierte Typ I Fehler, dass ein signifikantes Ergebnis mit einer festen Wahrscheinlichkeit richtig ist. 

Die Wahrscheinlichkeit das ein signifikantes Ergebnis falsch ist wird durch die False Discovery Rate (FDR) beschrieben. Hierzu überlegt man sich folgendes: ein signifikantes Ergebnis kann durch 2 Wege entstehen: entweder ein Typ I Fehler, der entsteht mit Wahrscheinlichkeit $\alpha$ wenn H0 falsch ist, oder ein echtes positives Ergebnis, das entsteht mit Wahrscheinlichkeit $1-\beta$ (Teststärke) wenn H0 wahr ist. Dies ist in Fig~\ref{fig: FDR}. Wenn man dann auch noch die Wahrscheinlichkeit dass H0 wahr ist kennt, so ergibt sich für die FDR

\begin{equation}
FDR := \frac{p(H0) \cdot \alpha}{ p(H0) \cdot \alpha + p(!H0) \cdot (1-\beta)}
\end{equation}


\begin{figure}[htb]
\begin{center}
\includegraphics[width = 11cm]{FDR}
\caption{False Discovery Rate}
\label{fig: FDR}
\end{center}
\end{figure}


\subsection{Maximum-Likelihood-Schätzwert}

Das zweite zentrale Verfahren der schließenden Statistik ist die Methode der Maximum Likelihood Schätzer (maximum-likelihood estimate, MLE). Für diese Methode betrachtet man die Wahrscheinlichkeit der beobachteten Daten als Funktion der Modellparameter $\theta$ 

\begin{equation}\ref{eq: likelihood}
L(\theta) := p(dD_{obs} | M(\theta))
\end{equation}

Der Maximum-Likelihood-Schätzer\marginnote{Merke: der Maximum Likelihood Schätzer ist der Parametersatz, für den die Daten am wahrscheinlichsten sind, nicht jedoch der wahrscheinlichste Parametersatz!} ist die Parameterkombination, für welche die Wahrscheinlichkeit in eq.~\ref{eq: likelihood}. 

Der Maximum-Likelihood-Schätzung (MLE) ist eine "Punktschätzer" (engl: point estimate), weil nur ein Parameterwert berechnet wird.\marginnote{Eine Punktschätzung ist vergleichbar mit einer einzelnen besten Schätzung.} Ein einzelner Wert ist aber oft nicht besonders nützlich, wenn man nicht weiß wie unsicher er ist.\marginnote{Das Konfidenzintervall beschreibt die Unsicherheit um die Punktschätzung} Deswegen wird der MLE normalerweise mit einem Konfidenzintervall angegeben. Ein korrekt definiertes 95\% Konfidenzintervall überdeckt bei wiederholten Experimenten den wahren Parameter in 95\% aller Fälle. Salopp gesagt ist das Konfidenzintervall ist der ungefähre Bereich, in dem wir den korrekten Parameter erwarten. 

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}
    
\textbf{Übungsfragen MLE:} 
\textbf{4.1} Wie ist die Likelihood der beobachteten Daten D für ein gegebenes Modell M mit Parameter x definiert?
\textbf{4.2} In einem Experiment wurde der Effekt von Stickstoff auf das Wachstum von Pflanzen getestet - der Likelihood ist maximal für eine Verdoppelung des Wachstums. Die Autoren schreiben: “Die Wahrscheinlichkeit die beobachteten Daten zu erhalten ist maximal wenn man annimmt dass Stickstoff das Wachstum von den beobachteten Pflanzen verdoppelt” - ist diese Aussage Korrekt?
\textbf{4.3} Weiter unten schreiben die Autoren: “Der wahrscheinlichste Wert für den Effekt von Stickstoff ist 2 (verdoppelung)” - ist diese Aussage korrekt?

%\end{mdframed}
\end{fullwidth}

\subsection{Bayes Verfahren}

Unsere\marginnote{Die Bayes'sche Statistik berechnen eine dritte Größe, die Posterior Wahrscheinlichkeit.} Übersicht der schließenden Methoden wäre nicht komplett ohne eine dritte Methode zu erwähnen, die Bayes'sche Statistik. In der Bayes'schen Statistik wird eine dritte Größe berechnet, die Posterior Wahrscheinlichkeit. Diese ist ähnlich, jedoch nicht identisch zu MLE. Die Bayes'sche Statistik wird bei komplizierteren Modellen sehr wichtig, hier wollen wir aber nicht weiter darauf eingehen. Bei Bedarf und Interesse hilf \citet{Gelman-BayesianDataAnalysis-2003} und diese \href{http://florianhartig.github.io/LearningBayes/}{Webseite}.

\subsection{Verschiedene Methoden != verschiedene Modelle}

Wir kennen nun die drei Methoden der schließenden Statistik (Hypothesentests, Maximum-Likelihood, und Bayes), und wissen dass diese drei verschiedene Größen liefern: p-Wert, MLE und die Posterior. 

Ich\marginnote{ANOVA , t-Tests und lineare Regression sind nur unterschiedliche Begutachtungen des selben Modells} betone dass, da ein häufiges Missverständnis darin besteht eine andere Auswertungsmethode mit einem anderen Modell zu verwechseln. Ein Beispiel hierfür sind die nun folgenden Methoden ANOVA, t-Tests und die lineare Regression. Alle basieren auf ein und dem selben datengenerierenden Prozess, nur mit andere Auswertung. ANOVA und t-Tests legen verschiedenste Nullhypothesen fest, und die lineare Regression sucht nach dem MLE. Man könnte für das gleiche Modell noch die Posterior berechnen. 

\section{Wichtige Hypothesentests}

Nachdem jetzt die grundlegenden statistischen Datenausgaben besprochen wurden, gehen wir nun in die Praxis über, um die zwei wohl häufigsten Hypothesentests kennenzulernen - den t-Test und ANOVA. Wie bereits erwähnt, basieren sie beide auf dem selben datengenerierenden Prozess, legen jedoch geringfügig unterschiedliche Nullhypothesen fest.

\subsection{t-Test}

Ein t-Test testet die Unterschiede in den Mittelwerten zweier normalverteilter Stichproben; oder im Falle einer einzigen Stichprobe auf einen Unterschied des Mittelwerts der Stichprobe zu 0. Die Nullhypothese ist, dass es keine Unterschiede zwischen den Mittelwerten der zwei normalverteilten Gruppen gibt, bzw., wenn wir nur eine Gruppe betrachten, dass der Stichprobenmittelwert 0 ist. Desweiteren sind eine Reihe von Anpassungen möglich, z.B. eine Lockerung der Annahme, dass die zwei Gruppen die selbe Varianz aufweisen. Hier folgt nun ein Beispiel in R, mit klassischen Daten von \citet{Student-probableerrormean-1908}. Die Daten zeigen die Wirkung von zwei Schlafmitteln (gesteigerte Anzahl an Schlafstunden verglichen mit der Kontrolle) an 10 Patienten. 



\begin{figure}[htbp]
\begin{center}
<< echo = F>>=
boxplot(extra ~ group, data = sleep, col = "lightgrey", xlab = "treatment", ylab = "Extra hours sleep")
@
\caption{Daten von \citet{Student-probableerrormean-1908}}
\label{fig: Student Sleep Data}
\end{center}
\end{figure}

<<eval=FALSE>>=
## Traditional interface
with(sleep, t.test(sleep$extra[sleep$group == 1], extra[group == 2]))
@

<<>>=
## Formula interface
t.test(extra ~ group, data = sleep)
@

Beachte, dass die Datenausgabe einen p-Wert (H0 = kein Unterschied), aber auch einen Maximum-Likelihood-Schätzwert der Mittelwertsvergleiche, zusammen mit den Konfidenzintervallen bietet. Dies geht weit über den klassischen T-Test hinaus. Vermutlich nahmen die Programmierer an, dass man zusätzlich den besten Schätzwert für die Mittelwertsunterschiede haben möchte.

Vorschlag für das Anzeigen des Ergebnisses: p>0.05: Unterschiede zwischen den Gruppen waren nicht signifikant. p<0.05: Wir erhielten einen Unterschied von X +- Konfidenzintervall zwischen den Gruppen (p-Wert für die Unterschiede eines T-Tests mit X). 

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}
    
\textbf{Übungsfragen Hypothesentests:} 
\textbf{4.4} Definieren Sie den p-Wert.
\textbf{4.5} Wie ist in der vorherigen Definition “extremer” (also >=) definiert?
\textbf{4.6} Stellen Sie sich vor Sie leben in Venedig, und es gibt 3 Taxibootfirmen. Sie wollen wissen ob es Unterschiede in der Beförderungsgeschwindigkeit gibt und machen deshalb 300 Fahrten mit jeder Firma und stoppen die Zeit. Was wäre eine geignete Nullhypothese H0, um auf einen Unterschied zu testen?
\textbf{4.7} Bonusfrage: Nennen Sie eine der vielen möglichen sinnvollen Teststatistiken
\textbf{4.8} In der Physik gibt es das sogenannte Standardmodell, dass die Eigenschaften und Interaktionen der Material beschreibt. In den letzten 20 Jahren wurde das Standardmodell immer und immer wieder getestet, so “erfolgreich” dass die Physiker schon ein bisschen deprimiert sind weil Sie nichts neues entdecken. Was ist die Nullhypothese die bei diesen Tests angewandt wird?
\textbf{4.9} Sie lesen einen Artikel über ein medizinisches Experiment. Getestet wurde ein Medikament gegen eine Kontrolle, und der p-Wert ist 0.03. Die Studie schreibt: “Die Wahrscheinlichkeit dass das Medikament nicht wirkt ist 3\%” - stimmen Sie zu?
\textbf{4.10} Schreiben Sie eine korrekte Interpretation des obigen Ergebnis auf.
\textbf{4.11} Definierent Sie den Typ I Fehler (falschen positive).
\textbf{4.12} Wie viel Typ I Fehler erwarten Sie bei einen Signifikanzlevel von 7\%?
\textbf{4.13} Definieren Sie den Typ II Fehler (falsche negative).
\textbf{4.14} Wie viel Typ II Fehler erwarten Sie bei einen Signifikanzlevel von 7\%?
\textbf{4.15} Nennen Sie 2 Faktoren die Typ II Fehler beeinflussen, und die Richtung des Einflusses (negativ = Typ II nimmt ab wenn Faktor hoch geht).
\textbf{4.16} Definieren Sie die Teststärke / Power.
\textbf{4.17} Sie testen 100 Gene auf eine Assoziation mit Krebs. Bei 5 Genen zeigt der Test Signifikanz an. Wie bewerten Sie dieses Ergebnis?
\textbf{4.18} Sie bekommen von einer allwissenden Macht die Zusatzinformation dass Sie in dem oben genannten Test eine Teststärke von 99\% hatte. Sind Sie nun zuversichtlicher dass Sie einen Effekt gefunden haben?
\textbf{4.19} Definieren Sie die False Discovery Rate (FDR).
\textbf{4.20} Wovon hängt die FDR ab?
\textbf{4.21}  In einer Reihe von Experimenten testen Sie Medikamente auf eine Wirkung. Ihre Power ist 100\%. Sie schätzen dass jedes 20. Medikament das Sie testen eine Wirkung haben solte. Wie ist ihre FDR bei einem Signifikanzlevel von 5\%? Es reicht wenn Sie den Rechenweg aufschreiben, Sie müssen den Wert nicht ausrechnen.

%\end{mdframed}
\end{fullwidth}


\subsection{Varianzanalyse (ANOVA)}

ANOVA (analysis of variance) oder Varianzanalyse kann von verschiedenen Personen unterschiedlich verstanden werden. Die Standard-ANOVA macht grundsätzlich die gleichen Annahmen wie ein T-Test (normalverteilte Resonanz), jedoch für mehr als zwei Gruppen. Genauer gesagt testet es, ob die gemessene abhängige Variable (z.B. eine abhängige Variable) von einem oder mehreren kategorischen Variablen mit zwei oder mehreren interagierenden Ebenen beeinflusst wird. Eine Wechselwirkung\marginnote{Eine Wechselwirkung: eine Variable verändert die Auswirkung einer anderen Variablen} zwischen zweier Variablen bedeutet, dass sich der Wert einer erläuternden Variable darauf auswirkt, wie sehr eine andere erläuternde Variable die abhängige Variable beeinträchtigt.

Während der Begriff ANOVA meist mit der oben genannten Erklärung in Verbindung gebracht wird (welche der von T-Tests / linearer Regression entspricht, siehe nächstes Kapitel), kann die Auffassung der ANOVA in der gleichen Weise erweitert werden, wie ein lineares Regressionsmodell zu einem allgemeinen linearen Modell (engl. Generalized Linear Models, glm) erweitert werden kann etc. Dies erlaubt uns also, ANOVAs auf Modelle mit nicht-normalverteilten Fehlern anzuwenden (natürlich muss dies der Software gesagt werden, es geht nicht automatisch davon aus). Deswegen muss man besonders Acht darauf geben, von was andere ausgehen, wenn sie diesen Term benutzen.

Hier ist ein einfaches Beispiel mit einer Standard-ANOVA (normalverteilte Fehler), in der getestet werden soll, ob Gewicht (von Hühnern) von ihrer Ernährung abhängt, wobei 'Ernährung' ein variabler Faktor mit vier Stufen ist:

<<>>=
aovresult <- aov(weight~Diet, ChickWeight)
summary(aovresult)
@

Wir erhalten einen p-Wert von 6.43e-07, welcher mit einem $\alpha$ Level von 0.05 höchst signifikant ist. Somit können wir die Nullhypothese abweisen, dass die Ernährung keinen Einfluss auf die abhängige Variable "`Gewicht"' habe. Beachte hier, dass wir keine Parameterschätzwerte erhalten und wir keine Aussagen darüber machen können, welche Ernährung sich von welcher unterscheidet. Hierfür gibt es zwei Möglichkeiten:

\begin{itemize}
\item Entweder wendet man den sogenannten Post-Hoc-Test an, welcher auf Unterschiede der Ernährungsweisen testet (Bsp. mit einem T-Test).
\item Oder man wechselt zu einer Regression, die im folgenden Kapitel genauer beschrieben wird.
\end{itemize}

Mit einem Post-Hoc-Test wendet man multiple Tests auf die gleichen Daten an. Dies kann zum Problem werden - der Grundgedanke des p-Werts ist eine Wahrscheinlichkeitkalkulation für das Betrachtet der Daten bezüglich EINER Nullhypothese. Nach einer solchen Durchführung erhält mal allenfalls einen  5\% Fehler bei einem $\alpha$ Level von 0.05. \marginnote{Beim Anwenden multipler Tests auf die selben Daten benötigt man eine Korrektur des p-Werts für multiples Testen.} Wenn wir jedoch multiple Tests durchführen, testen wir auch multiple Nullhypothesen und es ergeben sich mehr Möglichkeiten für die Testgrößen ein Signifikanzlevel nur durch Zufall zu erreichen. Deswegen müssen wir den p-Wert für multiples Testen korrigieren. Um hierzu mehr Informationen zu erhalten, ist Google dein bester Freund. 

\subsection{Weitere wichtige Tests}

T-Tests und ANOVA sind sehr häufig verwendete Tests, wobei es noch viele weitere gibt. Eine Liste mit Tests kann man beispielsweise im Wikipedia-Artikel unter diesem \href{http://en.wikipedia.org/wiki/Category:Statistical_tests}{Link} finden.


\section{Regression}

Wie schon zuvor beschrieben, bedeutet Regression nicht zwingend, dass man ein anderes statistisches Modell wie bei Hypothesentests verwendet (ANOVA und das lineare Regressionsmodell verwenden in R die gleichen Annahmen). Nichtsdestotrotz ist das Ziel einer Regression ein anderes. Während Hypothesentests nur überprüfen, ob die Daten mit einer Nullhypothese vereinbar sind, sucht eine Regression nach einer am besten passenden Hypothese oder Parametern (Maximum-Likelihood-Schätzwert). Somit sucht ein Regressionsmodell nach einer Parameterkombination, die mit der höchsten Wahrscheinlichkeit die beobachteten Daten, mit vorgegebenen Modellannahmen, ausgibt.

\subsection{Lineare Regression}

Das grundlegendste Regressionsmodell ist die lineare Regression. Hier lautet die Annahme, dass wir eine abhängige Variable haben, die von einem Prädiktor wie folgt abhängt:

\begin{equation} \label{eq: linear regression}
y \sim a \cdot x + b + \epsilon 
\end{equation}

wobei y die abhängige Variable, x der Prädiktor ist; a ist der Parameter, wie sehr der Prädiktor die abhängige Variable beeinflusst, b ist der Schnittpunkt und $\epsilon$ ist die Zufallsvariation, welche in einer linearen Regression als normalverteilt angenommen wird.

In R wird solch eine Regression durch folgenden Befehl erreicht

<<>>=
fit = lm(airquality$Temp~airquality$Ozone)
summary(fit)
@


\begin{figure}[htbp]
\begin{center}
<< echo = F>>=
plot(airquality$Temp ~ airquality$Ozone, ylab = "Temperature", xlab = "Ozone")
abline(fit)
@
\caption{Luftqualitäts Datensatz: Temperatur aufgetragen gegen Ozon. Zusammenhang durch die Regressionsgerade angegeben.}
\label{fig: LR}
\end{center}
\end{figure}

Dieser Code kann unabhängig davon verwendet werden, ob der Prädiktor stetig oder kategorisch ist. Im Falle einer stetigen Variable entsteht eine Linie anhand der Daten. Im Falle einer kategorischen Variable mit n Stufen ist die erste Stufe als Referenz gesetzt (Schnittpunkt), und n-1 Faktoren entsprechen den folgenden Stufen, die den Unterschied zur Referenz beschreiben.

Die entsprechenden Parameter erscheinen in der Spalte "`Estimate"'. Dies zeigt uns, wie sehr der Prädiktor, in diesem Fall Ozon, die abhängige Variable, hier Temperatur, beeinflusst: Für jede Einheit Ozon mehr steigt die Temperatur um 0.201 Einheiten, mit einem Standardfehler (Konfidenzintervall) von 0.019. Abgesehen davon, wie ein Regressions-Output aussieht, lehrt uns dies etwas anderes Wichtiges: Die Tatsache, dass wir die Temperatur als abhängige Variable und Ozon als Prädiktor benutzt haben, bedeutet nicht, dass Ozon ursächlich die Temperatur beeinflusst. \marginnote{Korrelation ist nicht Kausalität.}Tatsächlich ist es genau anders herum: wenn wir mehr Sonne haben, wird es wärmer und wir haben tendenziell mehr Ozon. Regression schafft nicht, wie die meisten anderen statistischen Analysen, Kausalität. Es schafft Korrelation. Was wir hier ausdrücken ist, dass wenn unsere Ozonmessungen steigen, wir ziemlich sicher davon ausgehen können, dass es auch gleichzeitig wärmer wird. Was wiederum nicht bedeutet, dass Ozon Hitze erschafft. Korrelation ist nicht Kausalität.

Die Regressionsergebnisse geben uns ebenfalls einige p-Werte aus. Das sind die Ergebnisse von mehreren Hypothesentests, die automatisch nach der Regression geschaltet sind. Beispielsweise erhält man einen p-Wert für jeden Parameter. Dieser p-Wert basiert auf einem bestimmten Typ von T-Test, wobei das vollständige Modell gegen ein Modell mit auf 0 gesetzten Parametern getestet wird. Zusätzlich gibt es noch einen weiteren p-Wert, welcher auf einer anderen Testgröße am Ende der Regressionsausgabe beruht. Dieser testet die Nullhypothese mit allen Parametern gleich 0.

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{Präzisieren von verschiedenen Modellannahmen in R:} 

abhängige Variable y hängt linear von Variable a (stetig oder kategorisch) ab

<<eval = FALSE>>=
fit = lm(y~a)
summary(fit)
@

abhängige Variable y hängt linear von zwei Variablen a und b (stetig oder kategorisch) ab, aber der Wert beider Variablen beeinflusst nicht den Effekt, welcher die andere Variable auf die abhängige Variable hat (keine Interaktion)

<<eval = FALSE>>=
fit = lm(y~a+b)
summary(fit)
@

abhängige Variable y hängt linear von zwei Variablen a und b (stetig oder kategorisch) ab, aber der Wert der einen Variable beeinflusst nicht den Effekt der anderen Variable auf die abhängige Variable (Interaktion)

<<eval = FALSE>>=
fit = lm(y~a*b)
summary(fit)
@

abhängige Variable y ist von einer Variablen a (stetig oder kategorisch) wie in $a + a^2$ abhängig

<<eval = FALSE>>=
fit = lm(y~a + I(a^2))
summary(fit)
@

die Schreibweise I() kennzeichnet eine darauf folgende mathematische Formel. 

\end{mdframed}
\end{fullwidth}



\subsection{Überprüfen der Annahmen}

Formal gesehen kann man jegliche Datensätze auf lineare Modelle übertragen. Jedoch sind hier wie bei jeder statistischen Folgerung die Ergebnisse (z.B. Parameterschätzungen, p-Werte) von den getroffenen Annahmen abhängig. Dementsprechend ist der p-Wert, den wir erhalten, von der Annahme abhängig, dass die Daten auch tatsächlich mit der Formel ~\ref{eq: linear regression} übereinstimmen. Wenn dies nicht der Fall ist, könnte der p-Wert völlig falsch sein. Somit muss man prüfen, ob diese Annahmen auch tatsächlich erfüllt wurden.

Also was genau waren die Annahmen einer linearen Regression? Ein Problem ist, dass sich Studenten an die Annahme der Normalverteilung erinnern. Deswegen überprüfen sie, ob die abhängige Variable-Variable normalverteilt ist. Wenn man jedoch die Formel ~\ref{eq: linear regression} genauer betrachtet, erkennt man, dass dies nicht entscheidend ist. Wenn man die Terme in der Formel ~\ref{eq: linear regression} verschiebt,
sehen wir das, was eigentlich normalverteilt sein soll

\begin{equation} \label{eq: linear regression}
y - (a \cdot x + b ) \sim \epsilon 
\end{equation}

nämlich der Unterschied zwischen beobachtetem Wert und den Modellvorhersagen. Diese Unterschiede werden Residuen genannt und sollten, entsprechend unserer Modellannahmen, normalverteilt sein. Um zu überprüfen, ob dies wirklich der Fall ist, sollten einige Tests durchgeführt werden. Der einfachste Test ist das Erstellen eines Plots mit den Residuen gegen die angepassten Werte UND gegen alle Prädiktoren im Modell. Der daraus entstehende Plot erlaubt es dann viele mögliche Probleme zu erkennen (Fig.~\ref{fig: ResidualPatterns})



\begin{figure}[htbp]
\begin{center}
<< echo = F>>=
oldpar <- par(mfrow = c(2,2))

plot(rnorm(1000), ylim = c(-4,4), main = "Perfekt Normal", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)

plot(rnorm(1000, sd = seq(0.01,2, len = 1000)), ylim = c(-4,4), main = "Heteroscedasticity", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)


plot(rnorm(1000, mean = seq(-2,2, len = 1000)), ylim = c(-4,4), main = "Pattern in the residuals", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)

plot(ifelse(runif(1000) <0.5, rnorm(1000, mean = 2, sd = 0.3), rnorm(1000, mean = -2, sd = 0.3)) , ylim = c(-4,4), main = "Distribution not normal", ylab = "Residual", xlab = "Fitted value or predictor")
abline(h=0, col = "red", lwd = 3)

par(oldpar)
@
\caption{Eine Sammlung möglicher Muster, wenn die Residuen gegen den angepassten Wert (voreingestellt in R) oder einen Prädiktor geplottet werden.}
\label{fig: ResidualPatterns}
\end{center}
\end{figure}

übliche Probleme und ihre Lösungen sind  

\begin{itemize}
  \item Heteroskedastizität (Varianzveränderungen) --> Verändere die abhängige Variable, oder verwende ein Regressionsmodell, das Heteroskedastizität berücksichtigen kann
  \item Muster in den Residuen -> Falsche Funktionsform des Regressionsmodells. Versuche Prädiktoren, quadratische Effekte, Interaktionen oder andere Dinge hinzuzufügen, die die funktionale Form ändern
  \item Verteilung nicht normal -> Angenommen, es liegt nicht an einem der früheren Probleme (kein Muster / keine Heteroskedastizität), kann man noch eine Variablentransformation ausprobieren oder eine Regression mit einer anderen Verteilungsannahme durchführen (siehe nächsten Abschnitt)
\end{itemize}  

Es gibt weitere spezialisierte Plots, um mit der Diagnose dieser Probleme zu helfen. Diese erfolgen bei der Anwendung des Befehls plot()


Sie erhalten grundlegende Residuendiagnosen durch Eingabe von plot(fit), wobei fit das angepasste Modell ist. Weitere Details zur Residuendiagnose siehe \href{http://www.statmethods.net/stats/rdiagnostics.html}{hier}.



\section{Allgemeine lineare Regressionsmodelle (GLM)}

Die allgemeine Vorstellung einer linearen Regression war, dass 1) die abhängige Variable stetig ist, theoretisch von - unendlich bis + unendlich, und 2) Residuen normal um die Modellvorhersagen verteilt sind. Der Grundgedanke der allgemeinen linearen Regressionsmodell-Rahmenbedingung ist, wie zuvor in dem linearen Regressionsbeispiel zu arbeiten, jedoch beide Annahmen über abhängige Variablegrößen von - bis + unendlich und die Normalität zu lockern. Dazu müssen wir zwei Dinge tun

\begin{itemize}
  \item Um die Ausgabewerte in dem Bereich zu erhalten, den wir wollen, wickeln wir das lineare Modell in eine Transformationsfunktion, die die abhängige Variable ins rechte Intervall zwingt (typische Intervalle sind positiv oder zwischen 0 und 1). Diese Transformation wird als Link-Funktion bezeichnet
  \item Um andere Verteilungen anzupassen, müssen wir dem Modell sagen, dass es etwas anderes als die Gaußsche Fehlerfunktion verwenden soll.
\end{itemize}    
   
Wir werden über diese Punkte nun etwas mehr ins Detail gehen.

\subsection{Die Link-Funktion}

Wir haben gesagt, dass eine lineare Regression folgende Form annimmt

\begin{equation}
y \sim a \cdot x + b 
\end{equation}

Das heißt, wenn x groß wird, könnte y jeden Wert einnehmen, positiv oder negativ. Ein Trick, um sicherzustellen, dass alle Vorhersagen für y positiv sind oder innerhalb eines bestimmten Bereichs liegen, ist eine Link-Funktion der Form

\begin{equation}
y \sim f^{link}(a \cdot x + b )
\end{equation}

Jede Funktion ist möglich, aber wie wir später sehen werden, sind typische Alternativen die Exponentialfunktion, die positive Ergebnisse garantiert, und die inverse Logit, die Bereich zwischen 0 und 1 garantiert.

\subsection{Andere Verteilungen}

Nun, das ist konzeptionell der einfache Teil, aber vielleicht ist noch nicht klar, welche Art von Distributionen neben der normalen vorhanden sind. Zwei typische Entscheidungen, die wir unten verwenden, sind die Binomial- (die Verteilung für das Münzwerfen) und die Poisson-Verteilung (eine diskrete Wahrscheinlichkeitsverteilung). Es stehen viele andere Möglichkeiten zur Verfügung. Vielleicht wird es deutlicher, wenn wir uns in den nächsten Abschnitten zu den konkreten Beispielen bewegen.

\subsection{0/1 Daten - logistische Regression}

Logistische Regression ist die häufigste Analyse für binäre Daten (Präsenz / Abwesenheit, überlebt / tot, infiziert / nicht infiziert). Logistische Regression geht davon aus, dass die Verteilung binomial ist (Münzwurf-Modell). Um den linearen Prädiktor auf einer Skala zwischen 0 und 1 zu erhalten, die für die Binomialverteilung erforderlich ist, verwenden wir die logistische Linkfunktion (oder inverse Logit).

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Hier ein Beispiel mit den Daten der Titanik-überlebenden. Beachte, dass die Logit-Link automatisch ausgewählt wird, wenn in R die Binomialverteilung verwendet wird. Bei Bedarf könnte man diese Wahl überschreiben.

<<>>=
library(effects)
fmt <- glm(survived ~ age + I(age^2) + I(age^3), family=binomial, data = TitanicSurvival)
summary(fmt)
@


\end{mdframed}
\end{fullwidth} 




\subsection{Zähldaten - Poisson-Regression}

Die Poisson-Regression ist die Standard-Wahl für die Arbeit mit Zähldaten, obwohl ein paar andere Optionen zur Verfügung stehen. In der Poisson-Regression wird standardisiert eine Exponentialfunktion gewählt, um alle Werte positiv zu machen. Das Inverse des Exponentials ist der Log, also nennen wir das den Log-Link. Nach wie vor wählt R diese automatisch aus, wenn man angibt die Verteilung zu "`poissonieren"'.

\vspace{1cm}
\begin{fullwidth}
\begin{mdframed}
    
\textbf{In R:} 

Ein Beispiel hierfür zeigt einige Daten über die Fütterung von Nesthockern in Bezug auf ihre Attraktivität:


<<>>=
schnaepper <- read.csv("schnaepper.txt", sep="")
fm <- glm(stuecke ~ attrakt, family=poisson, data = schnaepper)
summary(fm)
@

\end{mdframed}
\end{fullwidth} 


\subsection{Residuen-überprüfungen in allgemeinen linearen Regressionsmodellen}

Residuen in allgemeinen linearen Regressionsmodellen sollten nicht normalverteilt sein, deswegen verwendet man keine Standard-Kontrollen für Normalität wie normale Quantile-Quantile-Plots, um auf die Angemessenheit der Residuen zu überprüfen. Für nicht zu komplizierte Modelle gibt es eine Möglichkeit dieses Problem zu umgehen, indem man die sogenannten Pearsons-Residuen verwendet, die die beobachteten Unterschiede zwischen Modell und Daten durch die erwartete Varianz des Modells normiert \footnote{In R kann man die Option Pearson in vielen Funktionen angeben, einschließlich der Residual()-Funktion, die man auf ein angepasstes Objekt anwenden kann}

Ein Standardproblem in Poisson oder binomialen allgemeinen linearen Regressionsmodellen ist, dass die Varianz der Poisson- und Binomialverteilung nicht eingestellt werden kann, sondern durch den Mittelwert festgelegt wird. Dies ist ein Problem, das im normalen linearen Modell nicht auftritt, da hier der zufällige Teil durch eine Normalverteilung modelliert wird, die einen Parameter für die Varianz aufweist. Ein Problem, das sehr häufig in Poisson oder binomialen allgemeinen linearen Regressionsmodellen auftritt, ist eine überdispersion, d.h. dass die Residuen mehr Varianz, als unter dem angepassten Modell erwartet, zeigen. \marginnote{Man kann auf überdispersion prüfen, indem man die angepasste Abweichung betrachtet oder einen überdispersionstest anwendet} Der einfachste Weg, um dies zu korrigieren, ist die Verwendung der Quasi-Poisson- und Quasibinomialmodelle, die in der allgemeinen linearen Regressionsmodell-Funktion verfügbar sind. Diese Modelle bringen einen zusätzlichen Parameter ein, der die Varianz des Poisson- und des binomialen allgemeinen linearen Regressionsmodells abwandelt.

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}
    
\textbf{Übungsfragen Regression:} 
\textbf{4.22} Was sind die Annahmen der linearen Regression?
\textbf{4.23} Wie werden die Parameter in der linearen Regression bestimmt?
\textbf{4.24} Welche H0 steckt hinter den p-Werten der Parameter der linearen Regression?

%\end{mdframed}
\end{fullwidth}


\chapter{Prädiktive Statistik und Maschinelles Lernen}\label{ch: Prädiktive Statistik}

Prädiktive Statistik (auch: maschinelles Lernen) ist ein dritter Bereich der Statistik der in den letzten Jahren zunehmend an Bedeutung gewonnen hat. Das grundlegende Ziel dieser Methoden besteht darin, gute Vorhersagen aus einem komplexen Datensatz zu erzeugen. Dabei verwenden sie typischerweise relativ komplizierte, nicht-parametrische Verfahren, die typischerweise Vorhersagen ermöglichen, aber keine Berechnung klassischer Größen der schließenden Statistik, wie z.B. der p-Wert oder MLE. 

Die vielleicht bekannteste Methode des maschinellen Lernens ist der sogenannte Random Forest Algorithmus. Genau zu erklären wie dieses Algorithmus funktioniert würde hier zu weit führen, aber um eine grobe Idee zu geben: die Idee ist die Daten durch sogenannten Entscheidungsbäume zu beschreiben. Die Entscheidungsbäume sagen: Daten mit Eigenschaft A führen zu Ergebnis X, und Eigenschaft B führt zu Y. Eine Strategie für Vorhersagen wäre jetzt einen sehr detaillierten Entscheidungsbaum zu bauen. Genau das mach der Random Forest nicht. Statt dessen baut er viele einfache (= schlechte) Entscheidungsbäume und kombiniert diese dann. Erstaunlicherweise führt die Kombination von vielen schlechten Modellen oft zu einer besseren Vorhersage als die eines guten Modells. Hier ein Beispiel des Algorithmus, in dem wir die Menge an Ozon in der Luft aus verschiedenen anderen Wetterparametern voraussagen 

Mit der klassischen linearen Regression können wir ungefähr 60\% der Varianz in Ozon erklären

<<>>=
ozone.lm <- lm(Ozone ~ ., data=airquality, na.action=na.omit)
summary(ozone.lm)
@

Random Forest schafft es auf 70\%. Wir haben hier keine Vorhersagen auf neue Daten getestet, aber typischerweise fällt der Unterschied zwischen lm und rf in diesem Fall noch deutlicher aus. 

<<>>=
library(randomForest)
ozone.rf <- randomForest(Ozone ~ ., data=airquality, mtry=3,
                         importance=TRUE, na.action=na.omit)
print(ozone.rf)
@

Der Random Forest liefert keine p-Werte o.ä., aber man kann sich wenigsten die Wichtigkeit der verschiedenen Variablen anzeigen lassen, wobei sich hier Wichtigkeit auf die Wichtigkeit für die Vorhersage bezieht, eine kausale Annahme wird hier nicht gemacht. 

<<>>=
varImpPlot(ozone.rf)
@

Das Verhältnis zwischen den Anhängern der klassischen schließenden Statistik und den maschinellen Lernen ist seit langem von Spannungen geprägt. Klassische Statistiker bemängeln dass das maschinelle Lernen die Idee des "`Lernens von Daten"' im Sinne des Vergleichens von Hypothesen und Daten zugunsten einfacher Prognosen aufgegeben hat. Ein Anhänger des maschinellen Lernens würde darauf antworten, dass es bei vielen angewandten Problemen nicht um das Testen von kausalen Zusammenhängen, sondern nur um das Erkennen von Pattern geht. Das Ziel ist es, einen Algorithmus zu erstellen, der in der Lage ist bei einem komplexen Datensatz korrekt vorherzusagen. \footnote{Typische maschinelle Lernanwendungen beinhalten die Vorhersage der Interessen der Kunden in Web-Shops, die Korrelation komplexer Satellitendaten mit Bodensignalen oder Sprach/Gesichtserkennung.} Der Erfolg ist auf der Seite des maschinellen Lernens: Experten in diesem Feld sind derzeit bei Technologieunternehmen wie Google, Facebook, etc. heiß begehrt, und der Markt scheint immer weiter zu wachsen. Der Unterschied zwischen der schließenden und der prädiktiven Statistik sowie die Spannung zwischen diesen Bereichen ist Inhalt des äußerst empfehlenswerten Artikels "`Statistical Modeling: The Two Cultures"' von \citet{Breiman-StatisticalModelingTwo-2001}, dem Erfinder des Random Forest Algorithms. Hier ein Auszug aus diesem Text:

\begin{quote}
There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.
\end{quote}

Eine abschließende Bemerkung: Aufgrund der großen und wachsenden Bedeutung der prädiktiven Methoden in der modernen Statistik habe ich dieses kurze Kapitel in diesen Einführungstext aufgenommen. Natürlich gibt es wesentlich mehr zu diesem Thema zu sagen als der Platz hier erlaubt. Um mehr über prädiktive Methoden zu lernen würde ich Ihnen die sehr lesbaren Artikel \citet{Breiman-StatisticalModelingTwo-2001, Shmueli-Toexplainor-2010} sehr ans Herz legen, außerdem das Lehrbuch von \citet{James-IntroductiontoStatistical-2013}, das ich auch am Ende dieses für die weitere Lektüre empfehle. Es ist auch instruktiv zu sehen wie viel Interesse diesen Methoden im Internet entgegengebracht wird, und dass viele Firmen offene Wettbewerbe in prädikativer Statistik ausschreiben um Mitarbeiter zu gewinnen (siehe, z.B. \href{kaggle.com}{https://www.kaggle.com/}). 


\chapter{Versuchsplanung}\label{ch: design of experiments}

Kommen wir zurück zum Anfangspunkt dieses Skripts: den Daten. Wenn wir selbst Daten erheben, müssen wir eine Reihe von Fragen beantworten: Welche Variablen sollten gemessen werden? Wie variieren wir Variablen, über die wir Kontrolle haben. Und wie viele Replikate brauchen wir?


\section{Auswahl der Variablen}

In der Praxis interessieren wir uns typischerweise dafür, wie eine abhängige Variable von einer Anzahl von Prädiktoren beeinflusst wird. Wenn uns diese Beziehung interessiert, müssen wir logischerweise sowohl die abhängige Variable als auch die Prädiktoren.\marginnote{Korrelation ist keine Kausalität.} Eine minimale Anforderung ist dass wir verschiedene Werte der Prädiktoren haben sollten (wenn sich die Prädiktoren nicht ändern können wir auch keine Abhängigkeit beweisen. Normalerweise interessiert uns aber nicht nur Korrelation, sondern wir wollen mit einiger Sicherheit sagen können ob diese Korrelation kausal kausal ist. Um das zu tun, müssen wir alternative Erklärungen ausschließen. Variablen die als alternative Erklärung für einen Korrelation in Betracht kommen werden als Störfaktoren bezeichnet werden.

\subsection{Was ist ein Störfaktor?}

Stellen wir uns vor wir wollen wissen ob A von B abhängt, also $A~B$. Was passiert, wenn es einen zweiten Prädiktor C gibt, der auch A beeinflusst, uns aber eigentlich nicht interessiert? In Englischen nennt man C "extraneous variable", im Deutschen gibt es dafür leider keine gängige Übersetzung. \marginnote{Eine "extraneous variable" ist eine Variable, die die abhängige Variable beeinflussen kann, aber für den Experimentator nicht von Interesse ist.}. Also, es gibt auch $A~C$, aber diese Beziehung interessiert uns nicht. Die Korrelation $A~C$ ist es normalerweise kein Problem, solange C mit B unkorreliert ist. Wenn man C misachtet könnte es sein dass die unerklärte Varianz des Modells etwas höher ist, dadurch p-Werte etwas beinflusst werden können, aber im Großen und Ganzen sollten hierdurch keine großen Problem auftreten.

\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{Confounding}
\caption{Visualisierung eines Störfaktors. Eine wichtige Voraussetzung für einen Störfaktor ist, dass die Variable sowohl mit der abhängige Variable korreliert als auch mit den Prädiktorvariablen, die unsere ursprüngliche Hypothese bilden. Wenn die zweite Verbindung nicht vorhanden ist, ist die Variable nicht störend und kann ignoriert werden.}
\label{fig: Confounding}
\end{center}
\end{figure}

Die Variable C wird zu einem Problem wenn Sie auch mit B korreliert. Dann nennt man C einen Störfaktor.\marginnote{Eine Störfaktor ist eine externe Variable, die sowohl mit der abhängigen Variable, als auch mit einer primär interessanten Prädiktorvariable korreliert.} Wenn wir in diesem Fall nur $A~B$ messen, erscheint in der Korrelation $A~B$ auch die Auswirkungen von B und C auf A. In diesem Fall kann es passieren, dass eine Korrelation zwischen $A~B$ erscheint, obwohl A gar nicht mit B, sondern nur mit C kausal zusammenhängt.\marginnote{Eine Scheinkorrelation ist eine Korrelation, die durch einen Störfaktor verursacht wird.} Dieses Phänomen wird als Scheinkorrelation bezeichnet. Wenn der geschätze Effekt von $A~B$ falsch ist weil ein Störfaktor in in der Analyse bedacht wurde, so wird dies als omitted variable bias (OVB) bezeichnet. Eine extreme Version davon ist Simpson's Paradoxon. Simpson's Paradoxon beschreibt das Phänomen, dass ein Störfaktor die Richtung der Korrelation $A~B$ (Vorzeichen) umdrehen kann, so dass der Anschein besteht als ob der Einfluss in B umgekehrt ist als der wahre Einfluss. 


\subsection{Was macht man mit Störfaktoren?}

Im experimentellen Design gibt es drei Optionen, um den Einfluss von Störfaktoren zu minimieren:

\begin{enumerate}
\item Kontrolle: Entweder man hält den Faktor fest, oder man variiert ihn in kontrollierter Weise (siehe unten).
\item Randomisierung: Randomisierung heißt dass der experimentelle Aufbau eine Korrelation zwischen den gewünschten erklärenden Variablen und möglichen Störfaktoren unterbindet, e.g. durch geeignete Durchmischung. 
\item Messen: Wenn Störfaktoren gemessen sind kann man sie später durch die Statistik rausrechnen. Ein Problem hier ist dass hierdurch oft die Teststärke (Power) deutlich abnimmt
\end{enumerate}

Natürlich können die drei Techniken auch kombiniert werden. 


\section{Definition und Messfehler von Variablen}

Nach der Entscheidung WAS gemessen werden soll geht es jetzt um das WIE. Ein häufiger Fehler bei diesem Schritt des experimentellen Desigs ist es die Messungen für selbstverständlich zu erachten, und sich jetzt direkt mit der Anzahl an Replikaten usw. auseinanderzusetzen. Erst sollten wir uns aber fragen:


\begin{enumerate}  \marginnote{Die Betrachtung dieser beiden Fragen wird oft als Konstruktvalidität bezeichnet.}
  \item Misst mein Versuchsaufbau, was ich messen möchte
  \item Was ist der erwartete statistische (stochastische) Fehler in meinen Messungen und was ist der mögliche systematische Fehler?
\end{enumerate}

\begin{figure}[]
\begin{center}
\includegraphics[width = 10cm]{RandomizedBlockDesign}
\caption{Illustration eines randomisierten Blockdesigns, der wohl am häufigsten verwendeten Anordnung bei (Beobachtungs)-Experimente zur Randomisierung der Auswirkung unbekannter und nicht gemessener verwandter Variablen. Die Idee dieses Entwurfs ist, dass die unbekannten Variablen wahrscheinlich im Raum korrelieren. Indem wir alle experimentell verändernden Variablen zu einem Block zusammenfassen, vermeiden wir, dass sie von den unbekannten räumlichen Variablen gestört werden können.}
\label{fig: RandomizedBlockDesign}
\end{center}
\end{figure}


Der erste Punkt mag auf den ersten Blick ein wenig seltsam erscheinen. Doch in vielen Fällen der ökologischen Statistik und darüber hinaus messen wir nicht direkt die Variable an der wir interessiert sind, sondern einen Stellvertreter (Proxy). Vielleicht wollen wir die Temperatur an einer bestimmten Stelle wissen, verwenden hierzu aber die Temperatur von einer Wetterstation in 5 km Entfernung. Oder wir wollen die funktionale Vielfalt untersuchen, aber die können wir nicht direkt messe - Zugriff haben wir auf Artendiversität - ist das ein guter Proxy?

Die zweite Frage bezieht sich darauf, wie sehr sich zwei Messungen voneinander unterscheiden würden, wenn wir sie wiederholt durchführen (stochastischer Fehler) und wie viele Messungen systematisch ausfallen könnten (systematischer Fehler, oder auch Bias), z.B. weil eine Methode oder ein Instrument systematisch falsch ist oder weil der Mensch bestimmte Einflüsse zeigt.


\section{Auswahl der Werte für die unabhängigen (Prädiktor-) Variablen}

Inzwischen wisen wir welche Prädiktoren wir messen wollen, und wie. Normalerweise haben wir Einfluss auf die Auswahl der Werte der Prädiktoren (z.B. weil wir Manipulationen an dem System vornehmen). Wir müssen uns also jetzt überlegen wie wir die Werte variieren. 

In\marginnote{Die experimentelle Einheit ist die Einheit, die einer bestimmten Variablenkombination (z.B. Behandlung oder Kontrolle) zugewiesen werden kann. Beispiel: eine einzelne Pflanze oder ein Topf.} einer experimentellen Studie haben wir in der Regel eine Versuchseinheit (experimental unit), z.B. eine Pflanze, ein Topf oder eine Versuchsfläche, und diese Einheit wird dann manipuliert. Auch Beobachtungsstudien haben Versuchseinheiten (das was man beobachtet), aber es ist meist nicht möglich, die Prädiktoren vollständig zu kontrollieren. Allerdings hat man in der Regel die Möglichkeit bestimmte Selektionen vorzunehmen. Dies ist wichtig, denn in jedem Fall benötigt man eine ausreichende Variation der Prädiktorvariablen um eine aussagekräftige statistische Analyse zu gewährleisten.

Hier sind ein paar Punkte, die bei der Planung zu beachten sind

\subsection{Kollinearität der Prädiktoren}

In Beobachtungsstudien kommt es oft vor dass 2 Prädiktoren korrelieren (Kollinearität). Wenn man an einer der beiden nicht interessiert ist, ist diese ein Störfaktor. Aber selbst wenn man an beiden interessiert ist, so ist die Statistik mit Kollinearität normalerweise problematischer zu rechnen. Bei der Versuchsplanung sollte man alles tun um die Kollinearität möglichst klein zu halten. 

\subsection{Interaktionen}

Um Interaktionen zwischen Variablen erkennen zu können, genügt es nicht, jede Variable unabhängig zu verändern, sondern es bedarf bestimmter Kombinationen. Das Schlagwort hier ist (fractional) faktorielle Designs. Google wird hier weiterhelfen.

\subsection{Nichtlineare Effekte}

Die Verbindung von zwei Punkten ist eine Linie. Wenn man herausfinden möchte, ob die abhängige Variable auf eine Variable nichtlinear ist, benötigt man daher mehr als zwei Werte für jede Variable (Faustregel >4). 


\section{Wie viele Replikate?}

Wie wir gelernt haben, ist der Typ I Fehler unabhängig von der Anzahl der Datenpunkte. Was sich ändert ist Teststärke (engl. power) = 1 - Typ II Fehler (falsche Negative), und FDR. \marginnote{Teststärke ist die Wahrscheinlichkeit, Signifikanz für einen Effekt zu finden, wenn es welche gibt}. Für statistische Standardverfahren kann die Teststärke berechnet werden. Im Allgemeinen 

\begin{enumerate}
\item steigt die Teststärke wenn die Effekte stärker sind und die Daten mehr
\item sinkt die Teststärke wenn die Variabilität zunimmt, oder wenn man $\alpha$ erhöht
\end{enumerate}

In den meisten Fällen kann man Effektstärken, Rauschen etc. aus Erfahrung abschätzen. Dann kann man in einer Poweranalyse berechnen wir, wie viele Replikate benötigt werden.

\newpage
\begin{mdframed}
    
\textbf{Checkliste für die Versuchsplanung}

\begin{description}

\item[( )] Eindeutige, logische Frage? Schriftlich festhalten. Siehe auch Kapitel über gute Fragen  \href{http://florianhartig.github.io/ResearchSkills/}{{http://florianhartig.github.io/ResearchSkills/}

\item[( )] Frage auf Validität checken, siehe hier \href{http://florianhartig.github.io/ResearchSkills/}{{http://florianhartig.github.io/ResearchSkills/}

\item[( )] Versuchsplan

  \begin{description}

  \item[( )] Festlegen der zu messenden Variablen. Entscheide, ob man an linearen Hauptauswirkungen oder auch an nichtlinearen Effekten oder Interaktionen interessiert ist.
  
  \item[( )] Potenzielle Störfaktoren? Entscheide, ob sie besser kontrolliert, randomisiert oder gemessen werden sollten. Sind Kandidaten sicher störend (korreliert mit abhängige Variable UND einem oder mehreren der Prädiktoren)?
  
  \item[( )] Definiere final die zu testende statistische Hypothese, einschließlich der Störfaktoren. Festhalten, z.B. in der Form $height  \sim age + soil * precipitation + precipitation^2$. 
  
  \item[( )] Wähle aus, wie die Variablen im Experiment variiert werden. Ziehe die Verwendung von Software dafür in Betracht, z.B. für faktorielle Versuchsplanungen (in Beobachtungsstudien hat man manchmal begrenzte Kontrolle, aber man kann vielleicht schätzen, welche Variablenkombinationen beobachtet werden).
  
  \item[( )] Blocking - versuche, verschiedene Behandlungen / verschiedenste Kombinationen zusammen zu gruppieren. Das Ziel ist, dass unbekannte / nicht gemessene Variablen nicht mit experimentellen Variablen korrelieren (siehe Pseudoreplikation).
  
  \item[( )] Bestimme die Anzahl der Replikate. Mache eine Vermutung für Effektgröße und Variabilität der Daten, und berechne oder errate die Anzahl der notwendigen Wiederholungen, um genügend Teststärke zu erhalten. Was genügend bedeutet, hängt vom Sachgebiet ab, aber ich würde behaupten, dass du eine hohe Chance haben möchtest, einen Effekt zu sehen, wenn einer da ist, also wäre eine Teststärke von $>80\%$ gut.
  
  \end{description}
  
\item[( )] Versuchsaufbau prüfen.
  
  \begin{description}
  
  \item[( )] Spiele den Versuchsablauf durch, entweder in Kopf, als Pilotstudie, oder am Computer. 
  
  \item[( )] Spiele die Datenanalyse durch! Komplette Auswertung

  \end{description}


\item[( )] überarbeite alles, wenn nötig

\end{description}

\end{mdframed}


\chapter{Wissenswertes und weiterführende Lektüren}

\section{Reproduzierbarkeit und gute wissenschaftliche Vorgehensweise}

Reproduzierbarkeit bedeutet, dass jeder Schritt der Analyse wiederholbar ist. Die Erfahrung zeigt, dass es nicht so trivial ist, wie man denkt, die Reproduzierbarkeit zu gewährleisten. Hier einige Hinweise, wie man Datenanalyse reproduzierbar machen kann

\begin{itemize}

\item{Sobald man Rohdaten produziert hat, ändert man sie NIE. Man speichert sie an einem Speicherort, erstellt eine Sicherung und berührt sie niemals erneut}

\item{In der Regel muss man einige Aufwertungen, Umbenennung etc. vor der Datenanalyse erledigen. Wenn möglich, macht man dies durch ein Skript (z. B. R, Python, perl). Man speichert das Skript zusammen mit der Analyse ab.}

\item{Verwende ein Versionskontrollsystem für den Code und notiere für jede Ausgabe die Revisionsnummer, mit der die Ausgabe erstellt wurde.}

\item{Wenn die Analyse durchgeführt wird, speichere die random-seed und die Einstellungen des Computers, um die Reproduzierbarkeit sicherzustellen. In R ist der einfachste Weg dies zu tun, die random-seed auf random.seed(123) zu setzen und die Ergebnisse von sessionInfo(), welche die Versionsnummern aller verwendeten Pakete enthält, zu speichern.}

\item{Denke darüber nach, den Code in einer Reporting-Umgebung wie z.B. Rmd oder sweave auszuführen.}


%\footnote{Siehe auch die R-Task-Ansicht über \href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{reproduzierbare Forschung}

\end{itemize}

\section{Weiterführende Literatur}\label{sec: further readings}


\begin{itemize}


\item Als praktisches Lehrbuch für Anfänger empfehle ich als deutsche Option  \citet{Dormann-ParametrischeStatistik-2013} oder Dormann, C. & Kühn, I. (2011) Angewandte Statistik fuer die biologischen Wissenschaften (kostenloses Skript \href{https://www.biom.uni-freiburg.de/mitarbeiter/dormann/resolveuid/ed4f35206584421e7406414aa2d4470a}{hier}, und \citet{Gotelli-PrimerEcologicalStatistics-2004} als englische Option. 

\item Technisch etwas anspruchsvoller (aber immer noch sehr elementar) ist \citet{James-IntroductiontoStatistical-2013}. Man kann sich das PDF kostenlos herunterladen und es gibt ein MOOC mit Vorträgen und Übungen

\item Weitere Hilfe und Hinweise kann man in der Statistik-Hilfe unserer Abteilung \href{http://biometry.github.io/APES/}{hier} finden; siehe insbesondere die Empfehlungen zu Lehrbüchern \href{http://biometry.github.io/APES/Stats/stats90-references.html}{hier}.

\end{itemize}


\bibliographystyle{chicago}
\bibliography{/Users/Florian/Home/Bibliography/Databases/flo}

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\begin{appendices}

\vspace{1cm}
\begin{fullwidth}
%\begin{mdframed}

\chapter{Lösungen der Übungsfragen}

\textbf{4.1} p(D|M,x) - in Worten: die Wahrscheinlichkeits(dichte) für D, gegeben M und x.
\textbf{4.2} Ja.
\textbf{4.3} Nein. Der MLE gibt den Wert für den die Wahrscheinlichkeit der Daten maximual ist. Um die Aussage umzudrehen müssen wir Zusatzannahmen machen (alle Parameter gleich wahrscheinlich).
\textbf{4.4} p(d >= D | H0), in Worten: Wahrscheinlickhkeit die beobachteten oder extremere Daten zu bekommen wenn H0 wahr ist.
\textbf{4.5} Extremer bezieht sich auf die Teststatistik - diese ist ein Abstandsmaß dass der “Erfinder” des Testes wählt. Weil es hierfür unterschiedliche Möglichkeiten gibt gibt durchaus verschiedene Tests mit gleichen H0, aber anderen Teststatistiken.
\textbf{4.6} H0: Die durchschnittliche Zeit aller 3 Firmen ist gleich.
\textbf{4.7} Mögliche Antworten: Varian der 3 Mittelwerte, Unterschied größter / kleinster Mittelwert, Durchschnitt der Differenzen der Mittelwerte, … [alles womit man die Firmen vergleichen könnte .. natürlich könnten Sie auch Mittewert durch Median ersetzen].
\textbf{4.8} H0 = das Standardmodell.
Anmerkung: ich wollte Ihnen mit diesem Beispiel zeigen dass es 2 Möglichkeiten gibt Nullhypothesen aufzustellen: Wenn wir einen Effekt bestätigen wollen, stellen wir die umgekehrte Nullhypothese auf, also dass es keinen Effekt gibt (das ist der Normalfall in der Analyse von biologischen Daten). Der Hintergrund hier ist dass wir einen Effekt vermuten, aber nicht sicher sind oder: Wenn der Effekt / das Modell aber schon gut bekannt ist, d.h. es eine feste Theorie gibt, wäre das normale Vorgehen diesen Effekt / Theorie als H0 aufzustellen, und zu schauen ob es eine Abweichung von dem gibt was wir gerade als wahr ansehen.
\textbf{4.9} Nein, der p-Wert gibt die Wahrscheinlichkeit der Daten gegeben H0 an, nicht die Wahrscheinlichkeit von H0.
\textbf{4.10}“Die Wahrscheinlichkeit dass die beobachteten Effekte oder stärker unter H0 (d.h. ohne Wirkung des Medikaments) auftreten ist 3\%. Der Unterschied zwischen Kontrolle und Behandlung ist deshalb signifikant bei einen Signifikanzlevel von 5\%.”
\textbf{4.11} Wahrscheinlichkeit dass der p-Wert signifikant wird wenn H0 wahr ist.
\textbf{4.12} 7\%.
\textbf{4.13} Wahrscheinlichkeit dass der p-Wert nicht signifikant wird wenn H0 nicht wahr ist.
\textbf{4.14} Nur mit dieser Information kann man das nicht sagen, weil der Typ II von mehreren Faktoren abhängt.
\textbf{4.15} Mögliche Antworten: Sigifikanzlevel - negativ; Varianz - positiv; Effektstärke: negativ; power / Stichprobengröße: negativ.
\textbf{4.16} power = 1 - Typ I Fehler / in Worten: also Wahrscheinlichkeit dass der p-Wert signifikant wird wenn H0 nicht wahr ist.
\textbf{4.17} 5 von 100 signifikante Tests sind zu erwarten wenn keines der Gene eine Wirkung hat (wir machen hier multiples Testen). Das Ergebnis zeigt keine besondere Evidenz für einen Effekt an. Bemerkung: Trotzdem würde man natürlich die 5 signifikanten mit einen weiteren Experiment noch mal nachtesten, man weiß ja nie. Aber statistisch ist so ein Ergebnis zu erwarten, auch wenn keines der Gene eine Wirkung hat.
\textbf{4.18} Nein, denn wenn Ihre Teststärke fast 100\% ist und Sie wirklich Assoziationen in den Datensatz hätten dann müssten Sie ja trotzdem MEHR als 5\% positive sehen.
\textbf{4.19} Die Rate an Experimenten die Signifikanz anzeigt obwohl kein Effekt da ist wenn man viele Experimente macht.
\textbf{4.20} FDR hängt ab von Typ I und II Fehlerrate und der Wahrscheinlichkeit dass H0 wahr ist (bzw. dass die in den Experimente getesteten Effekt da sind).
\textbf{4.21} 0.95 * 0.05 / (0.95 * 0.05 + 0.05 * 1) - Rate an Typ I / Rate an signifikante Ergebnissen.
\textbf{4.22} Abh. Variable beschreibbar durch Polynom der unabhängigen Variable(n) + normalverteilte Streuung.
\textbf{4.23} Man sucht den MLE für die Annnahmen der linearen Regresion.
\textbf{4.24} H0 = Parameter ist 0.

%\end{mdframed}
\end{fullwidth}

\chapter{R und Rstudio}

R selbst ist ein Kommandozeilenprogramm. Das heißt man kommuniziert mit R über schriftliche Befehle, die man in die R-Konsole eingibt. Hierzu schreibt man entweder die Befehle direkt in die Konsole, oder man schreibt den R Code in einem Texteditor vor und fügt ihn dann in die R-Konsole ein.

Für die tägliche Arbeit ist dies jedoch recht unpraktisch. Man hätte gerne ein Programm, dass einen Texteditor mit der Konsole und weiteren Optionen (z.B. die Möglichkeit graphische Ausgaben aus R anzuzeigen) kombiniert. R bietet eine einfache Version eines solchen Programms an, die RGui (ist ist zu beachaten, dass die Mac und Windows Versionen der RGui leicht unterschiedlich aussehen).  Um die RGui unter Windows zu starten, startet man einfach das Programm "R". Wenn man in der RGui 2 + 2 im Hauptfenster eintippt und "Enter" drückt, sollte Folgendes zu sehen sein:

\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{rgui1.png}
\caption{Eine einfache Rechnung in RGui tippen}
\label{fig: Rgui1}
\end{center}
\end{figure}

Das Fenster, das man hier sieht, ist die R-Konsole. Durch die Konsole interagiert man mit dem R Kern, der die statistischen Berechnungen ausführt.

Nun schreiben wir etwas anderes: R hat einige Standarddatensätze, die automatisch geladen werden. Wir werden den "airmiles" Datensatz verwenden (standardmäßig in R verfügbar), der die Anzahl der vergebenen Meilen der amerikanischen Fluggesellschaften über die Zeit anzeigt. Um eine Abbildung dieses Datensatzes zu erstellen schreiben wir 

<<eval=F>>=
plot(airmiles, col = 4)
@

Das Ergebnis sollte wie in Fig.~\ref{fig: Rgui2} aussehen:

\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{rgui2.png}
\caption{Ein Graph erstellt mit RGui}
\label{fig: Rgui2}
\end{center}
\end{figure}

Wie man erkennen kann, öffnet dieser Befehl anscheinend ein neues Fenster (eine Grafik-Ausgabe) und plottet die Meilen gegen die Zeit. Wir werden gleich diskutieren, warum und wie das funktioniert. Bevor uns aber zu sehr an die RGui gewöhnen wechseln wir doch erst mal zu RStudio, einem alternatives Programm, um mit R zu interagieren.

\section{Der Rstudio Editor}
 
RStudio bietet grundsätzlich die gleichen Funktionen wie RGui, aber es bietet mehr Optionen. So sieht es aus:

\begin{figure}[]
\begin{center}
\includegraphics[width = 9cm]{rst_interface.png}
\caption{Der RStudio Editor, der wohl beliebteste Editor für R}
\label{fig: Rstudio}
\end{center}
\end{figure}


\paragraph{Konsole:} Die Konsole, die wir bereits gesehen haben, befindet sich im linken unteren Bereich. Man kann überprüfen, dass sie sich gleich verhält, indem man die gleichen Befehle wie zuvor eingibt, d.h. 2 + 2, oder das Diagramm erneut grafisch darstellt.

\paragraph{Editor:} über der Konsole (oben links) werden R Skriptdateien angezeigt und können im Editor geändert werden. Die Idee einer Skriptdatei ist, dass man alle Befehle sammelt, die man an die Konsole in einer Datei sendet, sodass man sie später erneut ausführen kann.  

Ein typisches Skript sieht so aus:

<<eval=F>>=
# Die Raute leitet einen Kommentar ein
# Skript von FH, 25.10.13

rm(list=ls(all=TRUE))  # rm (= remove) löscht alle geladenen Variablen

# load some data

# do some plots

@

Um einen Teil des Skripts an die Konsole zu senden, kann man den run-Button am oberen rechten Rand des Editorfensters verwenden. Für alles, was wir von nun an tun, würde ich dringend empfehlen, es in das Skript zu schreiben und es dann von dort aus an die Konsole zu senden.

\chapter{Verarbeitung von Daten in R}\label{HandlingDataInR}

\section{Variablen}

Wer hat jemals mit einer Programmiersprache gearbeitet? In einer Programmiersprache werden die Daten in Variablen/Objekten gespeichert. So weisen wir der Variablen "?VariableX"' das Wort "`test"' zu,

<<>>=
VariableX = "test"
@

Ich kann auf die Variable zugreifen, indem ich den Variablennamen in der Konsole eingebe. Hierdurch wird der gespeicherte Wert ausgeben.

<<>>=
VariableX
@

Die global vorhanden Variablen werden in der oberen rechten Ecke oder in RStudio angezeigt. Wie man sieht haben wir hier die Variable "?VariableX"' zusammen mit ihrem Wert.


\begin{figure}[]
\begin{center}
\includegraphics[width = 6cm]{rst_globenv.png}
\caption{Die globale Umgebung wird in der oberen rechten Ecke des R Studio-Editors angezeigt.}
\label{fig: Rstudio}
\end{center}
\end{figure}

\section{Datentypen und Strukturen}

Eine Variable kann verschiedene Dinge speichern: eine Zahl, ein Wort, eine Liste oder eine ganze Datenmenge.

Der einfachste Fall sind Variablen, die nur einen **einzigen Wert** enthalten. Hier ist die Frage, welche Art von Werten die Variable enthält. Die verschiedenen Datentypen, die ein einzelner Wert haben kann, heißen die **atomic types** - sie entsprechen den grundlegenden Datentypen in R. Wichtige atomic types sind:

  \begin{itemize}
		\item boolean (TRUE / FALSE)
		\item ganzzahlig (1, 2, 3, 5)
		\item numerisch (1.1, 2.5, 3.456)
		\item Faktor ("`rot"', "`grün"', "`blau"')
		\item Subjekt ("`ein Wort"', "`ein weiteres Wort"')
	\end{itemize}

Wenn wir eine Sammlung von mehreren 'atomic types' haben, sprechen wir von einer Datenstruktur oder einem Objekt (es gibt einen Unterschied, der hier aber keine Rolle spielt). Wichtige Beispiele hierfür sind:

  \begin{itemize}
		\item **Vektor** (Eine Reihe der gleichen 'atomic types', z.B. [1,2,3,4,5] )
		\item **Liste** (Im Grunde wie ein Vektor, kann aber verschiedene Typen enthalten, Bsp. [1, "`rot"', FALSE] )
		\item **data.frame** (Eine Liste von Vektoren; dies ist das Standardformat für Daten in R. Man stellt sich dies wie eine Tabelle vor - jede Spalte ist ein Vektor und kann einen anderen Typ haben)
	\end{itemize}

Eine vollständige Liste der Datentypen findet man \href{http://www.statmethods.net/input/datatypes.html}{hier}. 
 
\subsection{Prüfung von Datentypen und Strukturen}

Besonders nach dem Einlesen der Daten ist es wichtig zu prüfen, welchen Typ die Daten haben. Die Funktionen in R reagieren je nach atomic type unterschiedlich.

Wenn man wissen will, welchen Typ oder Struktur eine Variable hat, verwendet man den str-Befehl:

<<eval=F>>=
str(object)
@

Um eine Zusammenfassung der Struktur (z.B. Mittelwert pro Spalte; was genau zusammengefasst wird, ist abhängig von Datenstruktur und Typ) zu erhalten, verwenden man:

<<eval=F>>=
summary(object)
@

Um einen automatischen Plot zu erstellen (R wird selbst entscheiden, welcher am besten für diese Struktur/Typ geeignet ist), tippt man:

<<eval=F>>=
plot(object)
@

Aufgabe: probiere dies mit dem Objekt Luftqualität.

\subsection{Zugriff auf Spalten, Zeilen und Elemente in einem Datenframe oder einer Matrix}

Die häufigste Struktur in R ist der data.frame (df). Ein df ist eine Liste von Vektoren. Die Vektoren erscheinen als Spalten. Das heißt ein df ist eine Tabelle, in der jede Spalte ein anderer Datentyp sein kann.

Man kann Spalten auf verschiedenste Weisen auswählen:

- Nach Name:
<<eval=F>>=
airquality$Ozone
@

- Nach Spaltenindex: 

<<eval=F>>=
airquality[,1]
@

Zu beachten sei hier, dass die erste Spalte durch [,1] ausgedrückt wird. Die erste Reihe erhält man durch [1,].

\section{Daten auswählen}

Die letzte Art des Datenzugriffs ist ein Beispiel für das "`Slicen"'. Slicing ist eine sehr leistungsfähige Technik, die in den meisten wissenschaftlichen Programmiersprachen verfügbar ist. Das bedeutet, dass man auf seine Daten zugreifen kann, indem man Spalten, Zeilen oder bestimmte Elemente eingibt. Man schaue sich hierfür die folgenden Befehle an (Erläuterung immer unterhalb):

<<eval=F>>=
airquality[,1:2]
@

verschafft die ersten Spalten 1 und 2.


<<eval=F>>=
airquality[4:6,1]]
@

selektiert die Reihen 4 bis 6 in Spalte 1, und


<<eval=F>>=
airquality[c(1,2,3,4,7,8),1]
@

selektiert die Reihen 1,2,3,4,7,8 in Spalte 1. So können wir eine beliebige Kombination von gewünschten Elementen sehr bequem aus dem Datenframe auswählen. 

Man muss übrigens nicht immer alle Zahlen von Hand eintippen. Die Kombination 

<<eval=F>>=
1:10
@

verschafft uns zum Beispiel die Werte von 1 bis 10, und

<<eval=F>>=
c(1:5,7)
@

erzeugt die Werte 1 bis 5 und 7. Die c() - Funktion kombiniert Werte in einem Vektor (für das Slicen ist es notwendig die Werte in einem Vektor zu haben)

Wir können aber auch Selektionen mit logischen Operatoren erzeugen

<<eval=F>>=
airquality$Temp > 80
@

Erstellt einen Vektor mit True auf allen Temperaturwerten, die >80 sind. Ich kann diesen speichern und für eine Selektion, oder auch sofort verwenden

<<eval=F>>=
airquality[airquality$Temp > 80 , ]
@

Wählt alle Zeilen mit einer Temperatur >80 aus.


\section{Laden von Daten in R}

Bisher hatten wir die Daten schon im R Programm. Nun werden wir demonstrieren, wie einige Daten geladen werden (wir verwenden die airquality.txt-Datei, die zur Verfügung gestellt wird)

Es gibt grundsätzlich zwei Möglichkeiten Daten zu laden

- mit RStudio (Punkt und Klick) - gehe zu Environment (oben rechts), importiere den Datensatz und folge den Anweisungen

- aus dem Skript mit dem Befehl read.table ()

Das funktioniert so

<<eval=F>>=
data = read.table("airquality.txt", header = T)
@

In diesem Fall funktioniert es gut, weil ich die Daten auf die einfachste Weise vorbereitet habe. Wenn man andere Datenformate (Kommas, Semikolons, überschriften / keine überschriften) hat, befragt man die Hilfe-Option mit dem Befehl read.table. Siehe auch http://www.statmethods.net/input/importingdata.html für weitere Optionen, z.B. Excel oder Datenbankimport.

\subsection{Überprüfen der Daten}

Wie schon gesagt: nach dem Laden der Daten immer überprüfen, ob das Datenformat korrekt ist

<<>>=
str(data)
@

Hier sieht man den atomic type jeder Spalte. Stelle sicher, dass es dem entspricht, was man möchte (manchmal wird numerisch als Faktor eingelesen , oder umgekehrt). Wenn eine Spalte den falschen Typ haben würde, müssen wir das manuell ändern durch:

<<eval=F>>=
as.factor(x)
@

oder

<<eval=F>>=
as.numeric(x)
@

Hinweis: Wie gesagt, der airquality Datensatz ist in R enthalten. Man kann also trotzdem mit diesem Skript weiterarbeiten, auch wenn das Einlesen nicht funktioniert hat. Wenn es Proleme gab, setze einfach

<<eval=F>>=
data = airquality
@


\chapter{Plot-Befehle in R}

Der einfachste graphische Befehl in R ist plot. Normalerweise plottet er x gegen y Werte, so wie hier.

<<fig.height = 4, fig.width=4, fig.align='center'>>=
plot(airquality$Ozone, airquality$Temp)
@

Plot kann allerdings auch auf andere Datenstrukturen angewandt werden. In diesem Fall wird nicht der eigentlich plot Befehl ausgeführt, sondern die Funktion sucht sich automatisch den Befehl der am sinnvollsten für die gegeben Struktur erscheint. Wendet man, z.B., plot auf einen data.frame an, so wird der sogenannte pair plot ausgeführt:


<<fig.height = 4, fig.width=4, fig.align='center'>>=
plot(airquality)
@

Ihr könnte euch selbst überzeugen dass das identisch ist zu pairs(airquality). 

Hier ein paar weitere wichtige Abbildungstypen, zuerst ein Histogramm,\marginnote{Hier benutze ich zum ersten Mal Farbe - ein Überblick über die Farbennamen in R ist \href{hier}{http://research.stowers-institute.org/efg/R/Color/Chart/ }}


<<fig.height = 4, fig.width=4, fig.align='center'>>=
hist(airquality$Ozone, breaks = 30, col = "darkred", xlab = "Ozone ")
@

ein einfaches Balkendiagramm,

<<fig.height = 4, fig.width=4, fig.align='center'>>=
counts <- table(mtcars$gear)
barplot(counts, main="Car Distribution", 
   xlab="Number of Gears")
@

ein gruppiertes Balkendiagramm,

<<fig.height = 4, fig.width=4, fig.align='center'>>=
counts <- table(mtcars$vs, mtcars$gear)
barplot(counts, main="Car Distribution by Gears and VS",
  xlab="Number of Gears", col=c("darkblue","red"),
  legend = rownames(counts), beside=TRUE)
@

ein Boxplot, 

<<fig.height = 4, fig.width=4, fig.align='center'>>=
boxplot(mpg~cyl,data=mtcars, main="Car Milage Data", 
   xlab="Number of Cylinders", ylab="Miles Per Gallon")
@

ein 'notch boxplot', in dem zusätzlich eine Information über die Unsicherheit des Medians aufgetragen wird,

<<fig.height = 4, fig.width=4, fig.align='center'>>=
boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE, 
  col=(c("gold","darkgreen")),
  main="Tooth Growth", xlab="Suppliment and Dose")
@

%See http://www.statmethods.net/graphs/index.html 

und zuletzt eine sogenannte heatmap, die die Korrelation zwischen Variablen visalisiert. Eine praktische Eigenschaft dieser Funktion ist, dass Variablen neu angeordnet werden können, sodass stark korrelierte Variablen nahe beieinander liegen. Dies ist oft nützlich um einen Überblick über "cluster" von stark korrelierten Variablen zu bekommen.

<<fig.height = 6, fig.width=6, fig.align='center'>>=
round(Ca <- cor(attitude), 2)
symnum(Ca) # simple graphic
heatmap(Ca, symm = TRUE, margins = c(6,6)) # with reorder()
@

All dies sind nur Beispiele. Es gibt unendlich viele Grafikbefehle in R. Eine vollständige Auflistung ist an dieser Stelle unmöglich. Als weiterführende Literatur ist das Kapitel über R Grafiken in \citet{Ligges-ProgrammierenmitR-2008} sehr zu empfehlen. Langfristig muss man sich ein bisschen durch Beispiele klicken, z.B.

\begin{itemize}
\item \href{http://rgraphgallery.blogspot.de/search/label/3%20vartiable%20plots}{hier}
\item Sehr hilfreich, der \href{http://shiny.stat.ubc.ca/r-graph-catalog/#}{R Graphenkatalog}
\item \href{http://rgm3.lab.nig.ac.jp/RGM/R_image_list?page=2282&init=true}{R Graphik-Handbuch}
\item \href{http://www.statmethods.net/graphs/line.html}{QuickR}
\end{itemize}



\chapter{Regression in R}


\section{Stetige abhängige Variable - lineare Regression}

Die lineare Regression ist die einfachste Form der Regression. Die lineare Regression wird angewandt wenn die abhängige Variable metrisch ist. Die Annahme ist, dass die abhängige Variable von den Prädiktoren wie ein Polynom abhängt:

<<eval = F>>=
y ~ par1 * pred1 +  par2 * pred2 +  par3 * pred2^2 + ... + residual Error
@

Wobei die Parameter par1 ... par3 geschätzt werden und der Residuenfehler normalverteilt ist.

Werfen wir einen Blick auf einige Beispiele, mit den Luftqualitätsdaten. Wir sehen, dass es eine Korrelation zwischen Ozon und Temperatur gibt.

<<>>=
plot(airquality$Temp~airquality$Ozone)
@

Mit dem Befehl lm() können wir R versuchen lassen, eine bestmöglich passende Gerade zwischen den beiden Variablen zu finden.

<<>>=
fit = lm(airquality$Temp~airquality$Ozone)
@

Lassen Sie uns das Ergebnis zuerst visuell betrachten

<<>>=
plot(airquality$Ozone, airquality$Temp)
abline(fit, col = "blue")
@

Hier ist die Ausgabe im Detail

<<>>=
summary(fit)
@

In der Ausgabe sehen wir die Parameter für die Auswirkung von Ozon (genannt Regressionssteigung) und den Schnittpunkt. Wir werden später sehen, wie wir andere Funktionen spezifizieren können. 

\subsection{Residuenanalyse}

Mit plot(fit) erhalten wir die Residuen (die Abweichung von der Geraden). Wie bereits gesagt, geht die lineare Regression davon aus, dass diese normal verteilt sind, also sollten wir überprüfen, ob dies wirklich der Fall ist.

<<>>=
par(mfrow=c(2,2))
plot(fit)
@

Hier sind die Residuen nicht wirklich homogen um den vorhergesagten Wert gestreut, was darauf hindeutet, dass das Modell nicht sehr gut passt. Dies hätte man schon erahnen können, weil die Korrelation nicht sehr linear aussieht. Wir können einen quadratischen Term hinzufügen durch

<<>>=
fit2 = lm(airquality$Temp~airquality$Ozone + I(airquality$Ozone^2))
summary(fit2)
@

Die Residuen sehen jetzt besser aus

<<>>=
par(mfrow=c(2,2))
plot(fit2)
@

Plotten der Ergebnisse

<<>>=
plot(airquality$Ozone, airquality$Temp)
points(fit2$model[,2], predict(fit2), col = "blue")
@

\subsection{Kategoriale Prädiktoren}

Wenn wir kategoriale Variablen wie in diesem Datensatz haben

<<>>=
boxplot(PlantGrowth$weight~PlantGrowth$group, main = "growth of plants")
@

Funktioniert die Formel immer noch

<<>>=
fit <- lm(weight~group, data = PlantGrowth)
summary(fit)
@

Was aber auf den ersten Blick verwirrend ist, ist dass wir jetzt zwei Parameter-Schätzungen für den Prediktor group haben. Der Grund dafür ist, dass es 3 levels innerhalb von group gibt. Die erste Gruppe wird in diesem Fall automatisch als Referenz gesetzt, und für die anderen Gruppen werden Prädiktoren geschätzt. Die p-Werte für diese Prädiktoren sind also gegen die Referenz (wenn ich Prädiktor trt1 herausnehme, erhält er den Wert von ctrl). Daher hängen diese p-Werte für kategoriale Variablen von der Reihenfolge der Variablen ab (man kann die Reihenfolge von trt1 als Referenz ändern.)


\subsection{ANOVA}

Bei kategorialen Variablen (blau, grün, rot) testet die Regression also gegen die erste Möglichkeit (blau). Oft fragt man sich aber: Gibt es überhaupt Unterschiede zwischen den Gruppen? Diese Frage wird von der ANOVA beantwortet. Der ANOVA liegt das gleiche Modell wie der Regression zugrunde, nur dass sie einen anderen Test macht: getestet wird ob das Modell mit der Variable signifikant besser ist als ohne. 

<<>>=
aovresult <- aov(fit)
summary(aovresult)
@

Beachte, dass wir jetzt Signifikanz für einen allgemeinen Unterschied erhalten, obwohl wir keine Signifikanz in der Regression zuvor hatten. Wir könnten nun noch sogenannte Post-Hoc-Tests verwenden, um herauszufinden, welche Unterschiede signifikant sind.

Man kann eine ANOVA auch für metrische Variable machen, aber es macht wenig Sinn, da kommt das das gleiche raus wie bei der normalen Regression. 

Weitere Beispiele mit weiteren Faktoren findt man \href{http://www.statmethods.net/stats/anova.html}{hier}.


\subsection{T-Test}

Eine einfache Option für das Testen von nur zwei Gruppen gegeneinander ist der T-Test. Er nimmt eine normale Verteilung innerhalb der Gruppen an. Wir können dies verwenden, um Post-Hoc-Tests für das obige Beispiel durchzuführen:

<<>>=
attach(PlantGrowth)
t.test(weight[group=='ctrl'], weight[group=="trt1"])
@

Testen gegen Gruppe 2

<<>>=
t.test(weight[group=='ctrl'], weight[group=="trt2"])
detach(PlantGrowth)
@

Aber wenn wir wirklich beide Tests ausgeführt hätten, müssten wir für multiple Tests korrigieren

<<>>=
p.adjust(c(0.2504, 0.0479), method = "holm")
  @

Wie funktioniert das? Schreibe ?p.adjust in die Konsole. Lese auch \href{das hier}{http://webdev.cas.msu.edu/cas992/weeks/week10.html}.

\section{Das Framework der generalisierten lineare Modelle}

Das GML ist eine Verallgemeinerung von linearen Modellen (lm), mit dem vor allem auch Antwortvariablen mit nicht-metrischen Skalenniveau (0/1) behandelt werden können. Dies wird in R über die Spezifikation von Verteilung und Link Funktion erreicht. Die folgenden Funktionen mit Link defauls sind verfügbar

<<eval = F>>=
binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
@


Aber Schritt für Schritt ... schauen wir uns ein Beispiel für 0/1 Daten an


\section{0/1 abhängige Variable - die logistische Regression}

<<>>=
library(effects) 
data(TitanicSurvival)
head(TitanicSurvival)
str(TitanicSurvival)
attach(TitanicSurvival)
@

Visualisierung mit dem Mosaikplot. Möglicherweise muss man zuvor das dazu benötigte Paket installieren: install.packages ("vcd"). Informationen zur Visualisierung von Assoziationen findet man unter \href{http://www.statmethods.net/advgraphs/mosaic.html}{diesem Link}.

<<>>=
library(vcd)
mosaic(~ sex + passengerClass + survived, shade=TRUE, legend=TRUE) 
surv <- as.numeric(survived)-1 # glm requires 0 / 1 not true false
@


Wir werden zuerst testen, ob das überleben mit dem Alter korreliert

<<>>=
fmt <- glm(surv ~ age, family=binomial)
summary(fmt)
@

Die 0/1 Response wird hier durch  family=binomial spezifiziert. Das Ergebnis sieht so aus

<<>>=
plot(surv ~ age, main="only age term")
newage <- seq(min(age, na.rm=T), max(age, na.rm=T), len=100)
preds <- predict(fmt, newdata=data.frame("age"=newage), se.fit=T)
lines(newage, plogis(preds$fit), col="purple", lwd=3)
lines(newage, plogis(preds$fit-2*preds$se.fit), col="purple", lwd=3, lty=2)
lines(newage, plogis(preds$fit+2*preds$se.fit), col="purple", lwd=3, lty=2)
@

Nun setzen wir alle relevanten Variablen ein in:

<<>>=
surv <- as.numeric(survived)-1 # glm requires 0 / 1 not true false
fmt <- glm(surv ~ age  + sex + passengerClass, family=binomial)
summary(fmt)
@

\subsection{ANOVA für GLM}

Wenn eine ANOVA erwünscht ist

<<>>=
library(car)
Anova(fmt)

detach(TitanicSurvival)
@

\section{Zähldaten - Poisson's Regression}

Für Zähldaten verwenden wir den GLM mit der Poissonverteilung. Hier sind einige Beobachtungen der Verteilung von Futterstücke an junge Vögel und ihre wahrgenommene Attraktivität.

<<>>=
cfc <- data.frame(
  stuecke = c(3,6,8,4,2,7,6,8,10,3,5,7,6,7,5,6,7,11,8,11,13,11,7,7,6),
  attrakt = c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5) 
)
attach(cfc)
plot(stuecke ~ attrakt)
@


So wird der Poisson angegeben

<<>>=
fm <- glm(stuecke ~ attrakt, family=poisson)
summary(fm)
@

Vorhersagen

<<>>=
newattrakt <- c(1,1.5,2,2.5,3,3.5,4,4.5,5)
preds <- predict(fm, newdata=data.frame("attrakt"=newattrakt))
plot(stuecke ~ attrakt)
lines(newattrakt, exp(preds), lwd=2, col="green")
@

Das gleiche mit 95\% Konfidenzinterval:

<<>>=
preds <- predict(fm, newdata=data.frame("attrakt"=newattrakt), se.fit=T)
str(preds)
plot(stuecke ~ attrakt)
lines(newattrakt, exp(preds$fit), lwd=2, col="green")
lines(newattrakt, exp(preds$fit+2*preds$se.fit), lwd=2, col="green", lty=2)
lines(newattrakt, exp(preds$fit-2*preds$se.fit), lwd=2, col="green", lty=2)

detach(cfc)
@


\end{appendices}


 
\end{document}
